<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>【论文笔记】DLFuzz</title>
    <url>/2020/08/20/DLFuzz/</url>
    <content><![CDATA[<p>原文：DLFuzz: Differential Fuzzing Testing of Deep Learning Systems （ESEC/FSE’18）<a id="more"></a></p>
<p>代码：<a href="https://github.com/turned2670/DLFuzz">https://github.com/turned2670/DLFuzz</a> </p>
<h2 id="可控制变量及参数总结"><a href="#可控制变量及参数总结" class="headerlink" title="可控制变量及参数总结"></a>可控制变量及参数总结</h2><ul>
<li>输入集合（未标注）</li>
<li>待测试DNN</li>
<li>$k$：除原预测标签外，top-k个其他标签</li>
<li>$m$：欲覆盖的神经元个数</li>
<li><p>strategies：神经元选择策略</p>
<ul>
<li><p>策略1：选择过去测试中常被覆盖的神经元</p>
</li>
<li><p>策略2：选择过去测试中极少被覆盖到的神经元</p>
</li>
<li><p>策略3：选择权重高的神经元</p>
</li>
<li>策略4：选择激活阈值附近的神经元</li>
</ul>
</li>
<li>$\lambda$：平衡两个目标（预测类别差异和覆盖新的神经元）的参数</li>
<li>predict_weight：代码里在上公式中$\sum c_i$前的权重，默认为0.5（未在论文里出现的参数）</li>
<li>iter_times: 每个种子的迭代次数</li>
<li>threshold: 神经元激活阈值</li>
<li>learning_step：步长，代码里设为0.02</li>
</ul>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul>
<li><p>梯度约束：<font color="red">论文里说可以加保持符号约束或DeepXplore里的约束，但代码似乎没加任何约束，直接在输入上增加梯度*步长</font></p>
</li>
<li><p>生成图像距离约束：满足L2距离（&lt;0.02）（计算方式为L2_norm / orig_L2_norm）</p>
</li>
<li><p>约束：一个输入能提升的神经元覆盖率随着时间的增加而下降，对应的保留种子的阈值也随着运行时间的增加而降低<font color="red">（代码里体现为保留种子时最少需要提升的覆盖率随迭代次数增加而降低）</font></p>
<p><img src="/2020/08/20/DLFuzz/fig1.png" alt="fig1" style="zoom: 80%;"></p>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集和模型"><a href="#数据集和模型" class="headerlink" title="数据集和模型"></a>数据集和模型</h3><p>MNIST（LeNet-1, LeNet-4, LeNet-5）和ImageNet（VGG-16, VGG-19, ResNet50）</p>
<font color="red">与DeepXplore相同</font>

<h3 id="默认参数设置"><a href="#默认参数设置" class="headerlink" title="默认参数设置"></a>默认参数设置</h3><ul>
<li><p>随机选择20个初始输入<font color="red">（类别是否平衡？20是否过少？）</font></p>
</li>
<li><p>$k=4,m=10$，strategy为策略1，iter_times=3 </p>
</li>
</ul>
<h3 id="实验1：DLFuzz与DeepXplore比较"><a href="#实验1：DLFuzz与DeepXplore比较" class="headerlink" title="实验1：DLFuzz与DeepXplore比较"></a>实验1：DLFuzz与DeepXplore比较</h3><ul>
<li><p>实验方法：对相同的20个初始输入，比较DLFuzz相对DeepXplore神经元覆盖率、l2距离、生成对抗样本的个数、每个对抗样本平均生成时间</p>
</li>
<li><p>实验结果：</p>
<ul>
<li><p>覆盖率提升<font color="red">（DLFuzz的优化目标选择了10个神经元，DeepXplore只选了一个）</font></p>
</li>
<li><p>L2距离很小，生成的扰动更隐秘<font color="red">（DeepXplore未对距离做限制，甚至认为L1距离越大多样性越好）</font></p>
</li>
<li><p>生成对抗样本数量更多（DeepXplore对每组DNN每张图片最多只生成一个对抗样本，DLFuzz每个模型每张图片可以生成多个对抗样本）</p>
</li>
<li><p>更短的时间消耗（除了ResNet50，因为神经元数量大所以选择神经元的耗时长）</p>
</li>
</ul>
</li>
</ul>
<p><img src="/2020/08/20/DLFuzz/fig2.png" alt="fig2" style="zoom:67%;"></p>
<h3 id="实验2：四种神经元选择策略比较"><a href="#实验2：四种神经元选择策略比较" class="headerlink" title="实验2：四种神经元选择策略比较"></a>实验2：四种神经元选择策略比较</h3><ul>
<li><p>实验方法：比较四种策略和DeepXplore，随着测试图片生成数量增多，神经元覆盖率的增长趋势</p>
</li>
<li><p>实验结果：策略1略好<font color="red">（生成数量是否过少？19张神经元覆盖率就趋于平缓）</font></p>
<p><img src="/2020/08/20/DLFuzz/fig3.png" alt="fig3" style="zoom:60%;"></p>
</li>
</ul>
<h3 id="实验3：用生成图片重新训练"><a href="#实验3：用生成图片重新训练" class="headerlink" title="实验3：用生成图片重新训练"></a>实验3：用生成图片重新训练</h3><ul>
<li>实验方法：用生成的114个对抗样本重新训练MNIST的三个DNN模型，平均提升准确率1.8%<font color="red">（太少？）</font></li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>模糊测试</tag>
        <tag>测试输入生成</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】On Decomposing a Deep Neural Network into Modules</title>
    <url>/2020/11/16/DNN-Decomposition/</url>
    <content><![CDATA[<p>On Decomposing a Deep Neural Network into Modules （FSE’20）<a id="more"></a></p>
<p>会议视频：<a href="https://www.youtube.com/watch?v=EH1aUbFj0HQ">https://www.youtube.com/watch?v=EH1aUbFj0HQ</a></p>
<p>代码：<a href="https://github.com/rangeetpan/decomposeDNNintoModules">https://github.com/rangeetpan/decomposeDNNintoModules</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul>
<li><p>传统软件的模块化分解与重组：可用于软件重用、替换、独立测试、独立开发等</p>
<p><img src="/2020/11/16/DNN-Decomposition/1.png" alt="1" style="zoom:67%;"></p>
</li>
<li><p>为什么DNN也需要模块分解？应用场景举例</p>
<ul>
<li><p>场景1：数据集内部分解</p>
<ul>
<li>手写数字识别→0，1识别，传统方法需要重新训练，本方法可直接将DNN解构成10个模块，组合成0、1识别模型。</li>
</ul>
<p><img src="/2020/11/16/DNN-Decomposition/2.png" alt="2" style="zoom:50%;"></p>
</li>
<li><p>场景2：数据集之间</p>
<ul>
<li><p>数字识别+字母识别-&gt;16进制数字识别</p>
<p><img src="/2020/11/16/DNN-Decomposition/3.png" alt="3" style="zoom:50%;"></p>
</li>
</ul>
</li>
<li><p>场景3：模块替换</p>
<ul>
<li><p>数字识别模型中某个数字5识别效果较差，从另一个模型中分解出单独识别5的模块，与A的其他模块组合在一起</p>
<p><img src="/2020/11/16/DNN-Decomposition/4.png" alt="4" style="zoom:50%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="步骤1：关注点识别-Concern-Identification-CI"><a href="#步骤1：关注点识别-Concern-Identification-CI" class="headerlink" title="步骤1：关注点识别 Concern Identification (CI)"></a>步骤1：关注点识别 Concern Identification (CI)</h3><ul>
<li>关注点识别本质上是识别出整体模型中对特定功能或关注点有贡献的那些部分，解构DNN。</li>
<li>具体地，依次对模型喂入关注类别的训练样本：<ul>
<li>对未激活节点：将输入边和输出边的权重都置为零（删除边）</li>
<li>对激活节点：将输出边权重置为正（<code>算法1没看懂为什么是min</code>）</li>
<li>删除与输出层其他类别相连的边。</li>
</ul>
</li>
</ul>
<p><img src="/2020/11/16/DNN-Decomposition/5.png" alt="5" style="zoom:60%;"></p>
<h3 id="步骤2：纠缠识别-Tangling-Identification（TI）"><a href="#步骤2：纠缠识别-Tangling-Identification（TI）" class="headerlink" title="步骤2：纠缠识别 Tangling Identification（TI）"></a>步骤2：纠缠识别 Tangling Identification（TI）</h3><ul>
<li><p>第一步之后，解构出的模块只对目标类别的识别起作用，即目前只是一个<strong>单分类器</strong>，无法对非本类的样本进行判断。这就好比从一个程序中删除一个条件及其分支，从而产生直接执行剩余分支功能的子程序，但无条件地这样做。（<code>即所有样本都会被分类成这一个类别</code>）</p>
</li>
<li><p>解决方法：<font color="red">加回一些节点和边，帮助区分非目标类别。</font>四种TI方法：</p>
<ul>
<li><p>Imbalance (TI-I)</p>
<p><img src="/2020/11/16/DNN-Decomposition/6.png" alt="6" style="zoom:60%;"></p>
</li>
<li><p>Punish Negative Examples (TI-PN)</p>
<p><img src="/2020/11/16/DNN-Decomposition/7.png" alt="7" style="zoom:60%;"></p>
</li>
<li><p>Higher Priority to Negative Examples (TI-HP)</p>
<p><img src="/2020/11/16/DNN-Decomposition/8.png" alt="8" style="zoom:60%;"></p>
</li>
<li><p>Strong Negative Edges (TI-SNE)</p>
<p><img src="/2020/11/16/DNN-Decomposition/9.png" alt="9" style="zoom:60%;"></p>
</li>
</ul>
</li>
</ul>
<h3 id="步骤3：关注点模块化-Concern-Modularization-CM"><a href="#步骤3：关注点模块化-Concern-Modularization-CM" class="headerlink" title="步骤3：关注点模块化 Concern Modularization (CM)"></a>步骤3：关注点模块化 Concern Modularization (CM)</h3><p>这一步作用是将多个非关注点及其相应的神经元和边，抽象成一个输出层的节点（如下图的非0节点）：</p>
<p><img src="/2020/11/16/DNN-Decomposition/10.png" alt="10" style="zoom:50%;"></p>
<ul>
<li><p>Channeling (CM-C)：将最后一层输出到非关注点的边，通过取权重的平均值，都改向到一个”非“节点上</p>
<p><img src="/2020/11/16/DNN-Decomposition/11.png" alt="11" style="zoom:60%;"></p>
</li>
<li><p>Remove Irrelevant Edges (CM-RIE)：在Channeling之前，去掉倒数第二层仅对非关注点有贡献的边及其相关神经元</p>
<p><img src="/2020/11/16/DNN-Decomposition/12.png" alt="12" style="zoom:60%;"></p>
<p><img src="/2020/11/16/DNN-Decomposition/13.png" alt="13" style="zoom:70%;"></p>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h3><ul>
<li>数据集：<ul>
<li>MNIST</li>
<li>EMNSIT（手写英文字母）：只用A-J（10个字母）训练</li>
<li>FMNIST：衣服</li>
<li>KMNIST：日语字母</li>
</ul>
</li>
<li>模型：<ul>
<li>4个数据集分别训练带1、2、3、4个隐藏层的全连接神经网络，每个隐藏层49个神经元</li>
</ul>
</li>
<li>测试准则<ul>
<li>准确率：衡量DNN模型模块化后的准确率，对输入的样本用分解后的每个模块进行预测，将给出positive预测结果且置信度最高并的子模型预测结果作为预测类别（投票）。</li>
<li>Jaccard系数（JI）：衡量模块之间的相似度，将所有权重和偏置放在一个向量中，比较Jaccard系数。</li>
</ul>
</li>
</ul>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul>
<li><p>实验1：分解后的模块有效性如何？</p>
<p><img src="/2020/11/16/DNN-Decomposition/14.png" alt="14" style="zoom:80%;"></p>
<ul>
<li>TI-HP方法：JI最低，但准确率同样很低<ul>
<li>因为先更新负类样本权重，再更新正类样本权重，负类样本权重被覆盖</li>
</ul>
</li>
<li>TI-PN方法：多个模型上低准确率<ul>
<li>先更新正类样本权重，再更新负类样本权重，JI较高，因为负类样本权重重合较多</li>
</ul>
</li>
<li>TI-SNE方法：准确率最高</li>
<li>CM-RIE相比CM-C降低了JI系数，且在准确率上表现较好。</li>
<li>准确率不变或有提升的模型中：33.79%的边是失效的，说明这些模型与原模型并不是完全一样。</li>
<li>调整了每层神经元的个数至70个之后，CM-RIE方法效果几乎一致。</li>
</ul>
</li>
<li><p>实验2：模块化后的DNN支持<strong>重用</strong>吗？</p>
<ul>
<li><p>数据集内部重用</p>
<ul>
<li><p>从原模型模型构造两类别模型（共C(10,2)=45种情况)：用这两类数据重训练的相同结构的模型作为对比。结果显示与重训练的模型准确率差不多。</p>
<p><img src="/2020/11/16/DNN-Decomposition/15.png" alt="15" style="zoom:67%;"></p>
<p><img src="/2020/11/16/DNN-Decomposition/17.png" alt="17" style="zoom:50%;"></p>
</li>
</ul>
</li>
<li><p>数据集之间重用</p>
<ul>
<li>绝大多数模型有一定的精度下降</li>
</ul>
<p><img src="/2020/11/16/DNN-Decomposition/18.png" alt="18" style="zoom:50%;"></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>实验3：模块化后的DNN支持<strong>替换</strong>吗？</p>
<ul>
<li><p>同数据集模型替换</p>
</li>
<li><p>不同数据集模型替换</p>
<p><img src="/2020/11/16/DNN-Decomposition/19.png" alt="19" style="zoom:60%;"></p>
</li>
</ul>
</li>
</ul>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul>
<li><p>应用场景设想很新颖。</p>
</li>
<li><p>文章笔误太多，算法解释得不清楚。</p>
</li>
<li><p>直接对模型权重进行操纵的做法很大胆，实验用的模型都是全联接的小型模型，怀疑在其他大型模型上的可行性。</p>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN分解</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】DeepCT</title>
    <url>/2020/08/20/DeepCT/</url>
    <content><![CDATA[<p>原文：DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems （SANER’19 ERA Track Paper)  <a id="more"></a></p>
<p>代码：没找到</p>
<h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>提出了DNN系统的组合测试方法，根据目标要覆盖的神经元激活模式组合，从种子输入开始，通过search-based testing, 或guided random testing, 或symbolic constraint solving based testing方法生成满足条件的输入，同时最小化L∞距离。以生成覆盖更多的神经元激活模式组合的输入。</p>
<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><ul>
<li>为什么层析（tomographic）？<ul>
<li>DNN学习到的特征随着层数增加越来越复杂、抽象</li>
</ul>
</li>
<li>为什么组合（combinatorial）？<ul>
<li>每层神经元只与前后两层发生交互。逻辑单元之间可能存在逻辑(无形)的相互作用，其中当前层的神经元共同决定其下一层神经元的逻辑。我们想用CT捕捉和检查每一层神经元之间的这些无形的相互作用。</li>
</ul>
</li>
</ul>
<h2 id="组合测试标准"><a href="#组合测试标准" class="headerlink" title="组合测试标准"></a>组合测试标准</h2><h3 id="神经元激活配置（Neuron-activation-configuration"><a href="#神经元激活配置（Neuron-activation-configuration" class="headerlink" title="神经元激活配置（Neuron-activation configuration)"></a>神经元激活配置（Neuron-activation configuration)</h3><p>对第$i$层的一组神经元$M=\{n_1,n_2,…,n_k\}$，激活配置为一个元组$c=(b_1,b_2,…,b_k)$，其中$b_i \in \{0,1\}$。若测试集$T$中存在样本$t$能使得激活模式等于$c$，则称$T$可以覆盖$c$。</p>
<p>$\Theta(t,L_i)$: 第$i$层神经元所有$t$-路覆盖组合</p>
<p>$\theta \in \Theta(t,L_i)$：是$t$个神经元的集合，拥有$2^t$种激活配置</p>
<p>$\Theta_{Full}(t,L_i,T) \subseteq \Theta(t,L_i)$：能被$T$完全覆盖的$t$-路覆盖组合</p>
<h3 id="t-路组合稀疏覆盖"><a href="#t-路组合稀疏覆盖" class="headerlink" title="$t$-路组合稀疏覆盖"></a>$t$-路组合稀疏覆盖</h3><p>$T$的组合稀疏覆盖=覆盖了所有神经元配置的组合数 / 所有组合数</p>
<p>例子：</p>
<p><img src="/2020/08/20/DeepCT/fig1.png" alt="fig1" style="zoom:60%;"></p>
<p>$L_i$层中共有4个神经元${n1,n2,n3,n4}$，<br>2-路组合共有六种：$\{n1,n2\}，\{n1,n3\}，\{n1,n4\}，\{n2,n3\}，\{n2,n4\}，\{n3,n4\}$<br>每个2-路组合有四种神经元激活配置$(0，0)、(0，1)、(1，0)$和$(1，1)$<br>在6个双向组合中，T只覆盖了$\{n1，n2\}，\{n1，n4\}，\{n2，n3\}$和$\{n3，n4\}$的四种神经元激活配置(只有这四个组合出现了四种神经元激活配置)<br>则Li层的2-路组合稀疏覆盖率$= 4 / 6= 66.6\%$</p>
<p>由于$t$-路组合稀疏覆盖不能考虑每个神经元组合内的覆盖，接下来我们引入t-way组合稠密覆盖。</p>
<h3 id="t-路组合稠密覆盖"><a href="#t-路组合稠密覆盖" class="headerlink" title="$t$-路组合稠密覆盖"></a>$t$-路组合稠密覆盖</h3><p>$T$的组合稀疏覆盖= 被覆盖了的激活配置数 / 所有组合的所有激活配置数目</p>
<p>例子：同上图</p>
<p>由于$L_i$中有六个神经元的双向组合，并且每个组合有四个神经元激活配置,总共有24种激活配置</p>
<p>测试集T可以覆盖20种配置，未覆盖的神经元激活配置为$\{n1，n3\}=(0，1)，\{n1，n3\}=(1，0)，\{n2，n4\}=(0，1)，\{n2，n4\}=(1，0)$,因此，T的双向组合密集覆盖率为83.3%。</p>
<h3 id="p-t-完备性"><a href="#p-t-完备性" class="headerlink" title="$(p,t)$完备性"></a>$(p,t)$完备性</h3><p>达到$p$(百分比)覆盖的组合比例</p>
<p>例子：同上图</p>
<p>在神经元的双向组合中，$\{n1，n2\}，\{n1，n4\}，\{n2，n3\}，\{n3，n4\}$的覆盖双向构型比为100%，$\{n1，n3\}$和$\{n2，n4\}$的覆盖双向构型比为50%。则$L_i$的(0.5，2)-完备性为100%，$L_i$的(1，2)-完备性为66.6%。即达到100%覆盖的有4个,达到50%以上覆盖的有6个。</p>
<h2 id="DNN的鲁棒性测试"><a href="#DNN的鲁棒性测试" class="headerlink" title="DNN的鲁棒性测试"></a>DNN的鲁棒性测试</h2><h3 id="d-局部鲁棒性"><a href="#d-局部鲁棒性" class="headerlink" title="$d$-局部鲁棒性"></a>$d$-局部鲁棒性</h3><script type="math/tex; mode=display">\forall x':\parallel x'-x \parallel \leq d \Rightarrow C(x) = C(x')</script><p>约束：$L_{\infty}-norm$</p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/2020/08/20/DeepCT/al.png" alt="al" style="zoom:67%;"></p>
<ul>
<li>输入<ul>
<li>DNN</li>
<li>$t$:测试粒度，每个组合内神经元个数</li>
<li>组合测试标准</li>
<li>初始种子集合</li>
</ul>
</li>
<li>输出<ul>
<li>正确样本集合</li>
<li>对抗样本集合</li>
</ul>
</li>
<li>测试用例生成方法<ul>
<li>没有特殊规定，<font color="red">基于搜索、覆盖率引导的随机测试、符号分析/约束求解都可以</font></li>
<li>本文实验：DNN使用ReLU激活函数，约束求解方法（Cplex solver）生成测试用例<ul>
<li><font color="red">即将CT覆盖目标编码为目标的线性约束，使种子输入的$L_{\infty}$-范数扰动距离最小。</font>



</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul>
<li>数据集和模型<ul>
<li>MNIST和两个预训练好的DNN模型DNN1和DNN2（结构和精度略）</li>
</ul>
</li>
<li>从原测试集随机选择1000个样本作为初始种子，<font color="red">且这1000个都能被原DNN正确分类</font></li>
<li>随机测试（Random Testing)<ul>
<li><font color="red">随机生成10000个测试用例（如何随机生成的？）</font>，然后检测健壮性问题</li>
<li>实验结果：RT已经能够检测到DNN1上的194个样本和DNN2上的178个样本的健壮性问题，总共266个独特的问题，其中106个是DNN1和DNN2上共同的问题。</li>
</ul>
</li>
<li>DeepCT测试<ul>
<li>1000个样本中去掉266个随机测试已经发现问题的样本，剩余的734个样本中随机采样50个进一步分析$d$-局部鲁棒性（$d$设为0.15，使用2-路CT标准）</li>
<li>实验结果：<ul>
<li>生成图片数量相当的情况下，DeepCT达到的覆盖率远超随机测试</li>
<li>随着测试层数和生成的测试用例增加，覆盖率也逐渐增加</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/2020/08/20/DeepCT/fig2.png" alt="fig2" style="zoom:60%;"></p>
<h2 id="可控制参数-变量总结"><a href="#可控制参数-变量总结" class="headerlink" title="可控制参数/变量总结"></a>可控制参数/变量总结</h2><ul>
<li>$t$：覆盖粒度</li>
<li>$d$：鲁棒性约束</li>
<li>$L_{\infty}$约束的参数</li>
<li>初始种子集合</li>
<li>$p$：完备性指标</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://blog.csdn.net/qq_33935895/article/details/105454414">https://blog.csdn.net/qq_33935895/article/details/105454414</a></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>测试输入生成</tag>
        <tag>测试标准</tag>
        <tag>组合测试</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】DeepCheck</title>
    <url>/2020/08/20/DeepCheck/</url>
    <content><![CDATA[<p>原文：Symbolic Execution for Deep Neural Networks （ISSRE’18)  <a id="more"></a></p>
<p>代码：没找到</p>
<h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>提出DNN上的符号执行测试方法。将DNN转换为程序（激活函数转换成if-else），利用相关性识别出重要像素。约束求解器求解激活模式相同但预测标签不同的输入（改变重要像素的值）。</p>
<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><h3 id="传统软件测试的符号执行"><a href="#传统软件测试的符号执行" class="headerlink" title="传统软件测试的符号执行"></a>传统软件测试的符号执行</h3><p>软件测试中的符号执行主要目标是: 在给定的探索尽可能多的、不同的程序路径(program path)。对于每一条程序路径，(1) 生成一个具体输入的集合(主要能力)；(2) 检查是否存在各种错误，包括断言违规、未捕获异常、安全漏洞和内存损坏。使用符号执行分析一个程序时，该程序会使用符号值作为输入，而非一般执行程序时使用的具体值。在达到目标代码时，分析器可以得到相应的路径约束，然后通过约束求解器来得到可以触发目标代码的具体值。</p>
<h3 id="在DNN上使用符号执行的挑战"><a href="#在DNN上使用符号执行的挑战" class="headerlink" title="在DNN上使用符号执行的挑战"></a>在DNN上使用符号执行的挑战</h3><ul>
<li>DNN没有分支 </li>
<li>DNN高度非线性，没有约束求解器 </li>
<li>扩展性差：DNN神经元数量庞大，远远超出了当前符号推理工具的能力。</li>
</ul>
<p>我们的方法：</p>
<ul>
<li>将ReLU激活函数看作if-else语句，这样DNN就有了分支</li>
</ul>
<h2 id="符号执行框架"><a href="#符号执行框架" class="headerlink" title="符号执行框架"></a>符号执行框架</h2><p>翻译DNN：</p>
<ul>
<li><p>将ReLU函数转换为分支语句，则一个神经元可以看做代码段：</p>
<p><img src="/2020/08/20/DeepCheck/Fig2.png" alt="Fig2" style="zoom:60%;"></p>
</li>
</ul>
<h2 id="识别重要像素"><a href="#识别重要像素" class="headerlink" title="识别重要像素"></a>识别重要像素</h2><p>维护一个相关性数组$coef^h_{i,j}$：$h$层第$i$个神经元与输入中第$j$个像素的相关性</p>
<ul>
<li><p>初始化：若$i=j$，$coef^0_{i,j}=1$，其他为0 </p>
</li>
<li><p>迭代计算coef：</p>
<p><img src="/2020/08/20/DeepCheck/Fig3.png" alt="Fig3" style="zoom:60%;"></p>
</li>
<li><p>最终输出单元的Coef计算结果为：</p>
<p><img src="/2020/08/20/DeepCheck/Eq1.png" alt="Eq1" style="zoom:80%;"></p>
<p>从像素$x_j$到输出$y_i$的所有路径（输出非0）上所有权重的总和（依赖于给定输入集合的具体执行）</p>
</li>
</ul>
<p>然后我们计算每个输入变量重要分数，使用三个指标:</p>
<ul>
<li>$co$: 相关性分数，即$C_{i,j}$</li>
<li>$coi$：相关性分数乘以对应的输入变量：$C_{i,j} \times x_j$</li>
<li>$abs$：相关性分数的绝对值$|C_{i,j}|$</li>
</ul>
<h2 id="确定攻击像素"><a href="#确定攻击像素" class="headerlink" title="确定攻击像素"></a>确定攻击像素</h2><p>构造约束求解问题：改变像素的值，使得新图片与原图<strong>激活模式相同</strong>，但<strong>预测标签不同</strong>。使用约束求解器Z3求解。</p>
<ul>
<li><p>激活模式相同：($H$为神经网络中所有激活函数的个数，即每个神经元激活模式都保持不变)</p>
<p>$ PC= \land _{h=1}^{H}(B^{h}+ \sum _{i=1}^{t}C_{i}^{h} \cdot X_{i} \gamma 0) $ , 其中$ \gamma \in \left\{ &gt; , \leq\right\} $ </p>
</li>
<li><p>预测标签不同：($l’ \neq l$)</p>
<script type="math/tex; mode=display">AC= \land _{j=1,j\neq l'}^{n}f_{j}(X)<f_{l^{ \prime }}(X)</script></li>
<li><p>约束：$ RA= \land _{1}^{t} lo  \leqslant X_i \leqslant hi $  ($lo$和$hi$是输入值标准化后的上下界)</p>
</li>
<li><p>最终约束求解器目标：$PC \land AC \land RA$</p>
</li>
<li><p>攻击</p>
<ul>
<li>1像素攻击</li>
<li>2像素攻击：对前5%的重要像素的任意两两组合进行约束求解。</li>
</ul>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验配置"><a href="#实验配置" class="headerlink" title="实验配置"></a>实验配置</h3><ul>
<li>数据集和模型<ul>
<li>MNIST</li>
<li>模型结构：784×10×10×10×10</li>
<li>训练集：60000张图片</li>
<li>准确率：92%</li>
</ul>
</li>
<li>选择训练集里的10张图片（0-9）作为原始图片种子 </li>
</ul>
<h3 id="实验1：重要像素识别效果"><a href="#实验1：重要像素识别效果" class="headerlink" title="实验1：重要像素识别效果"></a>实验1：重要像素识别效果</h3><ul>
<li>Top 5% 和10%重要像素识别结果<ul>
<li>总的来说，符号执行的使用能够识别重要的像素，这有助于解释神经网络的分类决策。</li>
</ul>
</li>
</ul>
<p><img src="/2020/08/20/DeepCheck/table1.png" alt="table1" style="zoom:60%;"></p>
<h3 id="实验2：用符号执行进行1或2像素攻击的效果"><a href="#实验2：用符号执行进行1或2像素攻击的效果" class="headerlink" title="实验2：用符号执行进行1或2像素攻击的效果"></a>实验2：用符号执行进行1或2像素攻击的效果</h3><ul>
<li>对1像素攻击：首先穷举每个像素（共784个）来确认其是否可攻击<font color="red">（改变像素值后激活模式不变，但预测标签改变）</font>，10张图片的可攻击像素如下：</li>
</ul>
<p><img src="/2020/08/20/DeepCheck/fig5.png" alt="fig5" style="zoom:67%;"></p>
<ul>
<li><p>1-像素攻击：10个数字可攻击像素的个数和最小可攻击像素的ID（即穷举搜索需要尝试的个数）</p>
<p><img src="/2020/08/20/DeepCheck/table2.png" alt="table2" style="zoom:70%;"></p>
</li>
<li><p>2-像素攻击：</p>
<ul>
<li>许多2像素攻击由一个可攻击1像素的像素组成。然而，一些新的攻击对不包括任何像素可攻击的像素被发现为1像素攻击。</li>
</ul>
<p><img src="/2020/08/20/DeepCheck/table3.png" alt="table3"></p>
</li>
</ul>
<h3 id="实验3：比较识别出的重要像素和攻击使用的像素"><a href="#实验3：比较识别出的重要像素和攻击使用的像素" class="headerlink" title="实验3：比较识别出的重要像素和攻击使用的像素"></a>实验3：比较识别出的重要像素和攻击使用的像素</h3><p><img src="/2020/08/20/DeepCheck/table5.png" alt="table5" style="zoom:60%;"></p>
<ul>
<li>表V显示了对于每幅图像，在发现1像素攻击之前，三个指标中的每一个都必须探索的重要像素的最小数量。对于所有指标和所有图像，需要检查的重要像素不超过前三分之一，以找到攻击像素。</li>
<li>coi识别效果最好。</li>
<li>总的来说，利用本文重要像素识别算法，对于寻找1像素和2像素攻击具有重要作用。</li>
</ul>
<h2 id="可控制参数及变量总结"><a href="#可控制参数及变量总结" class="headerlink" title="可控制参数及变量总结"></a>可控制参数及变量总结</h2><ul>
<li>重要像素百分比（如5%、10%）</li>
<li>初始种子集合：10张图片</li>
<li>重要像素判断指标：$co,coi,abs$</li>
<li>攻击像素个数</li>
</ul>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>本文将DNN转换成程序从而获得执行路径的方法或许值得借鉴。</p>
<p>但开销过大，只能在小型数据集和模型上使用。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>符号执行</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】DeepCover</title>
    <url>/2020/08/24/DeepCover/</url>
    <content><![CDATA[<p>原文：Testing Deep Neural Networks （TECS’19）<a id="more"></a></p>
<p>（与DeepConcolic作者相同）</p>
<p>代码地址：<a href="https://github.com/TrustAI/DeepCover">https://github.com/TrustAI/DeepCover</a></p>
<h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>受MC/DC思想启发提出4种覆盖标准，使用线性规划模型进行约束求解（借助DeepConcolic的方法），生成满足“独立影响”条件且变化较小的测试用例。</p>
<h2 id="传统MC-DC覆盖"><a href="#传统MC-DC覆盖" class="headerlink" title="传统MC/DC覆盖"></a>传统MC/DC覆盖</h2><p>MC/DC是DO-178B Level A认证标准中规定的，欧美民用航空器强制要求遵守该标准。MC/DC覆盖测试<font color="red">在每个判定中的每个条件都曾独立影响判定的结果至少一次（独立影响意思是在其他条件不变的情况下，改变一个条件）</font>。</p>
<p>举个例子，制作咖啡需要同时满足壶、杯子和咖啡豆的条件：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>( kettle &amp;&amp; cup &amp;&amp; coffee ) {</span><br><span class="line">  <span class="keyword">return</span> cup_of_coffee;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">else</span> {</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>三个条件的取值共有8种情况：</p>
<p><img src="/2020/08/24/DeepCover/Fig2.png" alt="Fig2" style="zoom:80%;"></p>
<p>但仅4种情况（Test 4、6、7、8）就可以达到100%MC/DC覆盖率，因为：</p>
<ul>
<li>Tests 4 &amp; 8 ：Kettle可以独立影响结果</li>
<li>Tests 6 &amp; 8 ：Mug可以独立影响结果</li>
<li>Tests 7 &amp; 8：Coffe可以独立影响结果</li>
</ul>
<p><img src="/2020/08/24/DeepCover/Fig1.jpg" alt="Fig1" style="zoom:30%;"></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="DNN中的决策和条件"><a href="#DNN中的决策和条件" class="headerlink" title="DNN中的决策和条件"></a>DNN中的决策和条件</h3><ul>
<li>$\Psi_k$: 一个集合，其中每个元素是神经网络第$k$层节点的一个子集合，表示一个<strong>特征</strong><ul>
<li>核心思想：不仅要测试某个特征的存在，而且要测试简单特征对更复杂特性的影响。</li>
</ul>
</li>
<li>$t_k = |\Psi_k|$：特征个数</li>
<li>$\psi_{k,l} (1 \leq l \leq t_k)$：第$l$个特征</li>
<li>每个特征代表一个<strong>决策（decision）</strong>，其<strong>条件（conditions）</strong>是前一层与其相连的特征</li>
<li><p>特征的使用将DNN中的基本构建单位从<strong>单个节点</strong>推广到<strong>一组节点</strong>。</p>
</li>
<li><p>特征对$(\psi_{k,i},\psi_{k+1,j})$：相邻层的一对特征</p>
</li>
<li>符号变化<ul>
<li>$sc(\psi_{k,l},x_1,x_2)$：对$\psi_{k,l}$中的任意神经元 $n_{k,j}$，$sign(n_{k,j},x_1)\ne sign(n_{k,j},x_2)$</li>
<li>$nsc(\psi_{k,l},x_1,x_2)$：对$\psi_{k,l}$中的任意神经元 $n_{k,j}$，$sign(n_{k,j},x_1) = sign(n_{k,j},x_2)$</li>
</ul>
</li>
<li>值变化<ul>
<li>$vc(g,\psi_{k,l},x_1,x_2)$：$g(\psi_{k,l},x_1,x_2)=true$，$g$是一个值函数</li>
</ul>
</li>
</ul>
<h3 id="覆盖方法"><a href="#覆盖方法" class="headerlink" title="覆盖方法"></a>覆盖方法</h3><ul>
<li><p>Sign-Sign Coverage (SSC)</p>
<ul>
<li><p>对一个特征对$\alpha = (\psi_{k,i},\psi_{k+1,j})$，$\alpha$被两个测试用例$x_1$、$x_2$SS-覆盖，记为$SS(\alpha,x_1,x_1)$。其定义为：</p>
<ul>
<li>$sc(\psi_{k,i},x_1,x_2)$ 且$nsc(P_k  \backslash \psi_{k,i},x_1,x_2)$，其中$P_k$为第$k$层所有节点的集合</li>
<li>$sc(\psi_{k+1,j},x_1,x_2)$</li>
</ul>
<font color="red">（即一个条件特征量$\psi_{k,i}$的符号变化独立影响下一层决策特征$\psi_{k+1,j}的$符号变化)</font>
</li>
</ul>
</li>
<li><p>Value-Sign Coverage (VSC)</p>
<ul>
<li>对一个特征对$\alpha = (\psi_{k,i},\psi_{k+1,j})$，值函数$g$，$\alpha$被两个测试用例$x_1$、$x_2$VS-覆盖，记为$VS^g(\alpha,x_1,x_1)$。其定义为：<ul>
<li>$vc(g,\psi_{k,i},x_1,x_2)$ 且$nsc(P_k ,x_1,x_2)$，其中$P_k$为第$k$层所有节点的集合</li>
<li>$sc(\psi_{k+1,j},x_1,x_2)$</li>
</ul>
</li>
</ul>
</li>
<li><p>Sign-Value Coverage (SVC)</p>
<ul>
<li><p>对一个特征对$\alpha = (\psi_{k,i},\psi_{k+1,j})$，值函数$g$，$\alpha$被两个测试用例$x_1$、$x_2$SV-覆盖，记为$SV^g(\alpha,x_1,x_1)$。其定义为：</p>
<ul>
<li>$sc(\psi_{k,i},x_1,x_2)$ 且$nsc(P_k  \backslash \psi_{k,i},x_1,x_2)$，其中$P_k$为第$k$层所有节点的集合</li>
<li>$vc(g,\psi_{k+1,j},x_1,x_2)$且$nsc(\psi_{k+1,j},x_1,x_2)$</li>
</ul>
<font color="red">(捕获符号更改情况的决策特征的重大更改)</font>
</li>
</ul>
</li>
<li><p>Value-Value Coverage (VVC)</p>
<ul>
<li><p>对一个特征对$\alpha = (\psi_{k,i},\psi_{k+1,j})$，值函数$g_1$、$g_2$，$\alpha$被两个测试用例$x_1$、$x_2$SV-覆盖，记为$VV^{g_1,g_2}(\alpha,x_1,x_1)$。其定义为：</p>
<ul>
<li>$vc(g_1,\psi_{k,i},x_1,x_2)$ 且$nsc(P_k ,x_1,x_2)$，其中$P_k$为第$k$层所有节点的集合</li>
<li>$vc(g_2,\psi_{k+1,j},x_1,x_2)$且$nsc(\psi_{k+1,j},x_1,x_2)$</li>
</ul>
<font color="red">(条件特征没有符号变化，但决策特征的值发生显著变化)</font>

</li>
</ul>
</li>
</ul>
<h3 id="覆盖率计算"><a href="#覆盖率计算" class="headerlink" title="覆盖率计算"></a>覆盖率计算</h3><p>令$F= \left\{ SS,VS^{g},SV^{g},VV^{g_{1},g_{2}} \right\}$，给定DNN$N$和覆盖方法$f \in F$，测试特征对集合$O$，测试集$T$的覆盖率：</p>
<script type="math/tex; mode=display">M_{f}(N,T)= \frac{| \left\{ \alpha \in O| \exists x_{1},x_{2} \in T:f( \alpha ,x_{1},x_{2}) \right\} |}{|O|}</script><p>即被覆盖的测试特征对所占比例。</p>
<h3 id="与现有覆盖标准的强弱关系"><a href="#与现有覆盖标准的强弱关系" class="headerlink" title="与现有覆盖标准的强弱关系"></a>与现有覆盖标准的强弱关系</h3><p><img src="/2020/08/24/DeepCover/Fig3.png" alt="Fig3" style="zoom:60%;"></p>
<h3 id="自动测试用例生成"><a href="#自动测试用例生成" class="headerlink" title="自动测试用例生成"></a>自动测试用例生成</h3><ul>
<li>测试预言<ul>
<li>$X$为一组被正确标记的测试用例，$x \in X$，若$x’$与$x$足够接近且预测标签一致，则称$x’$通过测试预言</li>
</ul>
</li>
<li><strong>通过线性编程（LP）生成测试用例</strong><ul>
<li>方法同DeepConcolic</li>
</ul>
</li>
<li><strong>通过启发式搜索生成测试用例</strong><ul>
<li>基于梯度下降搜索覆盖特定特征对的输入对</li>
</ul>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul>
<li>MNIST、CIFAR-10、ImageNet</li>
</ul>
<h3 id="可控制参数及变量"><a href="#可控制参数及变量" class="headerlink" title="可控制参数及变量"></a>可控制参数及变量</h3><ul>
<li>LP调用时使用的约束：$\parallel x_2 - x_1 \parallel _ \infty$</li>
<li>$SV^g$中：$ g= \frac{u_{k+1},j \left[ x_{2} \right] }{u_{k+1},j \left[ x_{1} \right] } \geqslant \sigma $ ，$\sigma = 2$</li>
<li>$VV^{g_1,g_2}$中：$\sigma = 5$</li>
<li>每个特征包含神经元个数<ul>
<li>大小用参数$w$来调节：$\psi _{k,i} \leq w \cdot s_k$</li>
</ul>
</li>
</ul>
<h3 id="MNIST上的实验结果"><a href="#MNIST上的实验结果" class="headerlink" title="MNIST上的实验结果"></a>MNIST上的实验结果</h3><ul>
<li>一个特征即一个神经元</li>
<li>训练10个DNN（准确率&gt;97%)</li>
</ul>
<p><img src="/2020/08/24/DeepCover/Fig4.png" alt="Fig4" style="zoom:67%;"></p>
<ul>
<li>DNN错误查找结果：<ul>
<li>测试用例生成算法有效地实现了对所有覆盖标准的高覆盖，</li>
<li>覆盖方法被认为是有用的，因为找出了很多对抗样本。</li>
</ul>
</li>
<li>DNN安全分析：<ul>
<li>覆盖率$M_f$和对抗性实例百分比$AE_f$一起提供了评估DNN的定量统计数据。一般来说，给定一个测试集，一个具有高覆盖率水平$M_f$和低对抗百分比$AE_f$的DNN被认为是鲁棒的。</li>
<li>下图展示了对抗样本的距离和累积对抗样本数的关系。一个更稳健的DNN将在小距离末端(接近0)有更低的形状，因为报告的敌对的例子相对于他们原始的正确输入是更远的。直觉上，这意味着需要付出更多的努力来愚弄一个稳健的DNN，使其从正确的分类变成错误的标签。</li>
</ul>
</li>
</ul>
<p><img src="/2020/08/24/DeepCover/Fig5.png" alt="Fig5" style="zoom:60%;"></p>
<ul>
<li>逐层的行为：<ul>
<li>当深入DNN时，神经元对的覆盖会变得更加困难。在这种情况下，为了提高覆盖性能，在生成测试对时需要使用较大的数据集。图5b给出了在不同层中发现的敌对示例的百分比(在所有敌对示例中)。有趣的是，大多数对抗性的例子似乎都是在测试中间层时发现的。</li>
</ul>
</li>
</ul>
<p><img src="/2020/08/24/DeepCover/Fig6.png" alt="Fig6" style="zoom:60%;"></p>
<ul>
<li><p>高权重的SSC：</p>
<ul>
<li>为了减少测试特征对的数量，仅选择带有较高权重的一些神经元作为特征，改变前后二者的区别不大，因此实际使用时可以采用这种方式减少开销。</li>
</ul>
<p><img src="/2020/08/24/DeepCover/Fig7.png" alt="Fig7" style="zoom:60%;"></p>
</li>
<li><p>调用LP的开销</p>
<ul>
<li>对于每个DNN，我们选择一组神经元对，其中每个决策神经元位于不同的层。然后，我们测量变量和约束的数量，以及在解决每个LP调用上花费的时间(以秒计算)。表3中的结果证实了部分激活模式的LP模型确实是轻量级的，并且在遍历一个DNN的更深层时，其复杂度以线性方式增加。</li>
</ul>
</li>
</ul>
<p><img src="/2020/08/24/DeepCover/Fig8.png" alt="Fig8" style="zoom:60%;"></p>
<h3 id="CIFAR10上的实验结果"><a href="#CIFAR10上的实验结果" class="headerlink" title="CIFAR10上的实验结果"></a>CIFAR10上的实验结果</h3><ul>
<li>在不失一般性的前提下，卷积层节点的激活是通过激活一个子集的先例节点来计算的，每个节点都属于其层中的一个feature map。我们将算法1中的启发式测试用例生成应用于SS覆盖率，并在每个不同的层分别度量决策特征的覆盖率结果。</li>
<li>总的来说，SS覆盖率高于90%是通过相当一部分的敌对示例实现的。</li>
<li>一个有趣的观察是,<font color="red">更深层的因果变化的特性能够检测小扰动的输入导致对抗的行为</font>,而这有可能为开发人员提供有用的反馈调试或优化神经网络参数。</li>
</ul>
<p><img src="/2020/08/24/DeepCover/Fig9.png" alt="Fig9" style="zoom:60%;"></p>
<h3 id="ImageNet上的实验结果"><a href="#ImageNet上的实验结果" class="headerlink" title="ImageNet上的实验结果"></a>ImageNet上的实验结果</h3><ul>
<li><p>VGG16+启发式测试用例生成算法</p>
</li>
<li><p>特征：一组神经元</p>
<ul>
<li>大小用参数$w$来调节：$\psi _{k,i} \leq w \cdot s_k$，$s_k$为该层神经元总数</li>
</ul>
</li>
<li><p>我们测试了2000个随机特征对的SS覆盖情况，$w \in \{0.1\%、0.5\%、1.0\%\}$。</p>
<ul>
<li><p>覆盖结果：10.5%，13.6%和14.6%是对抗样本。对抗样本的平均距离和标准偏差：</p>
<p><img src="/2020/08/24/DeepCover/Fig10.png" alt="Fig10" style="zoom:60%;"></p>
</li>
<li><p>结果表明，特征对与输入扰动之间存在一定的关系。在生成的对抗样本中，<font color="red">更细粒度的特征比粗糙的特征能够捕获更小的扰动。</font></p>
</li>
</ul>
</li>
<li><p><font color="red">我们注意到访问边界激活值很可能要求对DNNs进行更大的更改。</font>我们设置功能大小使用$w$= 10%,获得的测试集中有22.7%的对抗样本。然而，这些敌对的例子的距离，平均L∞-norm距离3.49，标准差3.88，远远大于SS覆盖的距离</p>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>测试标准</tag>
        <tag>MC/DC</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】DeepConcolic</title>
    <url>/2020/08/19/DeepConcolic/</url>
    <content><![CDATA[<p>原文：Concolic Testing for Deep Neural Networks （ASE’18) <a id="more"></a></p>
<p>代码地址：<a href="https://github.com/TrustAI/DeepConcolic">https://github.com/TrustAI/DeepConcolic</a> （利普希茨覆盖测试部分用MATLAB写的）</p>
<h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><ul>
<li>提出了第一种DNN的Concolic测试方法。通过具体执行获得激活模式，通过反转神经元的值，优化得到变换后的输入（类似约束求解器）。使测试集达到高覆盖率。</li>
<li><p>提出利普希茨常数覆盖标准。</p>
</li>
<li><p>与其他方法比较：</p>
</li>
</ul>
<p><img src="/2020/08/19/DeepConcolic/compare.png" alt="compare"></p>
<font color="red">（DeepTest中的Jaccard距离并不是用来约束图片距离的，只是比较激活神经元的差异）</font>

<h2 id="形式化定义"><a href="#形式化定义" class="headerlink" title="形式化定义"></a>形式化定义</h2><h3 id="激活模式"><a href="#激活模式" class="headerlink" title="激活模式"></a>激活模式</h3><p>$ap[t]_{k,l} \in \{true,false\}$ 表示输入$t$的第$k$层第$l$个神经元是否被$ReLU$函数激活。</p>
<h3 id="覆盖要求"><a href="#覆盖要求" class="headerlink" title="覆盖要求"></a>覆盖要求</h3><p>略，公式较多但比较好理解，逻辑形式表达覆盖标准。</p>
<h3 id="覆盖要求的可满足性"><a href="#覆盖要求的可满足性" class="headerlink" title="覆盖要求的可满足性"></a>覆盖要求的可满足性</h3><p>给定一组测试用例$T$和覆盖要求$r$  ，$T \vDash r$表示测试用例满足了覆盖要求。</p>
<h3 id="覆盖要求的复杂度"><a href="#覆盖要求的复杂度" class="headerlink" title="覆盖要求的复杂度"></a>覆盖要求的复杂度</h3><p>检查$T \vDash r$需要能在多项式时间内完成。</p>
<h3 id="覆盖率"><a href="#覆盖率" class="headerlink" title="覆盖率"></a>覆盖率</h3><p>给定一个覆盖要求的集合$R$,测试用例集$T$可满足要求的数量所占比例即覆盖率。</p>
<h2 id="特定覆盖要求介绍"><a href="#特定覆盖要求介绍" class="headerlink" title="特定覆盖要求介绍"></a>特定覆盖要求介绍</h2><p>本节用上面的方法形式化定义了目前学术界已有的一些覆盖标准。</p>
<h3 id="利普希茨连续条件（Lipschitz-Continuity）"><a href="#利普希茨连续条件（Lipschitz-Continuity）" class="headerlink" title="利普希茨连续条件（Lipschitz Continuity）"></a>利普希茨连续条件（Lipschitz Continuity）</h3><p>对一个神经网络$N$，若存在$c\geq0$，使得对输入样本集合$D_{L1}$中的任意两个样本$x_1$、$x_2$，满足：</p>
<script type="math/tex; mode=display">\parallel v[x_1]_1-v[x_2]_1 \parallel \leq c \cdot \parallel x_1-x_2 \parallel</script><p>其中$v[x]_1$表示<font color="red">输入层</font>神经元的激活向量（？），$c$为利普希茨常数。满足条件的最小的$c$称为最佳利普希茨常数$c_{best}$。</p>
<p>利普希茨覆盖：是一组覆盖要求的集合，要求体现了给定输入子空间中的任意两个输入$x_1$、$x_2$满足上述利普希茨连续条件的情况</p>
<h3 id="神经元覆盖率（NC）"><a href="#神经元覆盖率（NC）" class="headerlink" title="神经元覆盖率（NC）"></a>神经元覆盖率（NC）</h3><p>DeepXplore中定义的神经元覆盖率，形式化表示为一组覆盖标准：</p>
<p>$\{\exists x.ap[x]_{k,i}=true|2\leq k \leq K-1,1 \leq i \leq s_k\}$</p>
<p>即对每个神经元提出一个覆盖要求——存在输入x能将其激活。</p>
<h3 id="MC-DC覆盖率（SS-coverage）"><a href="#MC-DC覆盖率（SS-coverage）" class="headerlink" title="MC/DC覆盖率（SS coverage）"></a>MC/DC覆盖率（SS coverage）</h3><p>DeepCover提出，略</p>
<h3 id="神经元边缘覆盖率（NBC）"><a href="#神经元边缘覆盖率（NBC）" class="headerlink" title="神经元边缘覆盖率（NBC）"></a>神经元边缘覆盖率（NBC）</h3><p>DeepGauge中提出，略</p>
<h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><h3 id="算法概览"><a href="#算法概览" class="headerlink" title="算法概览"></a>算法概览</h3><p><img src="/2020/08/19/DeepConcolic/overview.png" alt="overview" style="zoom:80%;"></p>
<p><img src="/2020/08/19/DeepConcolic/算法.png" alt="算法" style="zoom:60%;"></p>
<ul>
<li>输入<ul>
<li>一个DNN $N$</li>
<li>一个输入样本$t_0$</li>
<li>一组覆盖要求$R$</li>
<li>启发式$\delta$：<font color="red">与选择的覆盖要求有关，使得第7行能尽可能找到一组容易满足的要求</font></li>
</ul>
</li>
<li>输出：<ul>
<li>一组测试样本$T$，最初只包含$t_0$</li>
</ul>
</li>
<li><p>validity_check函数：检查$t’$是否有效</p>
</li>
<li><p>当$R$中所有要求被满足或不能满足$R$中的其他要求时算法停止</p>
</li>
</ul>
<h3 id="要求评估（requirement-evaluation函数，具体执行部分"><a href="#要求评估（requirement-evaluation函数，具体执行部分" class="headerlink" title="要求评估（requirement_evaluation函数，具体执行部分)"></a>要求评估（requirement_evaluation函数，具体执行部分)</h3><ul>
<li><p>寻找一组$(t,r)$，使得$t$变换成$t’$后最有可能满足要求$r$。</p>
</li>
<li><p>定义$arg max_x a:e$ ：满足布尔表达式$e$的所有样本里，使得算数表达式$a$最大的$x$</p>
</li>
<li><p>启发式：略，得到使得覆盖要求表达式（即前面的四种覆盖率）值取到最大的样本</p>
<ul>
<li>利普希茨连续</li>
<li>NC</li>
<li>SSC</li>
<li>NBC</li>
</ul>
</li>
</ul>
<h3 id="符号分析（symbolic-analysis函数）"><a href="#符号分析（symbolic-analysis函数）" class="headerlink" title="符号分析（symbolic_analysis函数）"></a>符号分析（symbolic_analysis函数）</h3><p>根据$(t,r)$变换$t$生成$t’$，有三种方法：</p>
<h4 id="1、利用线性编程进行符号分析"><a href="#1、利用线性编程进行符号分析" class="headerlink" title="1、利用线性编程进行符号分析"></a>1、利用线性编程进行符号分析</h4><p>将DNN实例$N(x)$映射到可以使用线性编程(LP)建模的激活模式$ap[x]$，并根据不同覆盖准则定义新的激活模式$ap’[x]$:</p>
<ul>
<li>NC：反转某神经元$n_{k,i}$的值，保持第$k$层之前的层的输出不变，因为该神经元只受之前层的影响。之后层的输出不用管。</li>
<li>SSC：反转$n_{k+1,j}$和$n_{k,i}$</li>
<li>NBC：目标使得神经元$n_{k,i}$的激活值超过其上界或低于其下界</li>
</ul>
<p>LP模型将对$ap’[x]$进行编码并求解满足要求$r$的输入$t’$</p>
<h4 id="2、基于全局优化的符号分析"><a href="#2、基于全局优化的符号分析" class="headerlink" title="2、基于全局优化的符号分析"></a>2、基于全局优化的符号分析</h4><p>基于全局优化算法达到上述目标生成$t’$</p>
<h4 id="3、Lipschitz测试用例生成"><a href="#3、Lipschitz测试用例生成" class="headerlink" title="3、Lipschitz测试用例生成"></a>3、Lipschitz测试用例生成</h4><p>本文提出了一种新颖的交替式罗盘搜索方案（略）</p>
<h3 id="测试预言（约束）"><a href="#测试预言（约束）" class="headerlink" title="测试预言（约束）"></a>测试预言（约束）</h3><p>给定实数$b$，如果测试用例$t’ \in T$满足：</p>
<script type="math/tex; mode=display">\parallel t-t' \parallel \leq b</script><p>则称$t’$为有效的。若$t$与$t’$的预测结果一致，则称DNN通过了测试预言。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul>
<li>时间限制：12h</li>
<li>所有覆盖结果均运行10次以上取平均</li>
</ul>
<h3 id="实验1：与DeepXplore对比"><a href="#实验1：与DeepXplore对比" class="headerlink" title="实验1：与DeepXplore对比"></a>实验1：与DeepXplore对比</h3><ul>
<li><p>数据集：MNIST、CIFAR-10</p>
</li>
<li><p>实验方法</p>
<ul>
<li>从随机采样的初始种子集合开始</li>
<li>DeepXplore要求多个模型差分测试：采用目标测试的DNN模型+DeepXplore论文里的2个模型</li>
<li>比较二者达到的覆盖率</li>
</ul>
</li>
<li><p>实验结果</p>
<p><img src="/2020/08/19/DeepConcolic/ex1.png" alt="ex1" style="zoom:70%;"></p>
<ul>
<li>DeepConcolic能达到比DeepXplore更高的覆盖率</li>
<li>但DeepXplore更快，几秒内运行结束<font color="red">（没有说DeepConcolic的运行时间）</font></li>
<li>生成的图像：<font color="red">(几乎完全反相也符合约束？)</font></li>
</ul>
<p><img src="/2020/08/19/DeepConcolic/ex1-2.png" alt="ex1-2" style="zoom:67%;"></p>
</li>
</ul>
<h3 id="实验2：比较NC-SCC和NBC"><a href="#实验2：比较NC-SCC和NBC" class="headerlink" title="实验2：比较NC, SCC和NBC"></a>实验2：比较NC, SCC和NBC</h3><ul>
<li><p>实验方法</p>
<ul>
<li>NC：从一张初始种子开始</li>
<li>SCC和NBC：初始集合包含1000张图片，且仅测试一部分神经元<font color="red">（和NC差别很大）</font></li>
<li>设置$L _\infty$的距离上界为0.3，$L_0$的距离上界为100个像素</li>
</ul>
</li>
<li><p>实验结果</p>
<p><img src="/2020/08/19/DeepConcolic/MyBlog\source\_posts\DeepConcolic\ex2-1.png" alt="ex2-1" style="zoom:70%;"></p>
<p><img src="/2020/08/19/DeepConcolic/ex2-3.png" alt="ex2-3"></p>
<ul>
<li><p>使用全局优化进行符号分析的开销太高。因此，具有$L_0$范数的SSC结果被排除在外。<font color="red">(?右图没有蓝色条形)</font></p>
</li>
<li><p>总体而言，DeepConcolic实现了高覆盖率，并使用健壮性检查检测到大量的对抗性示例。然而，NBC的覆盖范围是有限的</p>
</li>
<li><p>concolic测试可以发现距离极小的对抗样本：如1255≈0.0039（$L_{\infty}$范数），1像素（$L_0$范数）</p>
</li>
<li><p>对抗样本的平均距离：(对于同一网络，当距离度量改变时，使用NC发现的对抗性示例的数量可能会有很大变化。在设计DNN的覆盖标准时，需要使用各种距离度量来检查它们。)</p>
<p><img src="/2020/08/19/DeepConcolic/ex2-2.png" alt="ex2-2" style="zoom:67%;"></p>
</li>
</ul>
</li>
</ul>
<h3 id="实验3：利普希茨常数检验结果"><a href="#实验3：利普希茨常数检验结果" class="headerlink" title="实验3：利普希茨常数检验结果"></a>实验3：利普希茨常数检验结果</h3><p><img src="/2020/08/19/DeepConcolic/ex3-1.png" alt="ex3-1" style="zoom:80%;"></p>
<ul>
<li>我们通过随机测试生成的方式产生了100万个测试对，但最大Lipschitz转换率只达到了3.23，并且大多数测试对都在[0.01，2]的范围内。另一方面，我们的Concolic方法可以覆盖[0.01，10.38]的Lipschitz范围，其中大多数情况位于[3.5，10]，而随机测试生成很难覆盖这一范围。</li>
<li>目标：覆盖较大Lipschitz常数范围。如对于自动驾驶汽车等安全关键应用，Lipschitz常数较大的DNN本质上表明它更容易受到对手的扰动。因此，可以覆盖较大Lipschitz常数的测试方法为训练的DNN提供了有用的鲁棒性指标。我们认为，对于DNNs的安全测试，Lipschitz常数覆盖的Concolic测试方法可以补充现有的方法来实现更好的覆盖。</li>
</ul>
<h2 id="可控制参数和变量总结"><a href="#可控制参数和变量总结" class="headerlink" title="可控制参数和变量总结"></a>可控制参数和变量总结</h2><ul>
<li>初始样本集合</li>
<li>覆盖标准：利普希茨常数覆盖、NC、NBC、SSC</li>
<li>变换约束条件（$L_\infty$、$L_0$及其参数）</li>
<li>测试的神经元（全部还是部分）</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>测试输入生成</tag>
        <tag>Concolic测试</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】DeepGauge</title>
    <url>/2020/08/26/DeepGauge/</url>
    <content><![CDATA[<p>原文：DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems （ASE’18）<a id="more"></a></p>
<p>介绍网址：<a href="https://deepgauge.github.io/">https://deepgauge.github.io/</a></p>
<h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>提出基于深度神经网络的主功能区、边界区、层级三类覆盖率标准。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>$\phi (x,n)$：输入$x$在神经元$n$上的输出值</p>
<h3 id="神经元级别的覆盖率"><a href="#神经元级别的覆盖率" class="headerlink" title="神经元级别的覆盖率"></a>神经元级别的覆盖率</h3><ul>
<li><p><strong>K-multisection Neuron Coverage (KMNC)</strong></p>
<ul>
<li><p>主功能区：设一个神经元$n$在训练集的下界为$low_n$，上界为$high_n$，主功能区为$[low_n,high_n]$</p>
</li>
<li><p>将主功能区$[low_n,high_n]$均分为$k$等份，每份为$S_i^n$，则该神经元在测试集$T$上的覆盖率：</p>
<script type="math/tex; mode=display">\frac{ \left\{ S_{i}^{n}| \exists x \in T: \phi (x,n) \in S_{i}^{n} \right\} )}{k}</script></li>
<li><p>对所有神经元，KMNC定义为：（即所有神经元取平均）</p>
<p>$ KMNCov(T,k)= \frac{ \sum _{n \in N}| \left\{ S_{i}^{n}| \exists x \in T: \phi (x,n) \in S_{i}^{n} \right\} |}{k \times |N|}$ </p>
</li>
</ul>
</li>
<li><p><strong>Neuron Boundary Coverage（NBC）</strong></p>
<ul>
<li><p>边界区：$ (- \infty , low_n) \cup (high_n,+ \infty ) $ </p>
</li>
<li><p>NBC定义为所有神经元边缘被覆盖的比例：</p>
<script type="math/tex; mode=display">NBCov(T)= \frac{|UpperCornerNeuron|+|LowerCornerNeuron| }{2 \times |N|}</script></li>
</ul>
</li>
<li><p><strong>Strong Neuron Activation Coverage (SNAC)</strong></p>
<ul>
<li><p>这些极度活跃的神经元可能在神经网络中传递有用的学习模式</p>
</li>
<li><p>SNAC只计算上边界覆盖率：</p>
<script type="math/tex; mode=display">SNACov(T)= \frac{|UpperCornerNeuron|}{|N|}</script></li>
</ul>
</li>
</ul>
<h3 id="层级别的覆盖率"><a href="#层级别的覆盖率" class="headerlink" title="层级别的覆盖率"></a>层级别的覆盖率</h3><ul>
<li><p><strong>Top-k Neuron Coverage (TKNC)</strong></p>
<ul>
<li><p>表示所有神经元中有多少曾经做过top-k（存在某个$x$使得其激活值在该层属于top-k）</p>
<p>$ TKNCov(T,k)= \frac{|U_{x \in T}(U_{1 \leqslant i \leqslant 1}top_{k}(x,i))|}{|N|} $ </p>
</li>
</ul>
</li>
<li><p><strong>Top-k Neuron Patterns</strong></p>
<ul>
<li><p>给定一个测试输入$x$，每一层的top-k神经元序列也形成一个模式:</p>
<script type="math/tex; mode=display">TKNPat(T,k)=| \left\{ (top_{k}(x,1), \ldots ,top_{k}(x,l))|x \in T \right\} |</script></li>
</ul>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul>
<li>数据集和模型</li>
</ul>
<p><img src="/2020/08/26/DeepGauge/1.png" alt="1" style="zoom:80%;"></p>
<ul>
<li><p>对抗样本生成算法</p>
<ul>
<li>FGSM、BIM、JSMA、CW</li>
</ul>
</li>
<li><p>实验步骤</p>
<ul>
<li><p>对于MNIST：</p>
<ul>
<li><p>对于每个模型，生成10000张对抗样本和原10000张测试集图片混合在一起</p>
</li>
<li><p>参数设置</p>
<ul>
<li>$\sigma$:方差</li>
</ul>
<p><img src="/2020/08/26/DeepGauge/2.png" alt="2" style="zoom:80%;"></p>
</li>
<li><p>总共：3 (models)×5 (datasets)×14 (criterion settings) = 210 evaluation configurations</p>
</li>
</ul>
</li>
<li><p>对于ImageNet：</p>
<ul>
<li>随机选择5000张测试图片</li>
<li>总共：2 (models)×4 (datasets)×14 (criterion settings) = 112 experimental configurations<ul>
<li>（JSMA因为开销问题无法运行，dataset少一个）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul>
<li>覆盖率的增加表明对抗样本测试数据总体上探索了新的DNNs的内部状态，其中一些未被原始测试覆盖。</li>
<li>主功能区和边界区均可能出错</li>
<li>覆盖率提升意味着错误检测能力提升</li>
<li>测试数据更多覆盖主功能区域</li>
<li>低边界区比高边界区更难覆盖</li>
</ul>
<h3 id="与神经元覆盖率（NC）比较"><a href="#与神经元覆盖率（NC）比较" class="headerlink" title="与神经元覆盖率（NC）比较"></a>与神经元覆盖率（NC）比较</h3><ul>
<li>NC难以捕捉对抗样本和原测试集样本的区别</li>
<li>NC使用<strong>相同的阈值</strong>作为所有神经元的激活评价。但是，我们发现不同神经元的输出统计分布差异很大。给定一个用于分析的测试套件，一些神经元的输出可能表现出相当小的方差，但平均值很大，而另一些神经元可能表现出很大的方差，但平均值很低。<br>因此，对所有神经元使用相同的阈值而不考虑神经元功能分布的差异会大大降低精度。例如，给定一个具有非常小的平均值和标准偏差的神经元，即使用户指定的阈值稍微大一点，通常也会确定该神经元不能被覆盖。</li>
<li>NC对神经元取值进行了<strong>标准化（归约到[0,1])</strong>，所以相同的激活值在不同数据集代表了不同的意义（因为每个数据集的max和min不同）。</li>
</ul>
<p><img src="/2020/08/26/DeepGauge/3.png" alt="3" style="zoom:80%;"></p>
<h2 id="可控制变量及参数"><a href="#可控制变量及参数" class="headerlink" title="可控制变量及参数"></a>可控制变量及参数</h2><ul>
<li>KMNC和TKNC的$k$</li>
<li>NBC和SNAC可选增加参数$\sigma$(在分析过程中神经元输出的标准方差)</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>测试标准</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】DeepHunter</title>
    <url>/2020/08/20/DeepHunter/</url>
    <content><![CDATA[<p>原文：DeepHunter: A Coverage-Guided Fuzz Testing Framework for Deep Neural Networks （ISSTA’19)</p>
<p>Coverage-guided Fuzzing for Feedforward Neural Networks (ASE’19)  <a id="more"></a></p>
<p>代码地址：<a href="https://bitbucket.org/xiaofeixie/deephunter/src/master/">https://bitbucket.org/xiaofeixie/deephunter/src/master/</a></p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul>
<li><p>输入和可控变量：</p>
<ul>
<li>初始种子集合</li>
<li>种子选择策略：<ul>
<li>①随机</li>
<li>②根据新鲜程度选择</li>
<li>③平衡新鲜和多样<ul>
<li>带有参数$\gamma$、$p_{min}$</li>
</ul>
</li>
</ul>
</li>
<li>蜕变变换方法<ul>
<li>8种图像变换方法<ul>
<li>4种像素级别变换（像素值改变）：反相、亮度、模糊、噪声</li>
<li>4种仿射变换（移动像素位置）：平移、缩放、裁剪、旋转</li>
</ul>
</li>
<li>约束策略中的$\alpha$、$\beta$参数</li>
</ul>
</li>
<li>K：常数，变换次数</li>
<li>覆盖率标准：5种（NC、KMNC、NBC、SNAC、TKNC）标准及其附带参数</li>
</ul>
</li>
<li><p>输出：1、能最大化覆盖率的预测正确的样例 2、预测错误的样例</p>
<p><img src="/2020/08/20/DeepHunter/1.png" alt="1" style="zoom:60%;"></p>
<h3 id="保持蜕变变换语义不变的策略"><a href="#保持蜕变变换语义不变的策略" class="headerlink" title="保持蜕变变换语义不变的策略"></a>保持蜕变变换语义不变的策略</h3><ul>
<li><p>8种图像变换方法</p>
<ul>
<li>4种像素级别变换（像素值改变）：反相、亮度、模糊、噪声</li>
<li>4种仿射变换（移动像素位置）：平移、缩放、裁剪、旋转</li>
</ul>
</li>
<li><p>假设一次变换语义不变（参数合适的情况下）。</p>
</li>
<li><font color="red">策略：只允许一次仿射变换，像素变换可以使用多次，但要进行约束</font>

<ul>
<li><p>约束条件：像素变换应满足f(s,s’)的条件，$L_0$表示发生变化的像素数目的最大值，$L_\infty$表示像素值变化的最大值。即：要么发生变化的像素数不多，要么变化的像素数多，但像素值变化都不大。平衡二者</p>
<p><img src="/2020/08/20/DeepHunter/4.png" alt="4" style="zoom:70%;"></p>
</li>
</ul>
</li>
<li><p>reference image：一张图像在经过一系列变换后（最多一次仿射变换），计算f(s,s’)。如果没有仿射变换的话，参考图像为原图；否则参考图像为中间经过一次仿射变换后的图像。</p>
<ul>
<li><p>如果有仿射变换，则$L_0$、$L_\infty$计算方法如下</p>
<p><img src="/2020/08/20/DeepHunter/3.png" alt="3" style="zoom:60%;"></p>
</li>
</ul>
</li>
<li><p>种子选择策略（三种）</p>
<ul>
<li><p>传统程序和TensorFuzz、DeepTest：用栈，选最新生成的种子</p>
</li>
<li><p>uniform：随机选一个种子</p>
</li>
<li><p>新策略Probability：平衡新鲜性和多样性，种子s被选择的概率：</p>
<p><img src="/2020/08/20/DeepHunter/4.png" alt="4" style="zoom:60%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集和DNN模型"><a href="#数据集和DNN模型" class="headerlink" title="数据集和DNN模型"></a>数据集和DNN模型</h3><p><img src="/2020/08/20/DeepHunter/5.png" alt="5" style="zoom:60%;"></p>
<h3 id="共同参数设置"><a href="#共同参数设置" class="headerlink" title="共同参数设置"></a>共同参数设置</h3><ul>
<li><p>NC阈值：0.75（与DeepXplore相同）</p>
</li>
<li><p>KMNC的K=1000</p>
</li>
<li><p>SNAC、NBC、TKNC与DeepGauge相同设置</p>
</li>
<li><p>TRY_NUM=50</p>
</li>
<li><p>$\gamma=20$，$p_{min}=0.5$ </p>
</li>
<li>$K=20$</li>
</ul>
<h3 id="RQ1（蜕变变换）：从人类角度看，不同的变换及约束策略对生成和原图片相同语义的新图效果如何？"><a href="#RQ1（蜕变变换）：从人类角度看，不同的变换及约束策略对生成和原图片相同语义的新图效果如何？" class="headerlink" title="RQ1（蜕变变换）：从人类角度看，不同的变换及约束策略对生成和原图片相同语义的新图效果如何？"></a>RQ1（蜕变变换）：从人类角度看，不同的变换及约束策略对生成和原图片相同语义的新图效果如何？</h3><ul>
<li><p>实验设计</p>
<ul>
<li><p>比较三种变换及约束策略：<font color="red">（仅约束策略还是连带变换策略？没说清楚）</font></p>
<ul>
<li><p>DeepHunter：用f(s,s’)限制，设置$\alpha=0.02，\beta=0.2$</p>
</li>
<li><p>TensorFuzz： 用 $L_\infty=0.4$限制</p>
</li>
<li><p>DeepTest： 用保守的参数限制(原文的MSE限制仅限回归任务，所以没用)</p>
</li>
</ul>
</li>
<li><p>每个数据集随机选择30个种子输入生成5000张图片，3个数据集*3种生成方法*生成5000张图片=共45000张图片</p>
</li>
<li><p>9个测试者每人看一组图片：如果生成的图片与原图分类不同、或是没有语义则被标记为invalid。</p>
</li>
</ul>
</li>
<li><p>实验结果</p>
<p><img src="/2020/08/20/DeepHunter/6.png" alt="10"></p>
<ul>
<li>对三种策略：CIFAR-10数据集的无效率普遍高于其他两个数据集<ul>
<li>原因：CIFAR-10分辨率较低，即使是DNN能识别的有效输入，对于人类常难以识别</li>
</ul>
</li>
<li>即使DeepTest的参数设置很保守，但是仍生成了很多无效输入；TensorFuzz经过L∞限制后无效输入少了很多</li>
<li>结论：DeepHunter的蜕变变换策略有效减少了生成无效图片的数量</li>
</ul>
</li>
</ul>
<h3 id="RQ2（覆盖率）：CGF是否在DNN测试领域仍能有效提升覆盖率？不同覆盖标准下，不同的seed生成策略对提升覆盖率效果如何？"><a href="#RQ2（覆盖率）：CGF是否在DNN测试领域仍能有效提升覆盖率？不同覆盖标准下，不同的seed生成策略对提升覆盖率效果如何？" class="headerlink" title="RQ2（覆盖率）：CGF是否在DNN测试领域仍能有效提升覆盖率？不同覆盖标准下，不同的seed生成策略对提升覆盖率效果如何？"></a>RQ2（覆盖率）：CGF是否在DNN测试领域仍能有效提升覆盖率？不同覆盖标准下，不同的seed生成策略对提升覆盖率效果如何？</h3><ul>
<li><p>RQ2&amp;3 实验设计</p>
<ul>
<li>比较5种seed选取策略<ul>
<li>Random testing (RT)without coverage guidance. 作为baseline，随机测试，无覆盖率作为向导</li>
<li>DeepHunter+Uniform (DH+UF) ：使用不同的覆盖率标准作为向导，随机选择种子顺序</li>
<li>DeepHunter+Probability (DH+Prob)：使用不同的覆盖率标准作为向导，用概率策略选择种子顺序</li>
<li>DeepTest seed selection strategy with coverage guidance：选最新的；如果一个种子生成的所有新样例都不能提升覆盖率，则该种子被移出队列。队列可能变空。</li>
<li>TensorFuzz seed selection strategy with coverage guidance：随机选一个种子和队列最后的5个种子，再从中随机选一个</li>
</ul>
</li>
<li><p>21个fuzzers：5 个覆盖准则 × 4 个seed选择策略 + 1 RT with no coverage guidance，使用的模型为MNIST和CIFAR-10的四个模型；每个fuzzer运行10遍取平均</p>
</li>
<li><p>初始种子1000个：被所有模型都正确分类的测试数据</p>
</li>
<li>每个fuzzer迭代次数相同（5000次），使用的蜕变变换策略相同<font color="red">（是什么？）</font></li>
</ul>
</li>
<li><p>实验结果</p>
<p><img src="/2020/08/20/DeepHunter/7.png" alt="7" style="zoom:60%;"></p>
</li>
<li><p>结论</p>
<ul>
<li><p>尽管基本结构与传统程序很不同，CGF方法与随机测试相比较还是能有效最大化DNN程序的覆盖率</p>
</li>
<li><p>与传统软件的fuzzer优先选择最新生成的用例作为种子不同，DNN testing中种子选择的优先策略（多样性）也很重要</p>
</li>
<li><p>不同覆盖率标准提升的难易程度不同，KMNC, TKNC, 和 NC提升较容易。NBC and SNAC关注corner cases，本身初始seed覆盖率就极低，提升较难</p>
</li>
</ul>
</li>
</ul>
<h3 id="RQ3（错误检测）：现有的覆盖标准在引导错误检测的效果上有何不同？使用不同seed选取标准检测错误的效果有何不同？不同seed选取标准检测出的错误行为有何不同？"><a href="#RQ3（错误检测）：现有的覆盖标准在引导错误检测的效果上有何不同？使用不同seed选取标准检测错误的效果有何不同？不同seed选取标准检测出的错误行为有何不同？" class="headerlink" title="RQ3（错误检测）：现有的覆盖标准在引导错误检测的效果上有何不同？使用不同seed选取标准检测错误的效果有何不同？不同seed选取标准检测出的错误行为有何不同？"></a>RQ3（错误检测）：现有的覆盖标准在引导错误检测的效果上有何不同？使用不同seed选取标准检测错误的效果有何不同？不同seed选取标准检测出的错误行为有何不同？</h3><ul>
<li><p>实验结果</p>
<ul>
<li><p>检测错误数量</p>
<p><img src="/2020/08/20/DeepHunter/8.png" alt="8" style="zoom:60%;"></p>
<ul>
<li>DH+Prob和DH+UF检测错误数目多于另外三种策略，且模型较小时二者类似，模型较大时DF+Prob更优</li>
<li>KMNC在检测错误数目上少于其他4个覆盖率标准，因为cover更简单；但DH+Prob的表现还是优于其他策略</li>
<li>在最后的seed queue中，易于cover的覆盖率标准剩余的seed更多</li>
</ul>
</li>
<li><p>检测出的错误的多样性</p>
<p><img src="/2020/08/20/DeepHunter/9.png" alt="9" style="zoom:60%;"></p>
<ul>
<li>定义从同一个seed变换产生的错误属于同一类别，共1000类</li>
<li>DH+Prob和DH+UF能检测出更多类的错误，DeepTest和TensorFuzz检测出的错误甚至有的比RT还少，因为这二者都倾向于选择最新的seed</li>
<li>RT：过于随机，难以发现corner cases</li>
<li>选最新的seed：能发现Corner cases，但类别过于单一</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="RQ4（平台迁移）：DeepHunter是否适用于平台迁移过程中DNN量化引入的具体缺陷检测"><a href="#RQ4（平台迁移）：DeepHunter是否适用于平台迁移过程中DNN量化引入的具体缺陷检测" class="headerlink" title="RQ4（平台迁移）：DeepHunter是否适用于平台迁移过程中DNN量化引入的具体缺陷检测?"></a>RQ4（平台迁移）：DeepHunter是否适用于平台迁移过程中DNN量化引入的具体缺陷检测?</h3><ul>
<li><p>实验设计</p>
<ul>
<li><p>每个数据集选择一个模型，原模型都是32bit</p>
</li>
<li><p>3种量化方式</p>
<ul>
<li><p>随机选10%的权重缩减：32比特-&gt;16比特</p>
</li>
<li><p>随机选50%的权重缩减：32比特-&gt;16比特</p>
</li>
<li><p>所有权重缩减：32比特-&gt;16比特</p>
</li>
</ul>
</li>
<li><p>对前两种量化方式（10%和50%的量化模型）：sample10次得到10个模型，每个模型运行5遍取均值</p>
</li>
<li><p>为每个原始模型分配10小时生成图片，重复每种配置5次并平均结果</p>
</li>
</ul>
</li>
<li><p>实验结果</p>
<p><img src="/2020/08/20/DeepHunter/10.png" alt="10" style="zoom:60%;"></p>
<ul>
<li>corner相关的覆盖率标准更易发现量化错误</li>
<li>DNN模型越大，发现的错误越多</li>
<li>QR率越大，错误越多</li>
</ul>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>模糊测试</tag>
        <tag>测试输入生成</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】DeepImportance</title>
    <url>/2020/08/22/DeepImportance/</url>
    <content><![CDATA[<p>原文：Importance-Driven Deep Learning System Testing （ICSE’20)  <a id="more"></a></p>
<p>代码地址：<a href="https://github.com/DeepImportance/deepimportance_code_release">https://github.com/DeepImportance/deepimportance_code_release</a></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>提出了一种重要性驱动的覆盖标准，首先从训练集和预训练后的DNN模型中分析出重要神经元，然后将重要神经元的激活值聚类，计算测试集对重要神经元激活值簇的组合的覆盖比例，用于评价测试集的<font color="red">语义多样性</font>。</p>
<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>作者认为前人提出的覆盖标准（如DeepXplore和DeepGauge）：</p>
<ul>
<li>这些标准只是神经元(或神经元区域)的集合，它们的激活值符合一定的条件。通过只关注这些受约束的神经元属性而忽略整体DL系统行为，<font color="red">测试集和决策之间的因果关系是无信息性的。</font><ul>
<li>一个神经元可能有助于增强对其他类的信心，而不是正确的类，这是无法区分的。</li>
</ul>
</li>
<li>这些标准的实例化依赖于<font color="red">用户自定义的条件</font>(所选择神经元的区域、取值上限)，这些条件可能不能充分地表示DL系统的实际行为。</li>
<li>这些标准不能有效提供<font color="red">单个测试输入的贡献</font>。</li>
</ul>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p><img src="/2020/08/22/DeepImportance/Fig1.png" alt="Fig1"></p>
<p>步骤：</p>
<p>1、分析重要神经元</p>
<p>2、重要神经元激活值聚类</p>
<p>3、评价测试集的重要神经元覆盖率</p>
<h3 id="重要神经元分析"><a href="#重要神经元分析" class="headerlink" title="重要神经元分析"></a>重要神经元分析</h3><ul>
<li><p>逐层相关性传播算法（layer-wise relevance propagation）：</p>
<ul>
<li><p>全连接层中：每个神经元的相关性由下一层所有神经元的相关性计算得出：<font color="red">（分母为什么是$\sum i$ ??)</font></p>
<script type="math/tex; mode=display">R_{ij}= \sum _{k} \frac{ \phi (x,n_{ij})w_{ijk}}{ \sum _{i} \phi (x,n_{ij})w_{ijk}+ \epsilon }R_{i+1,k}</script><ul>
<li>相关性与激活值成正比：激活值越高的神经元相关性贡献越大</li>
<li>相关性与连接权重$w_{ijk}$成正比：通过更重要的连接，会产生更多的相关性</li>
</ul>
</li>
</ul>
</li>
<li><p>重要性计算算法</p>
<p><img src="/2020/08/22/DeepImportance/Fig2.png" alt="Fig2" style="zoom:70%;"></p>
<ul>
<li>第5行：通过前向传播获得最后一层输出值（softmax之前的值）</li>
<li>6-8行：反向传播计算相关性</li>
<li>第10行：分析函数分析所有输入的所有神经元的相关性得分，并根据优先级标准(例如，累积相关性，标准化相关性)对它们进行优先级排序（实验中我们使用累积相关性）</li>
<li>第11行：返回最重要的m个神经元</li>
</ul>
</li>
<li><p>相关性分析</p>
<ul>
<li>使用相关性来识别最重要的神经元是我们的方法的一个关键成分。基于最近<font color="red">DL系统可解释性的研究</font>，其目标是识别负责预测的输入部分，深度重要性的目标是识别最具影响力的神经元;这些是高风险的神经元，应该进行彻底的测试。尽管超出了这项工作的范围，我们也强调<strong>其他可解释性驱动技术</strong>可以用于鉴定最重要的神经元</li>
<li>与敏感性分析有很大区别：敏感性分析关注的是什么使<strong>已标记</strong>的样本(例如，一只狗)更多或更少地被归类为<strong>目标标签</strong>，而相关性分析研究的是什么使样本被归类为该标签。敏感度分数并不能真正解释为什么样本以某种方式被预测，而是解释输出在<font color="red">输入空间</font>的哪个方向最敏感<font color="red">（输入空间而不是神经元）</font>。相反，相关性得分表明哪些神经元/输入对分类是关键的。</li>
</ul>
</li>
</ul>
<h3 id="重要神经元聚类"><a href="#重要神经元聚类" class="headerlink" title="重要神经元聚类"></a>重要神经元聚类</h3><ul>
<li>动机<ul>
<li>由于每个神经元负责感知输入区域内的特定特征，我们认为，对于具有类似特征的输入，那些重要神经元的激活值集中在它们的<font color="red">值域中的特定区域</font>。非正式地说，这些区域形成一种模式，捕获DL系统中最具影响力的神经元的活动。</li>
<li>与DeepGauge中的KMNC相比较，我们的方法生成的簇对应每个神经元不同的<strong>语义特征</strong>。</li>
</ul>
</li>
<li>算法<ul>
<li>$k$-means聚类：我们将每个重要神经元的激活值划分为组(簇)，使同一组内的激活值与同一组内的其他激活值更相似，而与其他组内的激活值不相似。</li>
<li>如何确定$k$值？<ul>
<li>我们使用Silhouette index自动识别一种神经元特异性的最优策略，用于聚类每个重要神经元的激活值。（略）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="基于重要性的覆盖率（IDC）"><a href="#基于重要性的覆盖率（IDC）" class="headerlink" title="基于重要性的覆盖率（IDC）"></a>基于重要性的覆盖率（IDC）</h3><p>评价测试集对于重要神经元的激活值簇的组合的覆盖情况。</p>
<p>重要神经元激活值簇的组合定义为：</p>
<script type="math/tex; mode=display">INCC= \prod _{n \in D_{m}} \left\{ CENTROID( \Phi _{n}^{i})|\forall 1 \leqslant i \leqslant | \Phi _{n}| \right\}</script><p>IDC定义为：(即有多少组组合被覆盖)</p>
<p>$IDC(Y)= \frac{| \left\{ INCC(j)|\exists y \in Y:\forall V_{n}^{i} \in INCC(j) \bullet \min d( \phi (y,n),V_{n}^{i} \right\} |}{|INCC|}$</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集和模型"><a href="#数据集和模型" class="headerlink" title="数据集和模型"></a>数据集和模型</h3><p><img src="/2020/08/22/DeepImportance/Fig3.png" alt="Fig3" style="zoom:60%;"></p>
<h3 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h3><ul>
<li>神经元覆盖率NC：阈值为0.75</li>
<li>TKNC：$k=3$</li>
<li>KMNC：$k=1000$</li>
<li>NBC、SNAC：训练集激活值的最值为上下界</li>
<li>DSC、LSC：上界为2、2000，bucket数为1000</li>
<li>选择重要性神经元的层：<strong>倒数第二层</strong></li>
<li>$m \in \{6,8,10,12\}$</li>
<li>最大运行时间：3h</li>
<li>对抗样本生成方法：FGSM、BIM、JSMA、C&amp;W</li>
</ul>
<h3 id="RQ1-重要性-神经元重要性分析能识别出最重要的神经元吗"><a href="#RQ1-重要性-神经元重要性分析能识别出最重要的神经元吗" class="headerlink" title="RQ1(重要性):神经元重要性分析能识别出最重要的神经元吗?"></a>RQ1(重要性):神经元重要性分析能识别出最重要的神经元吗?</h3><ul>
<li>实验方法：使用[11]中可解释性工作的方法识别出前10%的重要像素，扰动这些像素的值（超过0.5则设为0，不足0.5设为1），测量随机选取的神经元和DeepImportance选取的重要神经元的值的改动情况，如下表所示。</li>
<li>实验结果：DeepImportance选取出的重要神经元变动幅度更大，说明这些神经元对给定输入的相关像素的变化更敏感。</li>
</ul>
<p><img src="/2020/08/22/DeepImportance/Fig4.png" alt="Fig4" style="zoom:30%;"></p>
<h3 id="RQ2-多样性-DeepImportance能有助于选择多样化的测试集吗"><a href="#RQ2-多样性-DeepImportance能有助于选择多样化的测试集吗" class="headerlink" title="RQ2(多样性):DeepImportance能有助于选择多样化的测试集吗?"></a>RQ2(多样性):DeepImportance能有助于选择多样化的测试集吗?</h3><ul>
<li><p>实验方法：</p>
<ul>
<li><p>$U_{DI}$: 在所有图片前2%重要像素（识别方法同上）上增加高斯白噪声扰动（MNIST 15个像素，CIFAR-10 20个像素，driving 200个像素）</p>
</li>
<li><p>$U_S$:和$U_{DI}$一样，但像素位置是随机选的</p>
<p><img src="/2020/08/22/DeepImportance/Fig5.png" alt="Fig5" style="zoom:60%;"></p>
</li>
</ul>
</li>
<li><p>实验结果</p>
<p><img src="/2020/08/22/DeepImportance/Fig6.png" alt="Fig6"></p>
<ul>
<li>对于IDC标准：$U_{O+DI}$在所有实验配置上覆盖率最高。表明IDC对决策任务中重要的输入特性更为敏感，而不是随机选择的特性。</li>
<li>随着$m$增加，IDC覆盖率逐渐降低，</li>
</ul>
</li>
</ul>
<h3 id="RQ3-有效性-DeepImportance在识别DL系统中的错误行为时的效果"><a href="#RQ3-有效性-DeepImportance在识别DL系统中的错误行为时的效果" class="headerlink" title="RQ3(有效性):DeepImportance在识别DL系统中的错误行为时的效果?"></a>RQ3(有效性):DeepImportance在识别DL系统中的错误行为时的效果?</h3><ul>
<li>生成对抗样本<ul>
<li>FGSM、BIM、JSMA、C&amp;W和RQ1中的白噪音数据集（$U_S$:方差为0.3,准确率97.4%）</li>
</ul>
</li>
</ul>
<p><img src="/2020/08/22/DeepImportance/Fig7.png" alt="Fig7" style="zoom:40%;"></p>
<ul>
<li>实验结果：<ul>
<li>与原始测试集$U_O$相比，所有DL系统的增强测试集的IDC覆盖率结果都有相当大的增长。</li>
<li>与高斯类噪声输入$U_{O+S}$相比，在含有对抗样本的测试集中，这种增长更为显著。</li>
<li>IDC对敌对的输入很敏感，并且在输入与之前遇到的输入在语义上不同的测试集中能够有效地检测出错误行为。</li>
</ul>
</li>
</ul>
<h3 id="RQ4-相关性-DeepImportance与现有覆盖率标准的相关性"><a href="#RQ4-相关性-DeepImportance与现有覆盖率标准的相关性" class="headerlink" title="RQ4(相关性):DeepImportance与现有覆盖率标准的相关性?"></a>RQ4(相关性):DeepImportance与现有覆盖率标准的相关性?</h3><ul>
<li>如上图。IDC显示了与DL系统其他覆盖标准相似的行为;因此，二者之间存在正相关关系。</li>
</ul>
<h3 id="RQ5-层灵敏度-特定神经元层的选择如何影响DeepImportance的行为"><a href="#RQ5-层灵敏度-特定神经元层的选择如何影响DeepImportance的行为" class="headerlink" title="RQ5(层灵敏度):特定神经元层的选择如何影响DeepImportance的行为?"></a>RQ5(层灵敏度):特定神经元层的选择如何影响DeepImportance的行为?</h3><p><img src="/2020/08/22/DeepImportance/Fig8.png" alt="Fig8" style="zoom:60%;"></p>
<ul>
<li>我们观察到，当分析在较深的层而不是浅层执行时，IDC值会增加</li>
<li>IDC对具有不同语义输入的测试集更敏感($U_{O+DI}$)</li>
<li>目标层的选择会影响IDC的结果。由于倒数第二层负责理解语义上重要的高级特性，我们认为它是使用IDC评估测试集充分性的合适选择。</li>
</ul>
<h2 id="可控制参数-变量"><a href="#可控制参数-变量" class="headerlink" title="可控制参数/变量"></a>可控制参数/变量</h2><ul>
<li>$m$：选择的重要神经元个数，决定了测试的粒度，$m$越大组合数爆炸式增长</li>
<li>选择重要性神经元的层</li>
</ul>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>这篇文章所采用的重要性神经元识别方法非原创（来自《On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation》）；覆盖率计算方法也很简单（组合测试）。亮点在于将DNN测试覆盖标准与语义可解释性相结合，这可能是未来的一个趋势。</p>
<p>疑问：有没有必要覆盖尽可能多的重要神经元激活值的组合？</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>测试标准</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】DeepTest</title>
    <url>/2020/08/20/DeepTest/</url>
    <content><![CDATA[<p>原文：DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars  (ICSE’18)  <a id="more"></a></p>
<p>代码地址：<a href="https://github.com/ARiSE-Lab/deepTest">https://github.com/ARiSE-Lab/deepTest</a></p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul>
<li><p>神经元覆盖率计算方式：卷积层与DeepXplore有所不同，<font color="red">输出特征图的平均值与激活阈值做比较</font></p>
</li>
<li><p>9种图像变换方式：</p>
<ul>
<li>线性变换：<ul>
<li>亮度：所有像素值加/减一个常数</li>
<li>对比度：所有像素值乘以一个常数</li>
</ul>
</li>
<li>仿射变换：平移、缩放、水平修剪、旋转 （模拟摄像头的移动）</li>
<li>卷积变换：<ul>
<li>模糊（4种：averaging, Gaussian, median, and bilateral）</li>
<li>雾雨（Adobe Photoshop）</li>
</ul>
</li>
</ul>
</li>
<li><p>覆盖率引导的贪心搜索变换叠加算法</p>
<p><img src="/2020/08/20/DeepTest/Fig1.png" alt="Fig1" style="zoom:75%;"></p>
<ul>
<li><p>蜕变关系约束</p>
<ul>
<li><p>对一张图片，蜕变后的预测角度$\theta_{ti}$与真实标记$θ_i$之间的差距应小于原始数据集平均MSE的λ倍</p>
<script type="math/tex; mode=display">(\theta_i-\theta_{ti}) \leq \lambda MSE_{orig}</script></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集及模型"><a href="#数据集及模型" class="headerlink" title="数据集及模型"></a>数据集及模型</h3><ul>
<li><p>数据集：Driving</p>
</li>
<li><p>模型：Udacity比赛排名第2、3、6的模型Rambo、Chauffeur、Epoch</p>
<p><img src="/2020/08/20/DeepTest/Fig2.png" alt="Fig2" style="zoom:80%;"></p>
</li>
</ul>
<h3 id="可控制参数或变量"><a href="#可控制参数或变量" class="headerlink" title="可控制参数或变量"></a>可控制参数或变量</h3><ul>
<li><p>maxFailedTries: 变换叠加时最大尝试次数</p>
</li>
<li><p>各变换参数及可选范围：</p>
<p><img src="/2020/08/20/DeepTest/Fig3.png" alt="Fig3" style="zoom:60%;"></p>
</li>
<li><p>$\lambda$: 控制蜕变关系约束</p>
</li>
<li><p>$\epsilon$：控制变换约束大小</p>
</li>
<li><p>神经元激活值</p>
</li>
</ul>
<h3 id="实验1：不同输入输出对是否覆盖不同神经元"><a href="#实验1：不同输入输出对是否覆盖不同神经元" class="headerlink" title="实验1：不同输入输出对是否覆盖不同神经元"></a>实验1：不同输入输出对是否覆盖不同神经元</h3><ul>
<li><p>实验方法：检查不同模型覆盖率与驾驶方向的Spearman相关性、覆盖率与驾驶角度的Wilcoxon非参数检验结果。（<font color="red">该实验设计是否合理？覆盖率增加，应对应的是输入或输出种类增加而非角度增大？？）</font></p>
</li>
<li><p>实验结果</p>
<ul>
<li>随着神经元覆盖率的增加，转向角度增加，反之亦然。即不同输出对应不同的神经元，神经元覆盖率可以很好地近似估计输入输出对的多样性。<font color="red">（相关系数结果并不明显？）</font></li>
<li>神经元覆盖率随导向方向的变化具有统计学意义($p&lt;2.2∗10^{−16}$时) ，有些子模型比其他子模型更负责改变方向。</li>
<li>总结：对于不同的输入输出对，神经元的覆盖范围很不同。因此，神经覆盖定向(NDG)测试策略可以帮助发现corner样本。</li>
</ul>
</li>
</ul>
<p><img src="/2020/08/20/DeepTest/Fig4.png" alt="Fig4" style="zoom:80%;"></p>
<h3 id="实验2：不同图像变换是否激活不同神经元"><a href="#实验2：不同图像变换是否激活不同神经元" class="headerlink" title="实验2：不同图像变换是否激活不同神经元"></a>实验2：不同图像变换是否激活不同神经元</h3><ul>
<li><p>实验2-1：从测试集随机选择1000张图片，对每张图片分别做7种变换（blur, brightness, contrast, rotation, scale, shear, and translation）以及尝试变换的多种参数，得到共70000张生成图片。在各个模型上运行这些图片，记录神经元激活情况。对任意两种变换的组合（如模糊vs旋转，旋转vs平移等等），设激活神经元集合分别为N1、N2，测量二者的差异大小（Jaccard距离）</p>
<ul>
<li>结果：（左图）：除了Chauffeur_LTSM模型之外，不同的变换激活的神经元有较大区别</li>
</ul>
</li>
<li><p>实验2-2：七种变换依次叠加在图片上，查看覆盖率提升情况</p>
<ul>
<li>结果（右图）：叠加每种变换后神经元覆盖率都增加了，<font color="red">说明每种变换都能激活不同神经元。（覆盖率提升是因为变换种类增多还是离原图距离越来越远？）</font></li>
</ul>
</li>
</ul>
<p><img src="/2020/08/20/DeepTest/Fig5.png" alt="Fig5" style="zoom:80%;"></p>
<ul>
<li><p>实验2-3：单个变换触发神经元的比例分布情况及平均神经元增加百分比情况</p>
<ul>
<li><p>结果：不同的图像变换以不同程度增加神经元的覆盖率。<font color="red">（下表的两行没看懂）</font></p>
<p><img src="/2020/08/20/DeepTest/Fig6.png" alt="Fig6" style="zoom:80%;"></p>
</li>
</ul>
</li>
</ul>
<h3 id="实验3：结合不同变换是否能进一步提升神经元覆盖率"><a href="#实验3：结合不同变换是否能进一步提升神经元覆盖率" class="headerlink" title="实验3：结合不同变换是否能进一步提升神经元覆盖率"></a>实验3：结合不同变换是否能进一步提升神经元覆盖率</h3><ul>
<li><p>实验方法：</p>
<ul>
<li>Baseline组：原始的100个种子输入</li>
<li>累积变换组：在100个种子输入上叠加7种变换的10种参数组合，得到7000张生成图片。</li>
<li>覆盖率引导的贪心搜索组：仅生成了254、221、864张图片（对应三个模型）</li>
</ul>
</li>
<li><p>实验结果：通过系统地结合不同的图像变换，神经元的覆盖率比原始种子图像的覆盖率提高了约100%。</p>
<p><img src="/2020/08/20/DeepTest/Fig7.png" alt="Fig7" style="zoom:80%;"></p>
</li>
</ul>
<h3 id="实验4：使用蜕变关系是否能检测到错误行为"><a href="#实验4：使用蜕变关系是否能检测到错误行为" class="headerlink" title="实验4：使用蜕变关系是否能检测到错误行为"></a>实验4：使用蜕变关系是否能检测到错误行为</h3><ul>
<li><p>实验4-1：生成图片与原图片和真实标记之间的偏差情况</p>
<p><img src="/2020/08/20/DeepTest/Fig8.png" alt="Fig8" style="zoom:60%;"></p>
<ul>
<li>结果：生成图片集合的MSE为0.41，原图片集合为0.035。因此生成的图片更有可能触发错误行为</li>
</ul>
</li>
<li><p>实验4-2：生成错误图片数量</p>
<ul>
<li><p>实验方法：</p>
<ul>
<li><p><strong>约束：为了防止图片变化过大或误报出现（如旋转后旋转角度应跟着变化），使用的变换（除了雾、雨）及其参数必须满足：</strong></p>
<p><script type="math/tex">|MSE(trans,param)-MSE_{org}|\leq \epsilon</script> （<font color="red">计算MSE用到了人工标记的oracle，在实际测试没有oracle时怎么办？</font>)</p>
</li>
</ul>
</li>
<li><p>实验结果：</p>
<ul>
<li><p>λ越大、$\epsilon$越小，错误数量越少（<font color="red">总生成图片数量没说？）</font></p>
<p><img src="/2020/08/20/DeepTest/Fig9.png" alt="Fig9" style="zoom:75%;"></p>
</li>
<li><p>对于某些转换，有些模型比其他模型更容易出现错误行为。（$\lambda=5，\epsilon=0.03$）</p>
<p><img src="/2020/08/20/DeepTest/Fig10.png" alt="Fig10" style="zoom:80%;"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>实验4-3：人工检查误报情况</p>
<ul>
<li>结果：误报较少</li>
</ul>
</li>
</ul>
<h3 id="实验5：使用生成图片重新训练DNN能否提高准确率"><a href="#实验5：使用生成图片重新训练DNN能否提高准确率" class="headerlink" title="实验5：使用生成图片重新训练DNN能否提高准确率"></a>实验5：使用生成图片重新训练DNN能否提高准确率</h3><ul>
<li><p>实验方法：用HMB_3的图片生成雾、雨图片，其中66%和原训练集一起重新训练Epoch模型，剩下34%做测试。<font color="red">（用的转换太少了）</font></p>
</li>
<li><p>实验结果：MSE降低。</p>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>模糊测试</tag>
        <tag>测试输入生成</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】DeepXplore</title>
    <url>/2020/08/19/DeepXplore/</url>
    <content><![CDATA[<p>原文：DeepXplore: Automated Whitebox Testing of Deep Learning Systems（SOSP’17）<a id="more"></a></p>
<p>代码地址：<a href="https://github.com/peikexin9/deepxplore">https://github.com/peikexin9/deepxplore</a></p>
<h2 id="可控制参数或变量"><a href="#可控制参数或变量" class="headerlink" title="可控制参数或变量"></a>可控制参数或变量</h2><ul>
<li><p>种子输入集合数量 (类别均衡，随机选择，<u>约束：所有DNN模型必须对每个种子的初始预测结果相同</u>)</p>
</li>
<li><p>多个DNN</p>
</li>
<li><p>$\lambda_1$：平衡其他DNN和被选中DNN预测的差别（$\lambda_1$越大，选中DNN预测为原类别c的概率越低，$\lambda_1$越小，越能维持其他DNN的预测结果）</p>
<p><img src="/2020/08/19/DeepXplore/1.png" alt="公式1"></p>
</li>
<li><p>$\lambda_2$：平衡两个优化目标大小（即预测差异和神经元覆盖率，$\lambda_2$越大，越关注覆盖更多神经元，否则更关注生成预测差异图片）</p>
</li>
</ul>
<p><img src="/2020/08/19/DeepXplore/2.png" alt="公式2"></p>
<ul>
<li>$s$: 梯度下降步长（$s$过大可能会导致极小值附近震荡，$s$过小导致迭代次数多）</li>
<li>$t$: 神经元覆盖率阈值（$t$越大，达到覆盖率目标越难）</li>
<li>$p$: 覆盖率目标</li>
<li>梯度下降迭代最大次数（代码里）</li>
<li><font color="red">在每次迭代的梯度$G$上施加的真实领域约束:</font><ul>
<li>（1）不同亮度模拟光照：$G$被$mean(G)$ 取代</li>
<li>（2）单个矩形($m*n$大小)模拟意外或故意遮挡：<ul>
<li>选择矩形左上顶点所在的位置$(i,j)$，将$G_{i:i+m,j:j+n}$施加于原图相应位置。</li>
<li><font color="red">$m$、$n$的大小和$(i,j)$为自定义的参数，迭代多次都在相同位置</font></li>
</ul>
</li>
<li>（3） 多个随机黑色小矩形模拟镜头污垢<ul>
<li>位置随机的若干个$m*m$大小的黑色矩形<font color="red">（每次迭代位置不同，$m$值自定义，与数据集图片大小有关）</font></li>
<li>如果$mean(G_{i:i+m,j:j+m})$，则图片不动，否则变黑（即只允许降低像素值）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="/2020/08/19/DeepXplore/算法.png" alt="算法" style="zoom:80%;"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集和模型"><a href="#数据集和模型" class="headerlink" title="数据集和模型"></a>数据集和模型</h3><p><img src="/2020/08/19/DeepXplore/dataset.png" alt="dataset"></p>
<h3 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h3><ul>
<li>种子输入集合：随机从测试集选择2000个输入（保持<font color="red">类别平衡</font>，满足所有DNN预测结果相同的约束）</li>
<li>参数$\lambda_1,\lambda_2,s,t$</li>
</ul>
<p><img src="/2020/08/19/DeepXplore/para.png" alt="para" style="zoom:67%;"></p>
<h3 id="实验1：神经元覆盖率的优势"><a href="#实验1：神经元覆盖率的优势" class="headerlink" title="实验1：神经元覆盖率的优势"></a>实验1：神经元覆盖率的优势</h3><h4 id="实验1-1：神经元覆盖率与代码覆盖率比较"><a href="#实验1-1：神经元覆盖率与代码覆盖率比较" class="headerlink" title="实验1-1：神经元覆盖率与代码覆盖率比较"></a>实验1-1：神经元覆盖率与代码覆盖率比较</h4><ul>
<li><p>实验方法：随机选择10个测试输入，测量代码覆盖率和神经元覆盖率大小</p>
</li>
<li><p>参数设置：设置$t=0.75$（神经元激活阈值，<font color="red">按比例缩放至[0,1]区间，不同DNN阈值区别很大？）前面都设的0，故意设大t凸显结果差异）</font></p>
</li>
<li><p>实验结果：少量测试输入即可达到100%代码覆盖率，但神经元覆盖率最多只有34%：</p>
<p><img src="/2020/08/19/DeepXplore/ex1-1.png" alt="ex1-1" style="zoom:80%;"></p>
</li>
</ul>
<h4 id="实验1-2：神经元覆盖率对DeepXplore生成能导致预测差异样本的作用"><a href="#实验1-2：神经元覆盖率对DeepXplore生成能导致预测差异样本的作用" class="headerlink" title="实验1-2：神经元覆盖率对DeepXplore生成能导致预测差异样本的作用"></a>实验1-2：神经元覆盖率对DeepXplore生成能导致预测差异样本的作用</h4><ul>
<li>实验方法：从MNIST测试集随机选择2000种子输入，分别设置$\lambda_2$为1和0（$\lambda_2=0$时优化目标里不考虑覆盖新的神经元）。测量生成预测差异样本（difference-inducing inputs）的多样性<strong>（平均$L_1$距离：改变前后像素值之差的和，<font color="red">是否适合作为衡量多样性的指标？</font>）</strong></li>
<li>参数设置：$t=0.25$</li>
<li>实验结果：神经元覆盖率帮助提升了生成样本的多样性<font color="red">（①神经元覆盖率NC提升并不明显？作者的解释是NC很高的情况下，NC的一点点提升就能带来多样性的大幅提升。②生成数量#Diffs减少了：作者的解释是设置$\lambda_2=1$后更倾向于生成不同输入而不是提高输入数量，作者认为生成数量并不是一个好的衡量生成样本的方式）</font></li>
</ul>
<p><img src="/2020/08/19/DeepXplore/ex1-2.png" alt="ex1-2" style="zoom:75%;"></p>
<h4 id="实验1-3：不同类别输入对激活神经元的影响"><a href="#实验1-3：不同类别输入对激活神经元的影响" class="headerlink" title="实验1-3：不同类别输入对激活神经元的影响"></a>实验1-3：不同<strong><em>类别</em></strong>输入对激活神经元的影响</h4><ul>
<li>实验方法：MNIST数据集的LeNet-5模型，运行100对相同类别（如类别8）图片，和100对不同类别（如类别8和类别4）图片，测量平均激活神经元个数和共同激活神经元的个数（overlap）。</li>
<li>实验结果：相同类别共同激活的平均神经元个数$&gt;$不同类别。神经元覆盖率可以有效地估计DNN测试中激活的不同规则的数量。</li>
</ul>
<p><img src="/2020/08/19/DeepXplore/ex1-3.png" alt="ex1-3" style="zoom:80%;"></p>
<h3 id="实验2：DeepXplore的效果"><a href="#实验2：DeepXplore的效果" class="headerlink" title="实验2：DeepXplore的效果"></a>实验2：DeepXplore的效果</h3><h4 id="实验2-1：DeepXplore在神经元覆盖率上的表现"><a href="#实验2-1：DeepXplore在神经元覆盖率上的表现" class="headerlink" title="实验2-1：DeepXplore在神经元覆盖率上的表现"></a>实验2-1：DeepXplore在神经元覆盖率上的表现</h4><ul>
<li><p>实验方法：使用三种方法（DeepXplore、对抗样本FGSM、原测试集随机选择）生成相同数量（测试集的1%）的输入，比较神经元覆盖率</p>
</li>
<li><p>实验结果：DeepXplore能达到更高的覆盖率，且随着$t$的提升，覆盖率逐渐降低。</p>
</li>
</ul>
<p><img src="/2020/08/19/DeepXplore/ex2-1.png" alt="ex2-1"></p>
<h4 id="实验2-2：运行时间"><a href="#实验2-2：运行时间" class="headerlink" title="实验2-2：运行时间"></a>实验2-2：运行时间</h4><ul>
<li><p>实验方法：不同数据集和模型生成100%覆盖率输入所需时间及相应的生成数量。<font color="red">所有图像分类模型不考虑全连接层的覆盖（作者解释因为全连接层有的神经元非常难覆盖到，但全连接层的覆盖情况应该意义更大？）</font></p>
</li>
<li><p>实验结果：DeepXplore生成高覆盖率的输入效率非常高。</p>
</li>
</ul>
<p><img src="/2020/08/19/DeepXplore/ex2-2.png" alt="ex2-2" style="zoom: 67%;"></p>
<h4 id="实验2-3：不同参数设置的影响"><a href="#实验2-3：不同参数设置的影响" class="headerlink" title="实验2-3：不同参数设置的影响"></a>实验2-3：不同参数设置的影响</h4><ul>
<li>实验方法：改变参数$s$、$\lambda_1$、$\lambda_2$，比较<font color="red">找到第一个导致预测差异的输入平均运行时间（这个指标是否足够适合评价参数？）</font></li>
<li>实验结果</li>
</ul>
<p><img src="/2020/08/19/DeepXplore/ex2-3.png" alt="ex2-3" style="zoom:67%;"></p>
<p><img src="/2020/08/19/DeepXplore/ex2-4.png" alt="ex2-4" style="zoom:60%;"></p>
<h4 id="实验2-4：当多个DNN决策边界相似时DeepXplore的效果"><a href="#实验2-4：当多个DNN决策边界相似时DeepXplore的效果" class="headerlink" title="实验2-4：当多个DNN决策边界相似时DeepXplore的效果"></a>实验2-4：当多个DNN决策边界相似时DeepXplore的效果</h4><ul>
<li>实验方法：<ul>
<li>对照组：MNIST训练集（60000个样本）和LeNet-1模型，10个epoch</li>
<li>实验组：改变①训练集样本个数，②DNN卷积层模型filter个数，③训练epoch数</li>
<li>设置初始种子数为100，对比发现生成第一个使得实验组DNN和对照组DNN预测差异样本的平均迭代次数</li>
</ul>
</li>
<li>实验结果：DeepXplore从非常相似的DNN中也能发现错误样本（除了第一行diff为1时），越相似，迭代轮数越多。</li>
</ul>
<p><img src="/2020/08/19/DeepXplore/ex2-5.png" alt="ex2-5" style="zoom:67%;"></p>
<h3 id="实验3：使用DeepXplore提升DNN"><a href="#实验3：使用DeepXplore提升DNN" class="headerlink" title="实验3：使用DeepXplore提升DNN"></a>实验3：使用DeepXplore提升DNN</h3><h4 id="实验3-1：使用生成样本扩大训练集，重新训练DNN"><a href="#实验3-1：使用生成样本扩大训练集，重新训练DNN" class="headerlink" title="实验3-1：使用生成样本扩大训练集，重新训练DNN"></a>实验3-1：使用生成样本扩大训练集，重新训练DNN</h4><ul>
<li>与对抗样本训练不同的是：不需标注（采取投票形式）</li>
<li>实验方法：重新训练MNIST数据集的三个模型5个epoch，增加100张DeepXplore生成的错误图片，与对抗样本和随机从测试集选择样本比较准确率。<font color="red">（测试样本是什么？多个DNN从何而来？）</font></li>
<li>实验结果：DeepXplore平均提升准确率1%-3%</li>
</ul>
<p><img src="/2020/08/19/DeepXplore/ex3-1.png" alt="ex3-1" style="zoom:85%;"></p>
<h4 id="实验3-2：检测被污染的训练样本"><a href="#实验3-2：检测被污染的训练样本" class="headerlink" title="实验3-2：检测被污染的训练样本"></a>实验3-2：检测被污染的训练样本</h4><ul>
<li>实验过程：用两个MNIST LeNet-5模型，一个正常训练，另一个类别9的样本中30%被故意标记成了类别1。使用DeepXplore生成被错误预测成9或1的样本，在训练集中找和错误样本结构相似的样本。能识别出95.6%的污染样本。<font color="red">（写得很简略）</font></li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>测试输入生成</tag>
        <tag>测试标准</tag>
        <tag>差分测试</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Dynamic Slicing for Deep Neural Networks</title>
    <url>/2020/11/10/Dynamic-Slicing-for-DNN/</url>
    <content><![CDATA[<p>Dynamic Slicing for Deep Neural Networks （FSE’20）<a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>传统程序切片技术：旨在从程序中提取满足一定约束条件（切片标准）的代码片段，是一种用于分解程序的程序分析技术。</p>
<ul>
<li>例如，通过将切片标准设置为某个导致错误的特定输出，可以获得与错误相关但比整个程序小得多的程序切片，因此更容易分析。</li>
</ul>
</li>
<li><p><strong>DNN切片</strong>的定义：计算获得一个神经元和突触子集，使其可以显著影响某些感兴趣的神经元的值。</p>
<ul>
<li>有助于理解神经网络的决策</li>
<li>有助于减小模型大小</li>
<li>把模型划分为重要/非重要部分，有利于优先保护模型的重要部分</li>
</ul>
</li>
<li><p>目标：找到在决策过程中更重要的神经元和突触的子集。</p>
</li>
<li><p>DNN切片的挑战：</p>
<ul>
<li>神经元的权重、连接难以理解</li>
<li>神经网络输出与几乎所有神经元都有关，因此必须区分每个神经元的贡献重要程度</li>
<li>DNN非常庞大，对性能要求高</li>
</ul>
</li>
<li><p>本文方法：<strong>NNSlicer</strong></p>
<ul>
<li><p>一种基于数据流分析的神经网络动态切片技术</p>
</li>
<li><p><strong>切片标准</strong>的定义：一组具有特殊意义的神经元</p>
<ul>
<li>如输出层神经元</li>
</ul>
</li>
<li><p><strong>神经网络切片</strong>的定义：一组对切片标准产生重要影响的神经元集合</p>
</li>
<li><p>动态切片：针对一组特定输入切片，而不是静态、与输入独立的</p>
</li>
<li><p>包括三个阶段：</p>
<p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/3.png" alt="3" style="zoom:67%;"></p>
<ul>
<li>Profiling phase：对每个神经元的平均行为（输出值）建模。训练集在该神经元输出的均值作为<font color="red">baseline</font>。</li>
<li>Forward analysis phase：对欲切片的一组感兴趣的输入，喂进DNN，记录神经元输出。输出与baseline的差异代表了<font color="red">神经元对输入的敏感程度</font></li>
<li>Backward analysis phase：切片过程从切片标准中定义的神经元开始，逐层向前分析具有重要影响的神经元。</li>
</ul>
</li>
<li><p>效率</p>
<ul>
<li>一个样本：<ul>
<li>ResNet10：40s</li>
<li>ResNet18：550s</li>
</ul>
</li>
<li>批样本：<ul>
<li>ResNet10：3s</li>
<li>ResNet18：40s</li>
</ul>
</li>
</ul>
</li>
<li><p>应用：</p>
<ul>
<li>对抗样本检测</li>
<li>模型剪枝</li>
<li>模型保护</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="问题形式化定义"><a href="#问题形式化定义" class="headerlink" title="问题形式化定义"></a>问题形式化定义</h2><ul>
<li>符号</li>
</ul>
<p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/1.png" style="zoom:70%;"></p>
<ul>
<li><p>程序切片</p>
<p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/2.png" alt="2" style="zoom:70%;"></p>
</li>
</ul>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="Profiling-和-Forward-analysis阶段"><a href="#Profiling-和-Forward-analysis阶段" class="headerlink" title="Profiling 和 Forward analysis阶段"></a>Profiling 和 Forward analysis阶段</h3><ul>
<li>Profiling阶段：记录每个神经元在训练样本上的输出均值$\overline{y^{n}( \mathcal{D})}$（卷积层一个通道看作一个神经元）</li>
<li>Forward analysis阶段：对特定的输入样本，计算神经元n的输出值与均值的差$ \Delta y^{n}( \xi )=y^{n}( \xi )- \overline{y^{n}( \mathcal{D} )} $。绝对值越大表示越敏感。</li>
</ul>
<h3 id="Backward-Analysis-和切片提取"><a href="#Backward-Analysis-和切片提取" class="headerlink" title="Backward Analysis 和切片提取"></a>Backward Analysis 和切片提取</h3><ul>
<li><p>对每个神经元和连接都计算一个贡献值$CONTRIB$，$CONTRIB \neq 0$表示该神经元或连接是重要的，$CONTRIB&gt;0$表示贡献值是正向的，否则是负向的。</p>
</li>
<li><p>$CONTRIB$的计算方法</p>
<ul>
<li><p><strong>递归</strong>地从后向前计算</p>
<p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/4.png" alt="4" style="zoom:67%;"></p>
</li>
<li><p>局部贡献值计算</p>
<ul>
<li>如Weighted sum操作: 对神经元$n$，前一层第$i$个神经元的局部贡献值$contrib_{i}=CONTRIB_{n} \times \Delta y^{n} \times w_{i} \Delta x_{i} $，其中$CONTRIB_n$为该神经元$n$当前的全局贡献值，$\Delta y^{n}$为该神经元$n$在forward analysis阶段计算出的相对敏感度，$\Delta x_{i}$为前一层神经元$n_i$的相对敏感度，即$\Delta y^{n_i}$。乘积$w_{i} \Delta x_{i}$表示$n_i$和$s_i$对全局贡献值$CONTRIB_n$的影响，比如，如果$\Delta y^{n}$是负的，$w_{i} \Delta x_{i}$是正的，说明$n_i$扩大了$n$的负性，对于$CONTRIB_n$有负向贡献。</li>
</ul>
<p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/5.png" alt="5" style="zoom:67%;"></p>
</li>
<li><p>全局贡献值=SIGN(局部贡献值)的累加，即局部贡献值只在{-1,0,1}中取值</p>
</li>
</ul>
</li>
<li><p>参数$\theta$控制切片粒度：抛弃贡献值低于$\theta$的前层神经元</p>
</li>
<li><p>多个样本的集合：贡献值=单个样本贡献值的和</p>
</li>
<li><p>切片：去掉贡献值为0的神经元和连接。</p>
</li>
</ul>
<h3 id="GPU和多线程加速"><a href="#GPU和多线程加速" class="headerlink" title="GPU和多线程加速"></a>GPU和多线程加速</h3><p>对于大样本集合$I$，前向分析过程在GPU上大批次处理，反向分析过程在CPU上一个小批次用一个线程处理。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="Overhead"><a href="#Overhead" class="headerlink" title="Overhead"></a>Overhead</h3><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/6.png" alt="6" style="zoom:67%;"></p>
<h3 id="对抗防御"><a href="#对抗防御" class="headerlink" title="对抗防御"></a>对抗防御</h3><ul>
<li><p>思想：对抗样本和正常样本的切片应该不同。从正常样本集合我们可获得一个切片，若新的输入切片与正常样本切片非常不同，则很有可能是对抗样本。</p>
</li>
<li><font color="red">方法：假设DNN模型为$\mathcal{M}$，输入样本为$\xi$，其在$\mathcal{M}$上预测的输出为$\mathcal{M}(\xi)$，对于切片标准$\mathcal{C}=(\xi, \mathcal{M}(\xi))$，使用NNSlicer方法得到切片$\mathcal{M}_\xi$。构造一个切片分类器$F$，其在训练样本上学习得到切片形状与输出类别之间的关系，若$F(M_\xi) \neq \mathcal{M(\xi)}$，则判断该样本为对抗样本。</font>

<ul>
<li>$F$的输入是一个切片$M_\xi$，具体表示为一个向量$vec_\xi$，即所有神经元之间的连接及该连接的贡献的集合。</li>
<li>本文采用决策树算法构造分类器$F$。</li>
</ul>
</li>
<li><p>用NNSlicer检测对抗样本的好处</p>
<ul>
<li>不需要更改或重训练模型</li>
<li>支持大型模型</li>
<li>只需要正常样本来训练分类器，不需要对抗样本</li>
</ul>
</li>
<li><p>实验设置</p>
<ul>
<li>对比方法：FeatureMap和EffectivePath，都使用了分类器</li>
<li>数据集和模型：CIFAR10和ResNet10，10000张正常样本训练分类器。</li>
<li>对抗攻击方法：17种（FoolBox实现）<ul>
<li>FGSM_2，FGSM_4，FGSM_8</li>
<li>DeepFoolL2，DeepFoolLinf</li>
<li>JSMA</li>
<li>RPGD_2,RPGD_4,RPGD_8</li>
<li>CWL2</li>
<li>ADef</li>
<li>SinglePixel</li>
<li>LocalSearch</li>
<li>Boundary</li>
<li>Spatial</li>
<li>PointWise</li>
<li>GaussianBlur</li>
</ul>
</li>
</ul>
</li>
<li><p>实验结果</p>
<ul>
<li>平均Recall为100%，平均Precision为83%</li>
</ul>
<p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/7.png" alt="7" style="zoom:60%;"></p>
</li>
</ul>
<h3 id="网络简化和剪枝"><a href="#网络简化和剪枝" class="headerlink" title="网络简化和剪枝"></a>网络简化和剪枝</h3><ul>
<li>一般的简化方法针对所有类别，NNSlicer针对特定的几个类别简化（如区分ImageNet里的各种狗）</li>
<li>方法：假设我们想对模型$\mathcal{M}$进行简化，目标类别为$\mathcal{O}^T$，剪枝比例为$r$。令$\mathcal{I}^T$为目标类别的输入样本，使用NNSlicer计算每个连接s的贡献$CONTRIB_s$，对每层每个连接的贡献值排序，剪掉最小的比例为$r$的这些连接，如果一个神经元的所有连接都被剪掉，则该神经元也被去掉。</li>
<li>实验设计：CIFAR10的10个类别中所有子集（210个）都作为目标类别，<ul>
<li>对比方法<ul>
<li>EffectivePath</li>
<li>Weight：去掉权重绝对值最小的边</li>
<li>Channel：去掉平均权值最小的神经元</li>
</ul>
</li>
</ul>
</li>
<li>实验结果：NNSlicer能保持准确率降低较少，这是因为我们牺牲其他类别的连接来保障目标类别的准确率能缓慢降低</li>
</ul>
<p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/8.png" alt="8" style="zoom:60%;"></p>
<ul>
<li><p>一轮微调：用10k个样本重训练一轮，即使剪枝比例很大，但NNSlicer已能达到较高的准确率</p>
<p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/9.png" alt="9" style="zoom:60%;"></p>
</li>
</ul>
<h3 id="模型保护"><a href="#模型保护" class="headerlink" title="模型保护"></a>模型保护</h3><ul>
<li><p>保护贡献值大的连接</p>
</li>
<li><p>对比方法</p>
<ul>
<li>EffectivePath</li>
<li>Weight</li>
<li>Random：随机选择连接</li>
</ul>
</li>
<li><p>实验设计：假设50%的连接参数被隐藏，攻击者试图用训练数据重训练的方式恢复连接参数，重训练准确率越低表示保护方法越好。</p>
</li>
<li><p>实验结果</p>
<ul>
<li>Target classes上准确率非常低，但All classes上准确率与Weight接近，说明非目标类上准确率可能很高，但NNSlicer不保护非目标类。</li>
</ul>
<p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/10.png" alt="10" style="zoom:60%;"></p>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>程序切片</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Compiler Validation via Equivalence Modulo Inputs</title>
    <url>/2020/10/08/EMI/</url>
    <content><![CDATA[<p>原文：Compiler Validation via Equivalence Modulo Inputs （PLDI’14) <a id="more"></a></p>
<h2 id="Equivalence-Modulo-Inputs-EMI"><a href="#Equivalence-Modulo-Inputs-EMI" class="headerlink" title="Equivalence Modulo Inputs (EMI)"></a>Equivalence Modulo Inputs (EMI)</h2><ul>
<li><p>给定程序$P$和一组输入值$I$，由$I$可生成一组程序集合$\mathcal{P}$，使得$\mathcal{P}$中每个程序$Q$都等价于$P$ modulo $I$：$\forall i \in I, Q(i) = P(i) $。则集合$\mathcal{P}$可用于对任意一个编译器$Comp$进行差分测试（differential testing），若存在某个$i \in I$和$Q \in \mathcal{P}$，使得$Comp(P)(i) \neq Comp(Q)(i)$，则该编译器存在bug。</p>
</li>
<li><p>核心思想：尽管$Q$只在输入集合$I$上与程序$P$语义等价，但编译器及其使用的静态分析和优化算法应该能为$Q$生成能在$I$上完全运行正确的中间代码。$P$和$Q$在数据流和控制流上可能很不同，经编译器优化后生成的代码也很不同，但结果应该完全一致。</p>
<p><img src="/2020/10/08/EMI/1.png" alt="emi" style="zoom:60%;"></p>
</li>
<li><p>生成EMI 变体的策略</p>
<ul>
<li>在$P$上运行输入集合$I$，获得运行轨迹，随机在未执行代码上做剪枝、插入、修改操作（假设$P$是一个确定的程序）</li>
</ul>
</li>
</ul>
<p>编译器的两类bug：</p>
<ul>
<li>导致编译器崩溃</li>
<li>生成错误代码（更加严重）<ul>
<li>导致正确的程序运行有bug</li>
<li>难以发现</li>
</ul>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>编译器测试</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Effective Path</title>
    <url>/2020/11/12/Effective-Path/</url>
    <content><![CDATA[<p>原文：Adversarial Defense Through Network Profiling Based Path Extraction （CVPR’19)  <a id="more"></a></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><ul>
<li><p>基本思想：找到Effective Path——一组重要连接和神经元的集合，使得模型得到最终的预测类别结果。可类比普通程序的执行路径。</p>
</li>
<li><p>前人方法（CDRP）</p>
<ul>
<li>需要重训练</li>
<li>维度过高</li>
</ul>
</li>
<li>提取路径信息的方法受到了传统程序分析中控制流和基本块的启发</li>
</ul>
<h3 id="单张图片提取"><a href="#单张图片提取" class="headerlink" title="单张图片提取"></a>单张图片提取</h3><p>欲提取有效路径$\mathcal{P=(N,S,W)}$，其中$\mathcal{N、S、W}$分别为重要神经元、连接、权重的集合。</p>
<ul>
<li><p>提取过程（由后向前）</p>
<ul>
<li><p>最后一层：只包括输出类别的神经元$n^L_p$</p>
</li>
<li><p>重要权重：最小权重集合，使得只计算该部分输出时，结果大于神经元$n^L_p$原本输出的$\theta$倍。由此得到前一层的重要神经元集合$N^{L-1}$。</p>
<ul>
<li>最小组合选取方法：对输入和权重的乘积进行排序，选最小且满足条件的组合</li>
</ul>
<p><img src="/2020/11/12/Effective-Path/1.png" alt="1" style="zoom:80%;"></p>
</li>
<li><p>对前一层的每个重要神经元：重复上述方法</p>
</li>
<li><p>卷积层特殊处理：</p>
<p><img src="/2020/11/12/Effective-Path/2.png" alt="2" style="zoom:80%;"></p>
<ul>
<li>根据感受野转换成全连接层</li>
<li>计算最小权重组合时无需对所有神经元排序，只需排感受野内的</li>
<li>由于卷积，多个连接可能共享相同的权重</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="多张图片提取"><a href="#多张图片提取" class="headerlink" title="多张图片提取"></a>多张图片提取</h3><ul>
<li>单张图片的路径相当于是神经元或连接的一个二元掩膜，指示了其是否在预测过程中产生了影响。多张图片的有效路径即其中所有单张图片路径组成的集合。使用每个类别预测正确的图片可得到per-class effective path，使用所有训练集预测正确的图片可得到overall effective path。</li>
<li>路径的稀疏性：<ul>
<li>设置$\theta=0.5$时，LeNet5、AlexNet、ResNet50、InceptionV4、VGG16上，Overall effective path在所有路径和权重中的占比分别为13.8%, 20.5%, 22.2%, 41.7%,17.2%，说明提取出的有效路径非常稀疏。</li>
</ul>
</li>
</ul>
<h2 id="路径的特殊化现象"><a href="#路径的特殊化现象" class="headerlink" title="路径的特殊化现象"></a>路径的特殊化现象</h2><p>不同类别的有效路径将网络拆分成不同的组件，可用于理解网络以及研究更改网络结构所带来的影响。</p>
<ul>
<li><p>路径特殊化现象：不同类别的有效路径相差很大</p>
<ul>
<li><p>计算Jacarrd系数（图2）：不同类别之间的相似度几乎都小于0.5，说明一半左右是共同激活的路径，一半是不同的</p>
</li>
<li><p>逐个合并ImageNet1000个类别的有效路径，密度一开始迅速上升然后在50个类别时开始趋于平缓（图3），与ImagNet有100个左右的基础类别这一事实相符合。</p>
<p><img src="/2020/11/12/Effective-Path/3.png" alt="3" style="zoom:80%;"></p>
</li>
</ul>
</li>
</ul>
<h2 id="对抗样本防御"><a href="#对抗样本防御" class="headerlink" title="对抗样本防御"></a>对抗样本防御</h2><ul>
<li><p>6种对抗攻击方法</p>
<p><img src="/2020/11/12/Effective-Path/4.png" alt="4" style="zoom:70%;"></p>
</li>
<li><p>对抗样本检测方法：计算<strong>路径相似度</strong></p>
<ul>
<li><p>计算待测图像的有效路径与该预测类别的有效路径之间的相似程度，因为单张图片的有效路径密度远低于类别的有效路径，所以Jaccard系数基本上取决于二者的交集占该图片有效路径的比例大小</p>
<ul>
<li>LeNet实验结果（图(a)）：正常样本相似度基本都是1.0左右，对抗样本相似度较低</li>
<li>AlexNet实验结果：<ul>
<li>Rank-1类别path：划分成不同层的相似度，对抗样本相似度同样较低（图c）。正常样本与对抗样本相似度之差（图d）显示中间层下降最多。</li>
<li>Rank-2类别path：正常样本相似度反而最低（图d），因为对抗样本的rank2类别往往是正常样本的rank1类别，所以对抗样本路径相似度更高。</li>
</ul>
</li>
</ul>
<p><img src="/2020/11/12/Effective-Path/5.png" alt="5"></p>
</li>
<li><p>防御模型</p>
<ul>
<li><p>使用rank1和rank2类别的有效路径来检测对抗样本</p>
</li>
<li><p>线性模型：每层rank1类别有效路径相似度$-$每层rank2类别有效路径相似度：$ \tilde{J}_{P}= \sum _{l=1}^{L} \omega ^{l}J_{P}^{l}- \sum _{l=1}^{L} \omega ^{l^{ \prime }}J_{P}^{l^\prime} $，若小于某阈值则判断为对抗样本</p>
</li>
<li><p>其他模型：random forest, AdaBoost, and gradient boosting</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>对抗防御</tag>
      </tags>
  </entry>
  <entry>
    <title>Google Girl Hackathon Season VI赛后感</title>
    <url>/2020/08/21/Google-Girl-Hackathon/</url>
    <content><![CDATA[<p>【8月27日更新】终于收齐所有奖品啦~~<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f601.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png?v8">😁</span><span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f601.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f601.png?v8">😁</span>  </p>
<p>一整套​Google周边<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f447.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f447.png?v8">👇</span><a id="more"></a></p>
<p><img src="/2020/08/21/Google-Girl-Hackathon/Fig1.jpg" alt="Fig1" style="zoom:15%;"></p>
<hr>
<p>今年的Google女生黑客马拉松比赛历时四个月终于在上个周末结束啦。我们队伍获得项目第一名（1/198）和最受欢迎奖（所有决赛参赛者投票投出来的），这个结果其实在意料之外也在意料之中，不过归根结底还是很不容易（要知道去年我可是连简历关都没过= = |||），今天来总结一下赛后感。</p>
<p>今年由于疫情，比赛改成了线上举办，赛题为设计并开发出一款对疫情有帮助的产品，形式不限，必须用到至少一个Google的工具。初赛交设计方案，复赛交代码、文档和视频Demo，决赛线上展示和答辩。（比赛主页：<a href="https://events.withgoogle.com/google-girl-hackathon-cn/#content">https://events.withgoogle.com/google-girl-hackathon-cn/#content</a> ）</p>
<p>我们提交的产品是一款疫情国际新闻浏览APP，但与一般新闻APP不同的是，我们的APP将不同媒体报道的同一新闻（或相关新闻）聚集成一个个新闻组，对新闻组里每条新闻组进行情绪分析后，将包含情绪分差较大的新闻组排到前面展示给用户。核心思想是情绪差异较大的文章往往意味着观点上的差异也较大，从中我们可以发现媒体偏见，尽管每篇文章本身并非完全客观的，但通过对比不同观点的文章，希望用户从中培养理性和独立思考的习惯，摒弃偏见和成见，增进互相理解、齐心抗疫。</p>
<p>这是我们的演示视频（更新后的版本）：</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/8401Arz0FHs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>APP前端用的是Google的Flutter，这个是第一次写，以前只写过IOS和一点点安卓，不过上手还是蛮快的，现成的模板有很多。下面是APP后端运行的一个原理流程图↓，后端由Python编写，新闻爬虫部分使用的是GitHub上的一个开源工具NewsPaper3k，能够自动从指定的新闻网站链接爬取新闻（标题、内容、图片等，共50多个英文网站），先保存在本地的MySQL数据库里。接下来我们将每篇文章用TF-IDF算法进行编码，然后使用基于密聚类度的DBSCAN算法将相关新闻聚类，选择合适参数下聚类效果还是不错的，其余没被聚成类的文章则被丢弃。然后用Vader工具对新闻组里的文章进行情绪分析，Vader使用的是基于词典的无监督情绪分析算法，根据标题和正文的综合得分给出每篇文章的情绪分数，Vader是跟其他两个工具（包括TextBlob和另外一个忘了）进行人工比较效果后选择的，效果还是可以的，比如我们观察到跟朝鲜相关的新闻，几乎都是一水儿的超级负面情绪，（评委曾问我如何评价效果，这个因为没有标注我说只能人工检查，或者说我们的效果依赖于使用工具的效果，暂时没想到更好的检查方法）。最后我们将包含情绪分差较大的新闻组排在前面，并上传到LeanCloud后端云数据库中（其实本来想用的是Google的Firebase，但网实在不太好，而且Firebase竟然没有关于在Flutter上如何使用的文档，震惊）。前端的设备直接和LeanCloud交互获得数据，以及完成一些辅助功能如评论、收藏、搜索等。</p>
<p><img src="/2020/08/21/Google-Girl-Hackathon/overview.png" alt="overview" style="zoom:30%;"></p>
<p>总结一下我觉得此次能得奖的关键之处吧（开始不要脸的自夸）：</p>
<p>首先我们的立足点足够大、解决的问题足够广泛，评委在颁奖时强调了我们产品注重多样性的特点与Google致力的目标一致。这个idea其实是我日常刷完各种公众号、豆瓣、知乎之后半夜躺在床上有感而发想出来的，视频和PPT里引用的谭德塞的话是某天知乎热榜标题上看到的，所以动机其实也是带着真情实感的。。虽然受到过队友质疑反对但还是据理力争做下来了。尽管情绪分析的方法很简单常见，但我们的最终目标其实是发现不同观点的文章，情绪分析只是我们挑选的一个容易下手的角度，还有更多复杂的观点分析方法可以运用在我们的产品中。</p>
<p>高中语文课学过的课文我一篇也记不起来了，但语文老师有两句话我一直记得，一句是他在我们文理分科前建议我们学理科，因为科技可以改变世界，但记住一定要做一个“有情怀的理科生”。另一句是他语重心长地和我们说人应该常怀“悲悯“。当时之所以印象深刻，可能是因为这两个词都曾被他大大地写在了黑板上，但那时候却并不明白个中意味，直到最近几年才常想起才觉得常想常新。我想今天拿到这个比赛结果也算没有辜负他的话。我始终觉得，作为科技行业从业者，开发出为人们提供日常生活便利和娱乐的产品当然好，但更高的要求是看到人世间的苦难并尝试为其做出一点改变，或是对人们的思想和精神领域带来正面的影响。</p>
<p>第二也要感激这一年的研究生生活，从写论文、看论文、讲论文的过程中不知不觉培养了自己英文学术写作和演示的能力。逐渐发现作品本身质量固然重要，但如何讲好一个完整的故事有时候更加重要，一个专业、逻辑清晰完整、美观大方的展示会给产品大大加分，反之搞不好会给本来不错的产品扣印象分，那就得不偿失了。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>杂记随感</category>
      </categories>
      <tags>
        <tag>比赛</tag>
        <tag>移动开发</tag>
      </tags>
  </entry>
  <entry>
    <title>用python从零实现一个神经网络</title>
    <url>/2020/11/30/My-Neural-Network/</url>
    <content><![CDATA[<p>不用tensorflow、pytorch等任何现有深度学习框架以及各种封装好的机器学习库，仅使用python语言及矩阵运算的库，从零开始实现一个全联接的神经网络。<a id="more"></a></p>
<h2 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h2><p>我们以一个回归任务为例，实现一个多层神经网络来拟合一个非线性函数$ y= \sin (x_{1})- \cos (x_{2}),x_{1} \in \left[ -5,5 \right] ,x_{2} \in \left[ -5,5 \right] $，函数图像如下图所示，输入层包含2个神经元，输出层包含1个神经元，隐藏层的数目和神经元数自定义，隐藏层采用ReLU作为激活函数，输出层用恒等函数做为激活函数，损失函数为均方误差（MSE）。</p>
<p>实际上我们实现的神经网络可以自定义输入输出大小、隐藏层结构、激活函数、损失函数等，因此用在其他任务上也是完全可以的。</p>
<p><img src="/2020/11/30/My-Neural-Network/1.png" alt="1" style="zoom:50%;"></p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>训练神经网络之前，我们需要先完成一些准备工作，包括生成训练和验证的数据集、定义用到的激活函数和损失函数。</p>
<h3 id="数据集生成"><a href="#数据集生成" class="headerlink" title="数据集生成"></a>数据集生成</h3><p>在轴$x_1$和$x_2$的$[-5,5]$区间上每隔0.1均匀地对样本点进行采样，然后计算目标函数的值，生成100*100=10000个训练数据，转换成为numpy​矩阵，作为训练数据集：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 生成训练数据集</span></span><br><span class="line">x_train = []</span><br><span class="line">y_train = []</span><br><span class="line"><span class="keyword">for</span> x1 <span class="keyword">in</span> np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="keyword">for</span> x2 <span class="keyword">in</span> np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>):</span><br><span class="line">        x_train.append([x1, x2])</span><br><span class="line">        y_train.append(np.sin(x1)-np.cos(x2))</span><br><span class="line">x_train = np.array(x_train)</span><br><span class="line">y_train = np.array(y_train)</span><br></pre></td></tr></tbody></table></figure>
<p>另外随机采样并生成大小为1000的验证数据集：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 生成验证数据集</span></span><br><span class="line">x_val = []</span><br><span class="line">y_val = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    x1 = random.uniform(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">    x2 = random.uniform(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">    x_val.append([x1, x2])</span><br><span class="line">    y_val.append(np.sin(x1)-np.cos(x2))</span><br><span class="line">x_val = np.array(x_val)</span><br><span class="line">y_val = np.array(y_val)</span><br><span class="line"> </span><br></pre></td></tr></tbody></table></figure>
<h3 id="激活函数和损失函数"><a href="#激活函数和损失函数" class="headerlink" title="激活函数和损失函数"></a>激活函数和损失函数</h3><p>定义本次任务用到的数学函数，包括激活函数和损失函数：</p>
<ul>
<li><p>激活函数</p>
<p>定义ActivationFunction类作为激活函数的基类，包括calculate（计算）和derivative（求导）两个抽象方法。本次任务中用到了ReLU和Pureline两种激活函数，若想使用其他激活函数可以仿照下面的形式定义（注意接受的参数x都是一个numpy矩阵）。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABCMeta, abstractmethod</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActivationFunction</span>:</span></span><br><span class="line">    <span class="string">'''激活函数基类'''</span></span><br><span class="line">    __metaclass__ = ABCMeta</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derivative</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReLU</span>(<span class="params">ActivationFunction</span>):</span></span><br><span class="line">    <span class="string">'''relu激活函数'''</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.maximum(x, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derivative</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.where(x &gt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pureline</span>(<span class="params">ActivationFunction</span>):</span></span><br><span class="line">    <span class="string">'''恒等激活函数'''</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derivative</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.ones(shape=x.shape)</span><br></pre></td></tr></tbody></table></figure>
</li>
</ul>
<ul>
<li><p>MSE损失函数</p>
<p>假设样本个数为$n$，计算公式：$MSE=\frac{1}{n} \sum_{i=1}^{n}(y_{true}[i]-y_{pred}[i])^2$</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse_loss</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="string">'''MSE损失函数'''</span></span><br><span class="line">    <span class="keyword">return</span> ((y_true - y_pred) ** <span class="number">2</span>).mean()</span><br></pre></td></tr></tbody></table></figure>
</li>
</ul>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="数据结构定义和初始化"><a href="#数据结构定义和初始化" class="headerlink" title="数据结构定义和初始化"></a>数据结构定义和初始化</h3><p>首先定义类NeuralNetwork表示神经网络类，初始化时用户向类构造函数传入参数input_size（输入层维度）、output_size（输出层维度）、hidden_size（隐藏层维度，用一个数组表示，如[5,3]表示两个隐藏层，分别包含5个和3个神经元），从而可以达到任意调整神经网络结构的目的。</p>
<p>接着定义两个数组self.w和self.b表示神经网络的权重和偏移量，每层的权重是一个numpy矩阵，用np.random.normal函数进行高斯分布初始化（设置均值为0，方差为0.1）；同时定义一个数组self.activations表示每层的激活函数，这里隐藏层激活函数为ReLU，输出层激活函数为Pureline：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 随机初始化权重和偏移量</span></span><br><span class="line">self.len = len(hidden_size) + <span class="number">1</span>    <span class="comment"># 层数(除输入层)</span></span><br><span class="line">loc = <span class="number">0.0</span></span><br><span class="line">scale = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 隐藏层</span></span><br><span class="line">self.w = [np.random.normal(loc, scale, size=(input_size, hidden_size[<span class="number">0</span>]))]</span><br><span class="line">self.b = [np.random.normal(loc, scale, size=(hidden_size[<span class="number">0</span>],))]</span><br><span class="line">self.activations = [ReLU]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(hidden_size)<span class="number">-1</span>):</span><br><span class="line">    self.w.append(np.random.normal(loc, scale, size=(hidden_size[i], hidden_size[i+<span class="number">1</span>])))</span><br><span class="line">    self.b.append(np.random.normal(loc, scale, size=(hidden_size[i+<span class="number">1</span>],)))</span><br><span class="line">    self.activations.append(ReLU)</span><br><span class="line"><span class="comment"># 输出层</span></span><br><span class="line">self.w.append(np.random.normal(loc, scale, size=(hidden_size[<span class="number">-1</span>], output_size)))</span><br><span class="line">self.b.append(np.random.normal(loc, scale, size=(output_size,)))</span><br><span class="line">self.activations.append(Pureline)</span><br></pre></td></tr></tbody></table></figure>
<p>然后定义五个数组用于保存训练时的中间结果，均初始化为全0：</p>
<ul>
<li>self.z：每个神经元激活前的输出。</li>
<li>self.a：每个神经元激活后的输出。</li>
<li>self.delta：神经单元误差$\delta$，即loss对每个神经元激活前输出$z$的梯度（用于反向传播更新权重）。</li>
<li>self.delta_w：权重w的累积更新值，用于处理完每个batch后更新权重。</li>
<li>self.delta_b：权重b的累积更新值。</li>
</ul>
<h3 id="前向计算中间结果"><a href="#前向计算中间结果" class="headerlink" title="前向计算中间结果"></a>前向计算中间结果</h3><p>每轮训练首先调用一个自定义的shuffle方法打乱训练样本集合。然后开始训练，设置训练轮数为30，batch_size为10，初始学习率为0.01。对每个输入样本，通过前向传播计算得到每层的输出self.z和self.a，如假设第$i$层（$i&gt;1$)的输入为$a[i-1]$，权重和偏移为$w[i]$和$b[i]$，则有:</p>
<p>$z[i]=a[i-1]\times w[i] + b[i]$，</p>
<p>$a[i]=activations[i].calculate(z[i])$</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="string">'''随机打乱数据'''</span></span><br><span class="line">    xy= np.c_[x,y]</span><br><span class="line">    np.random.shuffle(xy)</span><br><span class="line">    x = xy[:, :<span class="number">-1</span>]</span><br><span class="line">    y = xy[:, <span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, x_train, y_train, x_val, y_val, epochs=<span class="number">30</span>, batch_size=<span class="number">10</span>, lr=<span class="number">0.01</span></span>):</span></span><br><span class="line">    <span class="string">'''训练模型'''</span></span><br><span class="line"></span><br><span class="line">    self.validate(<span class="number">0</span>, x_train, y_train, x_val, y_val)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        x_train_shuffle, y_train_shuffle = shuffle(x_train, y_train)</span><br><span class="line">        batch_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x, y_true <span class="keyword">in</span> zip(x_train_shuffle, y_train_shuffle):</span><br><span class="line">            <span class="comment"># 前向传播得到每层输出</span></span><br><span class="line">            layer_output = x</span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">0</span>, self.len):</span><br><span class="line">                self.z[layer] = np.matmul(layer_output, self.w[layer]) + self.b[layer]</span><br><span class="line">                layer_output = self.activations[layer].calculate(self.z[layer])</span><br><span class="line">                self.a[layer] = layer_output</span><br></pre></td></tr></tbody></table></figure>
<h3 id="反向传播计算梯度"><a href="#反向传播计算梯度" class="headerlink" title="反向传播计算梯度"></a>反向传播计算梯度</h3><p>接下来计算输出层（第-1层）的神经单元误差$\delta$，对于每个样本，假如输出层维度为$m$，损失函数$C=\frac{1}{m}\sum_{i=1}^{m}(y_{true}[i]-a[-1][i])^2$（本次任务中$m=1$），则$\delta[-1] = \frac{\partial C}{\partial z[-1]} = \frac{\partial C}{\partial a[-1]} \cdot \frac{\partial a[-1]}{\partial z[-1]}=  \frac{1}{m} \cdot 2 \cdot (a[-1] - y_{true}) \cdot \theta’(z[-1])$，其中$\theta$为输出层的激活函数。</p>
<p>然后可以通过递推式反向计算出前面每层的神经单元误差，第$i$层的神经单元误差$\delta[i]=w[i+1] \times \delta[i+1] \times \theta’(z[i])$ ，其中$\theta$表示第$i$层的激活函数。</p>
<p>通过神经单元误差$\delta$我们可以计算出损失函数对w和b的导数，对第$i$层：$\frac{\partial C}{\partial w[i]} = a[i-1]^T \cdot \delta[i]$，$\frac{\partial C}{\partial b[i]}= \delta[i]$。并将导数累加结果记录在self.delta_w、self.delta_b数组中。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 计算最后一层的神经单元误差δ</span></span><br><span class="line">self.delta[<span class="number">-1</span>] = <span class="number">2</span> * (self.a[<span class="number">-1</span>] - y_true) * self.activations[<span class="number">-1</span>].derivative(self.z[<span class="number">-1</span>]) / len(self.delta[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向递推计算每层神经单元误差δ</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(self.len<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">    self.delta[layer] = np.matmul(self.w[layer + <span class="number">1</span>], self.delta[layer + <span class="number">1</span>]) * self.activations[layer].derivative(self.z[layer])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 累加导数</span></span><br><span class="line">last_layer_output = x</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(self.len):</span><br><span class="line">    self.delta_w[layer] += np.matmul(np.transpose([last_layer_output]), [self.delta[layer]])</span><br><span class="line">    self.delta_b[layer] += self.delta[layer]</span><br><span class="line">    last_layer_output = self.a[layer]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h3><p>每个batch结束后，更新w和b。对第i层：</p>
<p>$w[i] \leftarrow w[i]- lr \cdot \Delta w[i]$</p>
<p>$b[i] \leftarrow b[i]- lr \cdot \Delta b[i]$</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 每个batch结束后更新w、b</span></span><br><span class="line">batch_count += <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> batch_count == batch_size:</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> range(self.len):</span><br><span class="line">        self.w[layer] -= lr * self.delta_w[layer]</span><br><span class="line">        self.b[layer] -= lr * self.delta_b[layer]</span><br><span class="line">        self.delta_w[layer].fill(<span class="number">0</span>)</span><br><span class="line">        self.delta_b[layer].fill(<span class="number">0</span>)</span><br><span class="line">batch_count = <span class="number">0</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>这里采用了简单的学习率衰减方案，每过三轮学习率衰减为0.9倍。大家可以尝试其他衰减方案。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">3</span> == <span class="number">0</span>:</span><br><span class="line">    lr *= <span class="number">0.9</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="验证和可视化"><a href="#验证和可视化" class="headerlink" title="验证和可视化"></a>验证和可视化</h2><p>每轮训练完成后调用validate方法验证模型在训练数据集和验证数据集上的损失，并绘制图像。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="string">'''前向传播计算预测结果'''</span></span><br><span class="line">    layer_output = x</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">0</span>, self.len):</span><br><span class="line">        layer_output = self.activations[layer].calculate((np.matmul(layer_output, self.w[layer]) + self.b[layer]))</span><br><span class="line">    <span class="keyword">return</span> layer_output.squeeze()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">self, epoch, x_train, y_train, x_val, y_val</span>):</span></span><br><span class="line">    <span class="string">'''计算训练集和验证集上的损失，生成可视化图像'''</span></span><br><span class="line">    y_pred_train = self.feedforward(x_train)</span><br><span class="line">    train_loss = mse_loss(y_train, y_pred_train)</span><br><span class="line">    y_pred_val = self.feedforward(x_val)</span><br><span class="line">    val_loss = mse_loss(y_val, y_pred_val)</span><br><span class="line">    self.visualize(epoch, y_pred_train, train_loss, val_loss)</span><br><span class="line">    print(<span class="string">"Epoch %d, train loss: %.4f, validation loss: %.4f"</span> % (epoch, train_loss, val_loss))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span>(<span class="params">self, epoch, y_pred, train_loss, val_loss</span>):</span></span><br><span class="line">   <span class="string">'''可视化训练结果'''</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.gca(projection=<span class="string">'3d'</span>)</span><br><span class="line">    X = np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">    Y = np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">    X, Y = np.meshgrid(X, Y)</span><br><span class="line">    Z = y_pred.reshape((<span class="number">100</span>,<span class="number">100</span>))</span><br><span class="line">    surf = ax.plot_surface(X, Y, Z, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>, cmap=cm.jet, linewidth=<span class="number">0</span>, antialiased=<span class="literal">False</span>)</span><br><span class="line">    ax.set_zlim(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">    ax.zaxis.set_major_locator(LinearLocator(<span class="number">10</span>))</span><br><span class="line">    ax.zaxis.set_major_formatter(FormatStrFormatter(<span class="string">'%.02f'</span>))</span><br><span class="line">    ax.set_title(<span class="string">'epoch=%d, train loss=%.4f, validation loss=%.4f'</span>% (epoch, train_loss, val_loss))</span><br><span class="line">    fig.colorbar(surf, shrink=<span class="number">0.5</span>, aspect=<span class="number">5</span>)</span><br><span class="line">    plt.savefig(<span class="string">'figure/%04d.png'</span> % epoch)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure>
<h2 id="拟合效果"><a href="#拟合效果" class="headerlink" title="拟合效果"></a>拟合效果</h2><h3 id="增加隐藏层宽度对拟合效果的影响"><a href="#增加隐藏层宽度对拟合效果的影响" class="headerlink" title="增加隐藏层宽度对拟合效果的影响"></a>增加隐藏层宽度对拟合效果的影响</h3><p>设置神经网络只有一个隐藏层，考察隐藏层分别包含10、20、50、100个神经元时，训练集loss随训练轮数增加的变化情况（如下图）。（设置学习率为恒定值0.001，训练轮数为30轮）</p>
<p><img src="/2020/11/30/My-Neural-Network/3.png" alt="image-20201129230937618" style="zoom:70%;"></p>
<p>可见随着训练轮数增长，各个模型的训练集loss首先快速下降，接着缓慢下降。单隐藏层神经元个数越多，训练完成后的loss越低，说明增加神经元宽度能提升模型对目标函数的拟合能力。</p>
<h3 id="增加隐藏层深度对拟合效果的影响"><a href="#增加隐藏层深度对拟合效果的影响" class="headerlink" title="增加隐藏层深度对拟合效果的影响"></a>增加隐藏层深度对拟合效果的影响</h3><p>考察神经网络分别包含1、2、3、4个隐藏层，且每个隐藏层包含20个神经元时，训练集loss随训练轮数增加的变化情况（如下图）。（设置学习率为恒定值0.001，训练轮数为30轮）。</p>
<p><img src="/2020/11/30/My-Neural-Network/4.png" alt="image-20201129232317776" style="zoom:70%;"></p>
<p>由图可见，神经网络层数越多，30轮训练结束后训练集的loss越低，说明层数越多，模型对目标函数的拟合能力越强。</p>
<h3 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h3><p>经过以上实验，我们验证了增加隐藏层的宽度和深度能减小拟合的误差。最后为了让拟合误差尽量小，经过多次尝试后，我们最终选择设置隐藏层个数为5，分别包含100、80、50、30、10个神经元，学习率衰减方案为每经过3轮训练，lr衰减为0.9倍。最终训练集loss为0.00015，验证集loss为0.00016。第0轮（训练前）、第1轮、第5轮、第30轮训练后的拟合结果可视化后如下图所示：</p>
<p><img src="/2020/11/30/My-Neural-Network/5.jpg" alt="图片1"></p>
<p>动图效果：</p>
<p><img src="/2020/11/30/My-Neural-Network/2.gif" alt="2" style="zoom:80%;"></p>
<p>完整代码见：<a href="https://github.com/rubychen0611/MyNeuralNetwork">https://github.com/rubychen0611/MyNeuralNetwork</a></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>技术笔记</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Surprise Adequacy</title>
    <url>/2020/08/25/Surprise-Adequacy/</url>
    <content><![CDATA[<p>原文：Guiding Deep Learning System Testing using Surprise Adequacy (ICSE‘19)  <a id="more"></a></p>
<p>代码地址：<a href="https://github.com/coinse/sadl">https://github.com/coinse/sadl</a></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>提出了一个叫Surprise Adequacy的指标，对一个给定的测试用例，基于可能性或距离来衡量其激活模式相对于DNN训练集的新颖程度。实验部分较详细。</p>
<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>已有的覆盖方法：</p>
<ul>
<li>无法针对<strong>单个</strong>输入提供信息：比如更高神经元覆盖率的输入是否比更低的好？</li>
<li>评估的重点是显示对抗样本和提出的标准之间的相关性，而不是评估和指导它们<strong>在实际测试DL系统时的使用</strong>。</li>
</ul>
<p>直观地说，DL系统的一个好的测试输入集应该是系统多样化的，包括从类似于训练数据的输入、到明显不同和对抗的输入。在单个样本的粒度上，SADL测量输入对于训练系统的数据对DL系统的惊讶程度:</p>
<ul>
<li>基于系统看到类似的输入的可能性在训练</li>
<li>或基于神经元激活向量之间的距离</li>
</ul>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>设神经元集合为$N$，训练集为$T$，两种方式衡量测试输入$x$与训练集神经元激活向量$A_N(T)$之间的相似程度</p>
<h3 id="基于可能性的SA（LSA）"><a href="#基于可能性的SA（LSA）" class="headerlink" title="基于可能性的SA（LSA）"></a>基于可能性的SA（LSA）</h3><p>采用核密度估计（KDE）来获得输入数据的分布密度函数，这里使用高斯核函数</p>
<p>在这种方法的时候,为了减少计算量,有如下两种规则</p>
<ul>
<li><p>只选定特定的某层</p>
</li>
<li><p>方差过滤：过滤掉那些激活值的方差小于预先定义的阈值$t$的神经元</p>
</li>
</ul>
<p>密度函数定义：</p>
<script type="math/tex; mode=display">\widehat{f}(x)= \frac{1}{|A_{N_L}(T)|} \sum _{x_{i} \in T}K_{H}( \alpha _{N_{L}}(x)- \alpha _{N_{L}}(x_{i}))</script><p>公式的直观理解：对于所有的训练集中的用例，每个用例使用高斯核函数计算该用例与新输入x的激活迹的差值。概率密度低说明输入更加稀有，概率密度高说明输入更加相似。</p>
<p>LSA定义：</p>
<script type="math/tex; mode=display">LSA(x)=- \log ( \widehat{f}(x))</script><p>实际应用中只使用类别$c$的训练集数据$T_c$计算LSA。</p>
<h3 id="基于距离的SA（DSA）"><a href="#基于距离的SA（DSA）" class="headerlink" title="基于距离的SA（DSA）"></a>基于距离的SA（DSA）</h3><p><img src="/2020/08/25/Surprise-Adequacy/Fig1.png" alt="Fig1" style="zoom:60%;"></p>
<ul>
<li><p>对于输入样本$x$，其预测标签为$c_x$，设$x_a$为与其标签相同且激活向量距离最近的训练样本，$x_b$为激活向量离$x_a$最近且类别不同的训练样本，DSA定义为：</p>
<script type="math/tex; mode=display">DSA(x)= \frac{dist_{a}}{dist_{b}}</script></li>
<li><p>DSA越大，说明$x$对于类别$c_x$来说越surprise</p>
</li>
<li><p>DSA 只适用于分类任务</p>
</li>
</ul>
<h3 id="意外覆盖率的计算（SC"><a href="#意外覆盖率的计算（SC" class="headerlink" title="意外覆盖率的计算（SC)"></a>意外覆盖率的计算（SC)</h3><p>因为LSA和DSA取值都是连续的，我们用被覆盖的段数除以总段数来表示覆盖率：</p>
<p>给定上界$U$，将$(0,U]$分成n个SA段的bucket：$B=\{b_1,b_2,…,b_n\}$，一组输入$X$的SC定义如下：</p>
<script type="math/tex; mode=display">SC(X)= \frac{| \{ b_{i}| \exists x \in X:SA(x) \in \left[ U \cdot \frac{i-1}{n},U \cdot \frac{i}{n} \right] \}|}{n}</script><p>注意：一组具有较高SC的输入应该是一组多样化的输入。然而，具有特别高SA值的输入可能<strong>与问题域无关</strong>(如交通标志的图像将与动物图片分类器的测试无关)。因此，SC只能相对于<strong>预定义的上界</strong>来测量。<font color="red">（提前设置好的上界很重要）</font></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验配置"><a href="#实验配置" class="headerlink" title="实验配置"></a>实验配置</h3><ul>
<li>数据集和模型</li>
</ul>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig2.png" alt="Fig2"></p>
<ul>
<li><p>对抗样本生成方法</p>
<ul>
<li>FGSM</li>
<li>BIM-A、BIM-B</li>
<li>JSMA</li>
<li>C&amp;W</li>
</ul>
</li>
<li><p>合成图像生成方法（Driving数据集）</p>
<ul>
<li>Dave2模型：DeepXplore的输入生成（亮度、矩形、污点）和联合优化方法（3个DNN：Dave-2、Dave-dropout、Dave-norminit）</li>
<li>Chauffeur模型：DeepTest输入生成方法（translation, scale, shear, rotation, contrast, brightness, and blur）</li>
</ul>
</li>
<li><p>参数设置</p>
<ul>
<li><p>LSA默认方差阈值：$10^{-5}$</p>
</li>
<li><p>kde的带宽使用scott规则设置</p>
</li>
<li><p>RQ1</p>
<ul>
<li>MNIST选择activation_2层，CIFAR-10选择activation_6层</li>
</ul>
</li>
<li><p>RQ2</p>
<ul>
<li>CIFAR10的activation_7和activation_8层的LSA默认方差阈值设置为$10^{-4}$（减少计算消耗）</li>
</ul>
</li>
<li><p>RQ3</p>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig3.png" alt="Fig3" style="zoom:75%;"></p>
</li>
<li><p>RQ4</p>
<ul>
<li>MNIST选择activation_3层，CIFAR-10选择activation_5层</li>
<li>每次重新练运行20次</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="RQ1-SADL是否能够捕获DL系统输入的相对惊讶程度"><a href="#RQ1-SADL是否能够捕获DL系统输入的相对惊讶程度" class="headerlink" title="RQ1 SADL是否能够捕获DL系统输入的相对惊讶程度?"></a>RQ1 SADL是否能够捕获DL系统输入的相对惊讶程度?</h3><ul>
<li><p>实验方法</p>
<ul>
<li>从原始数据集检查是否SA越高的样本越难被分类正确</li>
<li>检查对抗样本是否SA较高</li>
<li>最后，我们使用逻辑回归对SA值进行对抗样本分类训练。对于每个敌对攻击策略，我们使用MNIST和CIFAR-10提供的10,000张原始测试图像生成10,000个对抗样本。使用随机选取的1,000张原始测试图像和1,000个对抗样本，我们训练<strong>logistic回归分类器</strong>。<font color="red">(标签是如何确定的？）</font>最后，我们使用剩余的9000幅原始测试图像和9000个敌对例子来评估训练过的分类器。如果SA值正确地捕获DL系统的行为，我们期望基于SA的分类器能够成功地检测出对抗样本。我们使用ROC-AUC进行评估。</li>
</ul>
</li>
<li><p>实验结果</p>
<ul>
<li><p>红点对应的图像集合(Ascending SA)从最小SA开始，随着SA的上升，越来越多的高SA</p>
</li>
<li><p>蓝点对应的图像组在相反的方向生长(即从SA高的图像到SA低的图像)。</p>
</li>
<li><p>作为参考，绿点表示在20次重复中随机增长的集合的平均精度。</p>
</li>
<li><p>可以看出，包含LSA值较高的图像，即更多令人惊讶的图像，会导致精度较低。</p>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig4.png" alt="Fig4"></p>
</li>
<li><p>为了在另一个数据集上进行视觉确认，我们也选择了DeepTest为Chauffeur从三个不同级别的LSA值合成的输入集:图3显示，LSA值越高，图像视觉识别就越困难。无论是从数量上还是从视觉上，观察到的趋势都支持了我们的声明，即SADL捕获输入的意外:即使对于看不见的输入，SA也可以度量给定输入的意外程度，这与DL系统的性能直接相关。</p>
</li>
<li><p><img src="/2020/08/25/Surprise-Adequacy/Fig5.png" alt="Fig5" style="zoom:60%;"></p>
</li>
<li><p>图4显示了由五种技术中的每一种生成的10,000个对抗样本的DSA值的排序图，以及原始的测试输入：</p>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig6.png" alt="Fig6" style="zoom:80%;"></p>
</li>
<li><p>图5是在MNIST和cifar-10的<strong>不同层</strong>中随机选取2000个对抗样本和原始测试集的LSA值的相似图。对于MNIST和cifar10，数据集提供的测试输入(用蓝色表示)往往是最不令人吃惊的，而大多数对抗样本通过较高的SA值与测试输入明显分开。这支持了我们的说法，即SADL可以捕获敌对示例中DL系统行为的差异。</p>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig7.png" alt="Fig7"></p>
</li>
<li><p>最后，表III给出了MNIST和CIFAR-10中使用所有神经元进行基于DSA的分类的ROC-AUC结果。结果表明，图4中DSA值的差距可以用于对敌对的例子进行高精度的分类。</p>
</li>
<li><p>对于相对简单的MNIST模型，分类器的ROC-AUC范围在96.97% - 99.38%之间，可以检测出对抗样本。</p>
</li>
<li><p>对于更复杂的CIFAR-10模型，基于DSA的分类显示较低的ROC-AUC值，但RQ2的回答表明，来自特定层次的DSA可以产生明显更高的精度。</p>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig8.png" alt="Fig8" style="zoom:80%;"></p>
</li>
<li><p><strong>基于三种不同的分析，RQ1的答案是SADL可以捕获输入的相对惊喜。高SA的输入更难正确分类;对抗性实例的SA值较高，可以根据SA进行相应的分类</strong>。</p>
</li>
</ul>
</li>
</ul>
<h3 id="RQ2：神经元层的选择是否对SA反映DL系统行为的准确性有任何影响"><a href="#RQ2：神经元层的选择是否对SA反映DL系统行为的准确性有任何影响" class="headerlink" title="RQ2：神经元层的选择是否对SA反映DL系统行为的准确性有任何影响?"></a>RQ2：神经元层的选择是否对SA反映DL系统行为的准确性有任何影响?</h3><ul>
<li><p>实验方法：我们通过计算各层的LSA和DSA，然后通过比较各层在SA上训练的对抗性例分类器来评估在SA上下文中的假设。</p>
</li>
<li><p>MNIST实验结果</p>
<ul>
<li><p>表IV给出了对敌示例分类的ROC-AUC，结果每一行分别对应MNIST中特定层的LSA和DSA上训练的分类器。行按其深度排序，即，activation_3是MNIST中最深也是最后一个隐藏层。每种攻击策略的最高ROC-AUC值以粗体显示。<font color="red">对于MNIST来说，没有明确的证据表明最深的一层是最有效的。</font></p>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig9.png" alt="Fig9" style="zoom:80%;"></p>
<ul>
<li>图5可以解释ROC-AUC是100%的情况：MNIST activation_1对抗样本和测试集里的样本曲线清晰地分离。而MNIST activation_3的LSA曲线有很多交叉。<font color="red">（这个结果比较反直觉）</font></li>
</ul>
</li>
</ul>
</li>
<li><p>CIFAR-10实验结果</p>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig10.png" alt="Fig10" style="zoom:75%;"></p>
<ul>
<li>对于LSA，没有强有力的证据表明最深的层产生最准确的分类器。</li>
<li>然而，对于DSA，最深层次为五种攻击策略中的三种(BIM-B、JSMA和C&amp;W)生成最准确的分类器，而第二层为BIM-A生成最准确的分类器。更重要的是，<font color="red"><strong>单层</strong>DSA比所有<strong>神经</strong>元DSA值产生的分类结果要准确得多</font>（从表III与表IV和表v的对比可以看出）。</li>
</ul>
</li>
<li><p>总结：DSA对其计算的层的选择很敏感，选择较深的层是有益的。然而，对于LSA，没有明确的证据支持更深层次的假设。不同的实例生成策略的层敏感性不同。</p>
</li>
</ul>
<h3 id="RQ3：SC是否与DL系统的现有覆盖率标准存在相关性"><a href="#RQ3：SC是否与DL系统的现有覆盖率标准存在相关性" class="headerlink" title="RQ3：SC是否与DL系统的现有覆盖率标准存在相关性?"></a>RQ3：SC是否与DL系统的现有覆盖率标准存在相关性?</h3><ul>
<li><p>实验方法</p>
<ul>
<li>我们通过累计添加输入控制输入的多样性，执行DL与这些输入，系统研究和比较各种覆盖标准的观察到的变化。包括SC和四个现有的标准:NC、KMNC、NBC、SNAC。</li>
<li>对于MNIST和cifar-10，我们从数据集提供的原始测试数据(10000幅图像)开始，每一步添加由FGSM、BIM-A、BIM-B、JSMA和C&amp;W生成的1000个对抗性示例。</li>
<li>对于Dave-2，我们从原始测试数据(5614张图像)开始，在每一步添加由DeepXplore生成的700张合成图像。</li>
<li>对于Chauffeur来说，每一步增加1000张合成图像(Set1到Set3)，每一张图像都是通过应用随机数量DeepTest的变换生成的。</li>
</ul>
</li>
<li><p>实验结果</p>
<ul>
<li><p>表6显示了不同的覆盖率标准如何对日益增加的多样性水平作出反应。列表示步骤，在每一个步骤中会向原始测试集添加更多的输入。如果与前一个步骤相比，<strong>步骤中覆盖率的增加小于0.1个百分点，则该值加下划线</strong>。</p>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig11.png" alt="Fig11" style="zoom:70%;"></p>
</li>
<li><p>图6显示了来自CIFAR-10和Chauffeur的结果的可视化。(注意DSC不能为这两个DL系统计算，因为它们不是分类器。)</p>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig12.png" alt="Fig12" style="zoom:60%;"></p>
</li>
<li><p>总的来说，大多数研究的标准会随着每一步的额外输入而增加。<strong>值得注意的例外是NC，它会在许多步骤中停滞不前。</strong>这与DeepGauge的结果一致。</p>
</li>
<li><p>添加的输入类型与不同标准的响应之间存在相互作用:SNAC、KMNC和NBC在CIFAR-10中加入BIM-B示例时显著增加，但在添加C&amp;W输入时变化不大。但是，对于Chauffeur，只有SNAC和NBC在增加输入集1时表现出类似的增长，而KMNC的增长更为稳定。</p>
</li>
<li><p>总的来说，除了NC之外，我们回答了RQ3, SC与到目前为止引入的其他覆盖率标准相关。</p>
</li>
</ul>
</li>
</ul>
<h3 id="RQ4：SA能否指导DL系统的再训练，以提高它们在对抗样本和由DeepXplore生成的合成测试输入时的准确性"><a href="#RQ4：SA能否指导DL系统的再训练，以提高它们在对抗样本和由DeepXplore生成的合成测试输入时的准确性" class="headerlink" title="RQ4：SA能否指导DL系统的再训练，以提高它们在对抗样本和由DeepXplore生成的合成测试输入时的准确性?"></a>RQ4：SA能否指导DL系统的再训练，以提高它们在对抗样本和由DeepXplore生成的合成测试输入时的准确性?</h3><ul>
<li><p>实验方法：</p>
<ul>
<li>我们检查SA是否可以指导额外训练输入的选择。从这些模型的对抗样本和合成输入中，我们从四个不同的SA范围中选择了4组100张图像。已知$U$为RQ3中用于计算SC的上界，我们将SA $[0, U]$的范围划分为四个重叠的子集: 第一个子集包括低25%的SA值($[0,\frac{U}{4}]$)，第二个子集包括低25%的SA值($[0,\frac{2U}{4}]$)，第三个子集包括低75%的SA值($[0,\frac{3U}{4}]$)，最后是整个范围$[0,U]$。这四个子集代表越来越多样化的投入。我们将范围R设为这四个中的一个，从每个R中随机抽取100张图像，并对现有的模型进行5个额外的epoch训练。最后，我们分别测量每个模型在整个敌对和合成输入下的性能(MNIST和cifar-10的精度，Dave-2的MSE)。我们期望用更多样化的子集进行再培训将会带来更高的性能。</li>
</ul>
</li>
<li><p>实验结果</p>
<p><img src="/2020/08/25/Surprise-Adequacy/Fig13.png" alt="Fig13" style="zoom:80%;"></p>
<ul>
<li>虽然我们的观察局限于DL系统和这里研究的输入生成技术，我们回答RQ4, SA可以提供指导，更有效的再训练DL系统。</li>
</ul>
</li>
</ul>
<h2 id="可控制参数-变量"><a href="#可控制参数-变量" class="headerlink" title="可控制参数/变量"></a>可控制参数/变量</h2><ul>
<li>选择神经元的层（单层/所有层）</li>
<li>过滤神经元的方差阈值</li>
<li>SA预定义的上界$U$</li>
<li>覆盖率计算式bucket个数（划分的段数）</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/qq_33935895/article/details/101155270">https://blog.csdn.net/qq_33935895/article/details/101155270</a></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>测试标准</tag>
        <tag>输入验证</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】TensorFuzz</title>
    <url>/2020/08/20/TensorFuzz/</url>
    <content><![CDATA[<p>原文：TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing （ICML’19) <a id="more"></a></p>
<p>代码：<a href="https://github.com/brain-research/tensorfuzz">https://github.com/brain-research/tensorfuzz</a></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="CGF基本流程"><a href="#CGF基本流程" class="headerlink" title="CGF基本流程"></a>CGF基本流程</h3><p><img src="/2020/08/20/TensorFuzz/1.png" alt="1" style="zoom:60%;"></p>
<h3 id="方法细节"><a href="#方法细节" class="headerlink" title="方法细节"></a>方法细节</h3><ul>
<li><p>输入选择：选择更新鲜的输入</p>
</li>
<li><p>输入变换：①白噪声（参数由用户给出）；②增加$L_{\infty}$约束的白噪声</p>
</li>
<li><p>目标函数：用户根据覆盖率和元数据情况自定义</p>
</li>
<li><p>覆盖分析器：当我们得到一个新的激活向量时，我们可以查找它的最近邻居，然后检查这个最近的邻居在欧几里得距离中有多远，如果这个距离大于某个量，就向语料中添加输入。</p>
<ul>
<li>可只选择部分神经元的值作为激活向量，如只选logits或logits前一层</li>
</ul>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验1：CGF可以高效地找到已训练神经网络的数值错误（导致NaN的错误）"><a href="#实验1：CGF可以高效地找到已训练神经网络的数值错误（导致NaN的错误）" class="headerlink" title="实验1：CGF可以高效地找到已训练神经网络的数值错误（导致NaN的错误）"></a>实验1：CGF可以高效地找到已训练神经网络的数值错误（导致NaN的错误）</h3><ul>
<li><p>实验方法：使将检查数值运算添加到元数据并运行模糊测试器（fuzzer）。我们训练了一个完全连接的神经网络来对MNIST数据集里的数字进行分类，故意用了一个很糟糕的交叉熵损失，这样就有可能出现数值误差。模型进行了35000步的训练，mini-batch size为100，验证精度为98%。检查MNIST数据集中不含导致数值误差的样本。</p>
</li>
<li><p>实验结果：TensorFuzz 却在10次随机初始化后快速找到了 NaN错误。</p>
<p><img src="/2020/08/20/TensorFuzz/2.jpg" alt="2" style="zoom:67%;"></p>
<ul>
<li>基于梯度的搜索技术可能无助于查找数值误差</li>
<li>随机搜索对于查找数值误差来说效率极低。</li>
</ul>
<h3 id="实验2：CGF-解决模型和量化版本不一致的问题"><a href="#实验2：CGF-解决模型和量化版本不一致的问题" class="headerlink" title="实验2：CGF 解决模型和量化版本不一致的问题"></a>实验2：CGF 解决模型和量化版本不一致的问题</h3><ul>
<li><p>仅检查已有的数据只能找到很少的错误：作为基线实验，我们训练了一个使用 32 位浮点数的 MNIST 分类器（这一次没有故意引入数值错误）。然后把所有权重和激活值修剪为 16 位。之后，我们对比了 32 位和 16 位模型在 MNIST 测试集上的预测，没有找到任何不一致性。</p>
</li>
<li><p>CGF 可以快速在数据周围的小区域中找到很多错误：然后运行 fuzzer，变化限制在种子图像周围的半径为 0.4 的无限范数球中，其中仅使用了 32 位模型作为覆盖的激活值。我们将输入限制在种子图像附近，因为这些输入几乎都有明确的类别语义。模型的两个版本在域外的垃圾数据（没有真实类别）上出现不一致性并没有什么意义。通过这些设置，fuzzer 可以生成 70% 样本的不一致性。因此，CGF 允许我们寻找在测试时出现的真实错误.</p>
</li>
</ul>
<h3 id="实验3：TensorFuzz可以发现流行模型实现中的bug"><a href="#实验3：TensorFuzz可以发现流行模型实现中的bug" class="headerlink" title="实验3：TensorFuzz可以发现流行模型实现中的bug"></a>实验3：TensorFuzz可以发现流行模型实现中的bug</h3><h3 id="实验4：TensorFuzz可以帮助进行保持语义的代码转换"><a href="#实验4：TensorFuzz可以帮助进行保持语义的代码转换" class="headerlink" title="实验4：TensorFuzz可以帮助进行保持语义的代码转换"></a>实验4：TensorFuzz可以帮助进行保持语义的代码转换</h3></li>
</ul>
<h2 id="可控制变量及参数总结"><a href="#可控制变量及参数总结" class="headerlink" title="可控制变量及参数总结"></a>可控制变量及参数总结</h2><ul>
<li><p>输入选择策略</p>
</li>
<li><p>变换参数</p>
</li>
<li><p>目标函数</p>
</li>
<li><p>激活神经元集合</p>
</li>
<li><p>距离阈值L</p>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>模糊测试</tag>
        <tag>测试输入生成</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Towards Improved Testing For Deep Learning</title>
    <url>/2020/08/26/Towards-Improved-Testing-For-Deep-Learning/</url>
    <content><![CDATA[<p>原文：Towards Improved Testing For Deep Learning（ICSE-NIER’19）<a id="more"></a></p>
<h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>提出了一种2-路覆盖标准：每个神经元单独对下一层神经元值的条件影响以及上一层中神经元值的组合对下一层神经元值的影响。</p>
<h2 id="当前领域的提升空间"><a href="#当前领域的提升空间" class="headerlink" title="当前领域的提升空间"></a>当前领域的提升空间</h2><h3 id="目前覆盖方法的局限性"><a href="#目前覆盖方法的局限性" class="headerlink" title="目前覆盖方法的局限性"></a>目前覆盖方法的局限性</h3><ul>
<li>DeepXplore：神经元覆盖率粗糙、不充分</li>
<li>DeepCover：DeepCover的覆盖标准考虑了DNN相邻层中的条件判定依赖性。该方法除了时在相对较小的网络上测试之外，它的前提条件还要求DNN是前馈全连接的神经网络。而且它不能推广到诸如RNN、LSTM、attention网络等的架构。DeepCover不考虑神经元在其所在层的环境，即同一层中神经元输出的组合。</li>
<li>DeepCT：DeepCT的组合测试是启发于覆盖标准：根据每层激活的神经元比值描述测试输入所使用的逻辑比值。它没有考虑DNN中的层级之间的关系，并且也没有被证实可以扩展到具有不同类型层的、能用于真实世界的DNN</li>
</ul>
<h3 id="目前测试输入生成方法的局限性"><a href="#目前测试输入生成方法的局限性" class="headerlink" title="目前测试输入生成方法的局限性"></a>目前测试输入生成方法的局限性</h3><p>测试输入可以通过指导方式生成或选择得到，它通常有两个主要目标：最大化未覆盖故障的数量，并最大化覆盖范围。 目前测试输入生成方法存在一些主要缺点：</p>
<ul>
<li>修改现有测试输入直到找到满足标准的测试输入的迭代过程单次执行耗时长。</li>
<li>与总的测试和生成的输入数量相比，那些能够导致覆盖范围和/或发现的角落案例增加的测试输入数量相当低。</li>
</ul>
<h3 id="Oracle选择方面的局限性"><a href="#Oracle选择方面的局限性" class="headerlink" title="Oracle选择方面的局限性"></a>Oracle选择方面的局限性</h3><ul>
<li>最直接的方法是收集尽可能多的实际数据并手动标记以检查其是否正确。但是，这样的过程需要大量的手动工作。</li>
<li>在某些工作中，会使用同一任务的多个实现作为oracle，并将其中的的差异行为标记为角落案例行为。然而，我们观察到这种方法会错误地将某些角落情况分类为正确的行为，此外，此方法仅在具有多个高精度且相似的实现的应用程序中有效。</li>
</ul>
<h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p>在本文中，作者基于对内部决策逻辑的覆盖提出了一个包含两个因子的覆盖标准:  DNN中的每个三元组$(n_{i,k-1}, n_{j,k-1}, n_{q,k})$的组合覆盖情况</p>
<font color="red">（覆盖率居然没有形式化定义？？）</font>

<p>解释：</p>
<ul>
<li>每个神经元单独对下一层神经元值的条件影响<ul>
<li>受MC/DC启发</li>
</ul>
</li>
<li>上一层中神经元值的组合对下一层神经元值的影响<ul>
<li>受组合测试启发</li>
</ul>
</li>
</ul>
<p>对于一个初始测试输入的结果，我们通过联合优化来实现指导测试输入的生成。任何没有达到100%覆盖率的三元组被随机选择，以确定哪些激活值的组合没有被覆盖。比如若对一个DNN三元组激活实例：$n_{i,k-1}$激活, $n_{j,k-1}$不激活，$n_{q,k}$激活，优化目标为：（直接将三者相加）</p>
<p>$ F_{n,t}=f_{n_{i,k-1}}(t)+f_{n_{j,k-1}}(t)+f_{n_{q,k}}(t) $ </p>
<p>通过这种迭代修改输入的方式最大化目标函数，以达到期望的三元组某种激活状态，从而覆盖三元组不同的激活状态，达到100%覆盖。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul>
<li>数据集和模型：MNIST上的三个DNN——LeNet-1，LeNet-4和LeNet-5</li>
<li>覆盖指标：<ul>
<li>10个随机测试输入获得的覆盖率（理想情况下应该很低）</li>
<li>边角案例数与总测试输入数的比率</li>
</ul>
</li>
<li><p>借鉴DeepXplore，我们使用<strong>多种实现</strong>作为oracle，只有一种图像处理——<strong>亮度</strong>。</p>
</li>
<li><p>实验结果</p>
<ul>
<li><p>结果显示，前面工作在相同的指标下获得了更高的覆盖率和对比值，而使用作者提出的标准时覆盖率直线下降，体现出作者提出的标准较于先前工作粒度更细。</p>
<p><img src="/2020/08/26/Towards-Improved-Testing-For-Deep-Learning/Fig1.png" alt="Fig1" style="zoom:60%;"></p>
</li>
</ul>
</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/ChapterZ/article/details/96116870">https://blog.csdn.net/ChapterZ/article/details/96116870</a></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>测试标准</tag>
      </tags>
  </entry>
  <entry>
    <title>DNN测试输入生成论文可控制参数及变量总结</title>
    <url>/2020/08/20/Variables-Comparison/</url>
    <content><![CDATA[<h2 id="共同参数或变量总结"><a href="#共同参数或变量总结" class="headerlink" title="共同参数或变量总结"></a>共同参数或变量总结<a id="more"></a></h2><h3 id="初始输入种子集合相关变量"><a href="#初始输入种子集合相关变量" class="headerlink" title="初始输入种子集合相关变量"></a>初始输入种子集合相关变量</h3><ul>
<li>数量：1个到几千个不等</li>
<li>采样方式：均为随机<ul>
<li>DeepXplore还要求了<strong>类别平衡</strong></li>
</ul>
</li>
<li>是否标注<ul>
<li>若已标注，是否要求<strong>初始预测结果正确</strong>（DeepCT要求了）</li>
<li>未标注（DeepXplore要求其使用的多个模型预测一致，DLFuzz）</li>
</ul>
</li>
</ul>
<h3 id="测试标准-覆盖率相关变量"><a href="#测试标准-覆盖率相关变量" class="headerlink" title="测试标准/覆盖率相关变量"></a>测试标准/覆盖率相关变量</h3><ul>
<li>已有测试/覆盖标准<ul>
<li>神经元覆盖率（DeepXplore）<ul>
<li>DeepTest中修改了卷积层神经元覆盖率的计算方式：输出特征图的平均值与激活阈值做比较</li>
<li>神经元覆盖率采用的激活阈值<ul>
<li>原算法：将所有输出值缩放到[0,1]，然后设置阈值（<font color="red">离群点等会否导致不同数据集阈值差距很不同</font>）</li>
<li>DeepXplore中用过多种阈值（0、0.25、0.75等），后人多数用的0，也有0.75等</li>
</ul>
</li>
</ul>
</li>
<li>神经元主功能区、边界区覆盖率、top-k神经元覆盖率（DeepGauge)</li>
<li>MC/DC覆盖（DeepCover）</li>
<li>输入样本的Surprise Adequacy</li>
<li>2-路覆盖（Towards Improved Testing For Deep Learning）</li>
<li>重要神经元覆盖（DeepImportance）</li>
<li>近邻覆盖（TensorFuzz）</li>
<li>利普希茨覆盖（DeepConcolic)</li>
<li>组合覆盖（DeepCT，包括$t$-路组合稀疏覆盖、$t$-路组合稠密覆盖、$(p,t)$完备性）</li>
</ul>
</li>
<li>覆盖神经元集合的选择：<ul>
<li>多数选择所有层的神经元</li>
<li>个别选择去掉一些层<ul>
<li>如DeepXplore测试达到100%覆盖率所用的时间时去掉了全连接层<font color="red">（但覆盖全连接层意义应该更大？）</font></li>
<li>TensorFuzz要求用户自己选择层</li>
<li>DeepImportance仅选择部分层里重要的神经元覆盖</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="图像上增加的扰动相关变量"><a href="#图像上增加的扰动相关变量" class="headerlink" title="图像上增加的扰动相关变量"></a>图像上增加的扰动相关变量</h3><ul>
<li>扰动生成方式<ul>
<li>基于梯度优化</li>
<li>基于覆盖率引导的模糊测试</li>
<li>基于约束求解</li>
<li>基于搜索</li>
<li>基于已有的对抗样本生成方法</li>
</ul>
</li>
<li>扰动种类<ul>
<li>DeepTest说不同种类变换能激活不同神经元（<font color="red">存疑</font>）</li>
</ul>
</li>
<li>扰动位置：<ul>
<li>DeepXplore中的单个矩形位置由用户提前设定，多个黑色矩形的位置随机</li>
</ul>
</li>
<li>扰动大小：<ul>
<li>DeepXplore中的矩形大小由用户提前设定（与数据集图像大小有关）</li>
</ul>
</li>
<li>扰动约束条件：<ul>
<li>$L_0-norm$<ul>
<li>如DeepCheck（识别重要像素攻击）</li>
</ul>
</li>
<li>$L_1-norm$<ul>
<li>DeepXplore未对距离做限制，甚至认为$L_1$距离越<strong>大</strong>多样性越好</li>
</ul>
</li>
<li>$L_2-norm$<ul>
<li>如DLFuzz：满足L2距离（&lt;0.02）（计算方式为L2_norm / orig_L2_norm）</li>
</ul>
</li>
<li>$L\infty-norm$<ul>
<li>如TensorFuzz</li>
</ul>
</li>
<li>平衡$L_0$和$L\infty-norm$<ul>
<li>如DeepHunter采用的</li>
</ul>
</li>
<li>约束MSE（DeepTest)：$|MSE(trans,param)-MSE_{org}|\leq \epsilon$ <font color="red">(用到了oracle)</font></li>
</ul>
</li>
</ul>
<h3 id="模糊测试相关变量（DLFuzz、TensorFuzz、DeepHunter、DeepTest）"><a href="#模糊测试相关变量（DLFuzz、TensorFuzz、DeepHunter、DeepTest）" class="headerlink" title="模糊测试相关变量（DLFuzz、TensorFuzz、DeepHunter、DeepTest）"></a>模糊测试相关变量（DLFuzz、TensorFuzz、DeepHunter、DeepTest）</h3><ul>
<li>判断种子是否应该保留在队列时最少需提升的覆盖率</li>
<li>每个种子的迭代次数</li>
<li>从队列中优先挑选哪些种子的策略<ul>
<li>随机</li>
<li>选择新鲜的</li>
<li>平衡新鲜和多样性</li>
</ul>
</li>
</ul>
<h3 id="优化算法相关变量"><a href="#优化算法相关变量" class="headerlink" title="优化算法相关变量"></a>优化算法相关变量</h3><ul>
<li>优化目标选择需要新激活的神经元个数</li>
</ul>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul>
<li>每个种子生成对抗样本的个数<ul>
<li>有的方法只生成一张，有的多张</li>
</ul>
</li>
<li>蜕变关系<ul>
<li>图像分类问题：扰动满足约束的情况下，预测类别应该不变</li>
<li>回归问题：<ul>
<li>DeepTest：因为只测试了Driving数据集（回归问题），使用MSE判断蜕变关系是否满足$(\theta_i-\theta_{ti}) \leq \lambda MSE_{orig}$</li>
<li>DeepRoad：没有使用MSE，直接比较输出结果是否小于阈值$\epsilon$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="测试输入生成类论文总结"><a href="#测试输入生成类论文总结" class="headerlink" title="测试输入生成类论文总结"></a>测试输入生成类论文总结</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">文章名</th>
<th>可控制参数/变量总结</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">DeepXplore<br>(SOSP’17)</td>
<td><img src="/2020/08/20/Variables-Comparison/DeepXplore.png" alt="DeepXplore"></td>
</tr>
<tr>
<td style="text-align:center">TensorFuzz<br>(ICML’19)</td>
<td><img src="/2020/08/20/Variables-Comparison/TensorFuzz.png" alt="TensorFuzz" style="zoom:80%;"></td>
</tr>
<tr>
<td style="text-align:center">DeepTest<br>(ICSE’18)</td>
<td><img src="/2020/08/20/Variables-Comparison/DeepTest.png" alt="DeepTest" style="zoom:80%;"></td>
</tr>
<tr>
<td style="text-align:center">DLFuzz<br>(ESEC/FSE’18)</td>
<td><img src="/2020/08/20/Variables-Comparison/DLFuzz.png" alt="DLFuzz" style="zoom: 80%;"></td>
</tr>
<tr>
<td style="text-align:center">DeepHunter<br>（ISSTA’19）</td>
<td><img src="/2020/08/20/Variables-Comparison/DeepHunter.png" alt="DeepHunter" style="zoom:80%;"></td>
</tr>
<tr>
<td style="text-align:center">DeepConcolic<br>（ASE’18）</td>
<td><img src="/2020/08/20/Variables-Comparison/DeepConcolic.png" alt="DeepConcolic" style="zoom: 80%;"></td>
</tr>
<tr>
<td style="text-align:center">DeepCT<br>(SANER’19)<br><font color="red">(未找到代码)</font></td>
<td><img src="/2020/08/20/Variables-Comparison/DeepCT.png" alt="DeepCT" style="zoom: 80%;"></td>
</tr>
<tr>
<td style="text-align:center">DeepCheck<br>（ISSRE’18）<br><font color="red">(未找到代码)</font></td>
<td><img src="/2020/08/20/Variables-Comparison/DeepCheck.png" alt="DeepCheck" style="zoom: 80%;"></td>
</tr>
<tr>
<td style="text-align:center">DeepRoad<br>（ASE’18）<br><font color="red">(未找到代码)</font></td>
<td><img src="/2020/08/20/Variables-Comparison/DeepRoad.png" alt="DeepRoad" style="zoom:80%;"></td>
</tr>
<tr>
<td style="text-align:center">DeepGauge<br>(ASE’18)</td>
<td><img src="/2020/08/20/Variables-Comparison/DeepGauge.png" alt="DeepGauge" style="zoom:80%;"></td>
</tr>
<tr>
<td style="text-align:center">DeepCover<br>(TECS’19)</td>
<td><img src="/2020/08/20/Variables-Comparison/DeepCover.png" alt="DeepCover" style="zoom:80%;"></td>
</tr>
<tr>
<td style="text-align:center">Surprise Adequacy(ICSE’19)</td>
<td><img src="/2020/08/20/Variables-Comparison/Surprise.png" alt="Surprise" style="zoom:80%;"></td>
</tr>
<tr>
<td style="text-align:center">Towards Improved Testing For Deep Learning<br>（ICSE-NIER’19）<br><font color="red">(未找到代码)</font></td>
<td>借鉴DeepXplore使用<strong>多种实现</strong>作为oracle，只用了一种图像处理方式——<strong>亮度</strong>。</td>
</tr>
<tr>
<td style="text-align:center">DeepImportance<br>(ICSE’20)</td>
<td><img src="/2020/08/20/Variables-Comparison/DeepImportance.png" alt="DeepImportance" style="zoom:80%;"></td>
</tr>
</tbody>
</table>
</div>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>科研笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows后台运行Python脚本，开机自动运行</title>
    <url>/2020/10/14/Windows%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8CPython/</url>
    <content><![CDATA[<p>为了每天自动约洗澡也是拼了<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8">😅</span> <a id="more"></a></p>
<h2 id="后台运行python脚本"><a href="#后台运行python脚本" class="headerlink" title="后台运行python脚本"></a>后台运行python脚本</h2><p>已知目前有一每天定时预约洗澡的Python脚本shower.py，cmd设置后台运行：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">pythonw shower.py</span><br></pre></td></tr></tbody></table></figure>
<p>pythonw.exe是无窗口的Python可执行程序，意思是在运行程序的时候，没有窗口，代码在后台执行。</p>
<p>注意如果像我一样电脑同时安装了Python2 和Python3，需要区分用的是哪个phthonw.exe，最简单的是使用绝对路径：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">D:\python37\pythonw.exe E:\shower.py</span><br></pre></td></tr></tbody></table></figure>
<p>可打开任务管理器检查pythonw进程是否已经启动。</p>
<h2 id="设置开机自动启动"><a href="#设置开机自动启动" class="headerlink" title="设置开机自动启动"></a>设置开机自动启动</h2><p>1、新建批处理文件run_shower.bat：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">@echo off </span><br><span class="line">if "%1"=="h" goto begin </span><br><span class="line">start mshta vbscript:createobject("wscript.shell").run("""%~nx0"" h",0)(window.close)&amp;&amp;exit </span><br><span class="line">:begin </span><br><span class="line">::</span><br><span class="line">start /b cmd /k "D:\python37\pythonw.exe E:\shower.py"</span><br></pre></td></tr></tbody></table></figure>
<p>这段代码可以隐藏批处理运行的窗口。</p>
<p>解释：</p>
<blockquote>
<p>如果双击一个批处理，等价于参数为空，而一些应用程序需要参数，比如在cmd窗口输入shutdowm -s -t 0,其中-s -t 0就为参数。shutdown为%0，-s为%1，-t为%2，以此类推。<br>第一行我们先跳过，看第二行，表示利用mshta创建一个vbs程序，内容为:createobject(“wscript.shell”).run(……).<br>如果运行的批处理名为a.bat，在C:\下，那%0代表C:\a.bat，%~nx0代表a.bat。h为参数%1，0表示隐藏运行。<br>由于你双击运行，故第一次批处理%1为空，if不成立，转而运行下一句。然后再次打开自己，并传递参数h，此时if成立，跳转至begin开始运行。<br>这两行很经典，可以使批处理无窗口运行。</p>
</blockquote>
<p>2、将bat文件放在开机启动项里：Win+R打开运行窗口，输入shell:startup，将bat文件复制进启动文件夹里。</p>
<p>3、重启测试</p>
<p>参考：<a href="https://www.cnblogs.com/nmap/articles/8329125.html">https://www.cnblogs.com/nmap/articles/8329125.html</a></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>常用技巧</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】mMutant</title>
    <url>/2020/11/02/mMutant/</url>
    <content><![CDATA[<p>原文：Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing  (ICSE’19)  <a id="more"></a></p>
<p>代码地址：<a href="https://github.com/dgl-prc/m_testing_adversatial_sample">https://github.com/dgl-prc/m_testing_adversatial_sample</a></p>
<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>目前对抗样本检测方法：</p>
<ul>
<li>训练数据扩充+重训练：不能防止未知的对抗样本</li>
<li>鲁棒优化、对抗训练：增加了训练成本</li>
<li>测试标准、黑盒测试、白盒测试、concolic测试等：不能提升DNN鲁棒性，也不能为DNN在被对抗攻击时提供鲁棒性保证</li>
<li>形式化验证DNN的鲁棒性：高成本、只针对少量特定类型DNN和特定性质</li>
</ul>
<p>本文：</p>
<ul>
<li>通过DNN模型的变异测试和提出了一种运行时检测对抗样本的方法。</li>
<li>基于的观察：相比正常样本，对抗样本对于模型的变异更加敏感（预测标签更易改变）</li>
</ul>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="模型变异方法"><a href="#模型变异方法" class="headerlink" title="模型变异方法"></a>模型变异方法</h3><p>采用DeepMutation中的4个模型级变异方法（无须训练）：</p>
<ul>
<li><p>Gaussian Fuzzing（GF）：根据高斯分布扰动权重</p>
<p><img src="/2020/11/02/mMutant/4.png" alt="4" style="zoom:70%;"></p>
</li>
<li><p>Weight Shuffing（WS）：随机排列选中的权重</p>
<p><img src="/2020/11/02/mMutant/5.png" alt="5" style="zoom:67%;"></p>
</li>
<li><p>Neuron Switch（NS）：交换同一层的两个神经元</p>
<p><img src="/2020/11/02/mMutant/6.png" alt="6" style="zoom:67%;"></p>
</li>
<li><p>Neuron Activation Inverse（NAI）：变更神经元的激活状态</p>
<p><img src="/2020/11/02/mMutant/3.png" alt="3" style="zoom:70%;"></p>
</li>
</ul>
<h3 id="验证假设（Empirical-study）"><a href="#验证假设（Empirical-study）" class="headerlink" title="验证假设（Empirical study）"></a>验证假设（Empirical study）</h3><p>通过实验验证对抗样本和正常样本的标签改变率（Label change rate， LCR）的差异。由于生成的变异模型可能存在质量很低的情况，因此我们扔掉了在验证集上低准确率的模型（如准确率低于90%的）。</p>
<p>令变异后的模型集合为$F$，变异模型$f_i$对于输入$x$的预测标签$f_i(x)$，LCR的计算方法：</p>
<p>$\zeta ( x ) = \frac { | \{ f _ { i } | f _ { i } \in F \text { and } f _ { i } ( x ) \neq f ( x ) \} | } { | F | }$</p>
<p>直觉上，LCR衡量了一个输入$x$对于DNN变异模型的敏感程度。</p>
<p><strong>在MNIST和CIFAR-10上的实证研究</strong></p>
<ul>
<li>使用NAI变异方法生成了500个变异模型（随机选择一些神经元，更改激活状态）</li>
<li>实验结果：对抗样本的预测标签更易受到扰动<img src="/2020/11/02/mMutant/1.png" alt="1"></li>
<li>解释：对抗样本常处于决策边界附近，更易受到模型扰动的干扰。</li>
</ul>
<h3 id="检测算法"><a href="#检测算法" class="headerlink" title="检测算法"></a>检测算法</h3><ul>
<li><p>输入：一个DNN模型$f$，一个样本$x$，阈值$\zeta_ h$（可自动计算获得）</p>
</li>
<li><p>使用假设检验：</p>
<p>​    $ H_{0}: \zeta (x) \geqslant \zeta_ h $</p>
<p>​    $ H_{1}: \zeta (x) \leqslant \zeta_ h $</p>
<ul>
<li>三个标准参数$\alpha$、$\beta$、$\delta$，控制错误可能性<ul>
<li>Type I 错误（H0为真但拒绝H0）的可能性小于$\alpha$</li>
<li>Type II 错误（H1为真但拒绝H1）的可能性小于$\beta$</li>
<li>indifferent region：$(r-\delta,r+\delta)$</li>
</ul>
</li>
</ul>
</li>
<li><p>不断生成mutate后的模型（准确率大于阈值），直到触发停止条件。（模型可保存使用，无需运行时生成）</p>
</li>
<li><p>两种方法决定算法是否停止（我们有足够信心拒绝假设）</p>
<ul>
<li><p>1、Fixed-size Sampling Test (FSST)：运行固定数量的测试</p>
</li>
<li><p>2、Sequential Probability Ratio Test (SPRT)：模型数量不固定，每次更新阈值$\zeta_ h$后自动判断是否停止，一般运行速度更快（拒绝假设后就停止运行）。</p>
<ul>
<li>SPRT概率计算方法：$ pr= \frac{p_{1}^{z}(1-p_{1})^{n-z}}{p_{0}^{z}(1-p_{0})^{n-z}} $，其中$p_1=\zeta_ h - \delta$，$p_0=\zeta_ h + \delta$</li>
</ul>
<p><img src="/2020/11/02/mMutant/2.png" alt="2" style="zoom:67%;"></p>
</li>
</ul>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul>
<li><p>模型和数据集</p>
<ul>
<li>MNIST-LeNet：236个神经元</li>
<li>CIFAR10-GoogleNet：7914个神经元</li>
</ul>
</li>
<li><p>变异率</p>
<ul>
<li>MNIST模型：{0.01,0.03,0.05}</li>
<li>CIFAR10模型：{0.003,0.005,0.007}</li>
</ul>
</li>
<li><p>保留测试集上准确率高于90%的变异模型。每种变异率生成500个变异模型。</p>
</li>
<li><p>对抗样本生成方法</p>
<ul>
<li>FGSM</li>
<li>JSMA</li>
<li>C&amp;W</li>
<li>DeepFool</li>
<li>Black-Box</li>
</ul>
</li>
<li><p>每种攻击方法生成1000张图片（不一定都攻击成功）</p>
<p><img src="/2020/11/02/mMutant/7.png" alt="7" style="zoom:75%;"></p>
</li>
<li><p>评价指标</p>
<ul>
<li>标签改变率的距离$d_{lcr}=\zeta_{adv}/\zeta_{nor}$，越大表明对抗样本与正常样本的标签改变率差异越大</li>
<li>ROC、AUC</li>
<li>检测准确率：二分类（正常/异常样本）准确率</li>
</ul>
</li>
</ul>
<h3 id="RQ1-正常样本和对抗样本的LCR是否有显著差异？"><a href="#RQ1-正常样本和对抗样本的LCR是否有显著差异？" class="headerlink" title="RQ1 正常样本和对抗样本的LCR是否有显著差异？"></a>RQ1 正常样本和对抗样本的LCR是否有显著差异？</h3><p>结合Table II和IV可见，对抗样本LCR显著高于正常样本。</p>
<p><img src="/2020/11/02/mMutant/8.png" alt="8" style="zoom:80%;"></p>
<p><img src="/2020/11/02/mMutant/9.png" alt="9"></p>
<h3 id="RQ2：LCR是否适合作为检测指标？"><a href="#RQ2：LCR是否适合作为检测指标？" class="headerlink" title="RQ2：LCR是否适合作为检测指标？"></a>RQ2：LCR是否适合作为检测指标？</h3><p>多数情况下LCR比baseline的AUROC更高。</p>
<p><img src="/2020/11/02/mMutant/10.png" alt="10" style="zoom:80%;"></p>
<h3 id="RQ3：检测对抗样本的效果如何？"><a href="#RQ3：检测对抗样本的效果如何？" class="headerlink" title="RQ3：检测对抗样本的效果如何？"></a>RQ3：检测对抗样本的效果如何？</h3><p>检测精度和最少mutation次数：</p>
<ul>
<li>设置$\zeta_h=\rho \cdot \zeta_{nor}$，其中$\zeta_{nor}$为正常样本lcr的上界</li>
</ul>
<p><img src="/2020/11/02/mMutant/11.png" alt="11"></p>
<h3 id="RQ4：检测算法的开销如何？"><a href="#RQ4：检测算法的开销如何？" class="headerlink" title="RQ4：检测算法的开销如何？"></a>RQ4：检测算法的开销如何？</h3><p>$c_g$：生成变异模型的时间</p>
<p>$c_f$：前向传播预测的时间</p>
<p>检测一张图片的时间开销：$C=n(c_g+c_f)$，$n$为生成模型的个数</p>
<p>检测$m$张图片的时间开销：$C(m)=m\cdot n \cdot c_f + n \cdot c_g$</p>
<p><img src="/2020/11/02/mMutant/12.png" alt="12" style="zoom:80%;"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>DNN测试</tag>
        <tag>变异测试</tag>
      </tags>
  </entry>
  <entry>
    <title>【算法复习】基础知识——算法分析、分治策略</title>
    <url>/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul>
<li><p><strong>算法</strong>的定义：定义良好的计算过程。即一系列的计算步骤，用来将输入数据转换成输出结果。</p>
</li>
<li><p>正确的算法：如果一个算法对其每一个输入实例，都能输出正确的结果并停止，则称它是正确的。</p>
</li>
</ul>
<a id="more"></a>
<h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2><ul>
<li>算法的运行时间：特定输入时，算法执行的基本操作数（步数）。</li>
<li><p>一般考察算法的最坏运行时间</p>
</li>
<li><p>$\Theta$记号：</p>
<ul>
<li><p>$\Theta(g(n))=\{f(n):存在正常量c_1、c_2和n_0，使得对于所有n\geq n_0，有0 \leq c_1g(n) \leq f(n)\leq c_2g(n)\}$</p>
<p><img src="/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/算法分析1.png" alt="算法分析1" style="zoom:70%;"></p>
</li>
</ul>
</li>
<li><p>$O$记号和$\Omega$记号</p>
<ul>
<li>$O(g(n))=\{f(n):存在正常量c和n_0，使得对于所有n\geq n_0，有0 \leq f(n)\leq cg(n)\}$</li>
<li>$\Omega(g(n))=\{f(n):存在正常量c和n_0，使得对于所有n\geq n_0，有0 \leq cg(n) \leq f(n)\}$</li>
</ul>
</li>
<li><p>$f(n)=\Theta(g(n))$当且仅当$f(n)=O(g(n))$且$f(n)=\Omega(g(n))$</p>
</li>
</ul>
<h2 id="分治策略（divide-and-conquer"><a href="#分治策略（divide-and-conquer" class="headerlink" title="分治策略（divide-and-conquer)"></a>分治策略（divide-and-conquer)</h2><p>把原问题划分成n个规模较小而结构与原问题相似的子问题；递归地解决这些问题，然后合并其结果，就得到原问题的解。</p>
<ul>
<li>分解（Divide）：划分子问题</li>
<li>解决（Conquer）：递归地求解子问题</li>
<li>合并（Combine）：合并子问题的解</li>
</ul>
<h3 id="最大子数组问题"><a href="#最大子数组问题" class="headerlink" title="最大子数组问题"></a>最大子数组问题</h3><p>分治法求解数组A的和最大的非空连续子数组。</p>
<ul>
<li><p>三种情况：</p>
<p><img src="/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/最大子数组1.png" alt="最大子数组1" style="zoom:75%;"></p>
</li>
<li><p>求解跨越中点的子数组（$\Theta(n)$）：必须包含中点，向左右依次寻找最大数组然后合并即可</p>
<p><img src="/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/最大子数组2.png" alt="最大子数组2" style="zoom:80%;"></p>
</li>
<li><p>递归求解：</p>
<p><img src="/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/最大子数组3.png" alt="最大子数组3" style="zoom:80%;"></p>
</li>
<li><p>时间开销：$\Theta(nlgn)$</p>
</li>
</ul>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// LeetCode 53: 最大子序和</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> MAX_INT = <span class="number">0xFFFFFFF</span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">max</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b, <span class="keyword">int</span> c)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span> (a &gt;= b &amp;&amp; a &gt;= c)</span><br><span class="line">            <span class="keyword">return</span> a;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (b &gt;= a &amp;&amp; b &gt;= c)</span><br><span class="line">            <span class="keyword">return</span> b;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> c;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findCrossingMaxSumArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> mid, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">int</span> left_max = -MAX_INT, right_max = -MAX_INT, left_sum = <span class="number">0</span>, right_sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = mid; i &gt;= left; i--)</span><br><span class="line">        {</span><br><span class="line">            left_sum += a[i];</span><br><span class="line">            <span class="keyword">if</span> (left_sum &gt; left_max)</span><br><span class="line">                left_max = left_sum;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = mid+<span class="number">1</span>; i &lt;= right; i++)</span><br><span class="line">        {</span><br><span class="line">            right_sum += a[i];</span><br><span class="line">            <span class="keyword">if</span> (right_sum &gt; right_max)</span><br><span class="line">                right_max = right_sum;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> left_max + right_max;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findMaxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span>(left == right)</span><br><span class="line">            <span class="keyword">return</span> a[left];</span><br><span class="line">        <span class="keyword">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">int</span> left_max = findMaxSubArray(a, left, mid);</span><br><span class="line">        <span class="keyword">int</span> right_max = findMaxSubArray(a, mid+<span class="number">1</span>, right);</span><br><span class="line">        <span class="keyword">int</span> crossing_max = findCrossingMaxSumArray(a, left, mid, right);</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(left_max, right_max, crossing_max);</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>{</span><br><span class="line">        <span class="keyword">return</span> findMaxSubArray(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>()<span class="number">-1</span>);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure>
<h3 id="矩阵乘法的Strassen算法"><a href="#矩阵乘法的Strassen算法" class="headerlink" title="矩阵乘法的Strassen算法"></a>矩阵乘法的Strassen算法</h3><ul>
<li>减少一次递归，时间开销：$\Theta(n^{lg7})$，即$O(n^{2.81})$</li>
</ul>
<h3 id="求解递归式的方法"><a href="#求解递归式的方法" class="headerlink" title="求解递归式的方法"></a>求解递归式的方法</h3><ul>
<li>代入法：猜测解的形式，数学归纳法证明</li>
<li>递归树</li>
<li>主方法<ul>
<li>$T(n) = aT(n/b) + f(n)$</li>
</ul>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法导论</tag>
      </tags>
  </entry>
  <entry>
    <title>【算法复习】排序和顺序统计量</title>
    <url>/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="排序算法总结"><a href="#排序算法总结" class="headerlink" title="排序算法总结"></a>排序算法总结</h3><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/小结.png" alt="小结" style="zoom: 80%;"></p>
<ul>
<li>任何比较排序在最坏情况下都要经过$\Omega(nlgn)$次比较，因此归并排序和堆排序都是渐近最优的</li>
</ul>
<a id="more"></a>
<h3 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h3><ul>
<li>把第j个数作为key插入到A[1…j-1]的有序数列中：从后向前查找，将大于key的数向后移</li>
</ul>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/插入排序.png" alt="插入排序"></p>
<ul>
<li>时间开销：$\Theta(n^2)$</li>
<li>空间开销：原址的（仅有常数个元素 需要在排序过程中存储在数组之外）</li>
</ul>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InsertionSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; a.<span class="built_in">size</span>(); i++)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">int</span> key = a[i];</span><br><span class="line">        <span class="keyword">int</span> j = i - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(j &gt;= <span class="number">0</span> &amp;&amp; a[j] &gt; key)</span><br><span class="line">        {</span><br><span class="line">            a[j + <span class="number">1</span>] = a[j];</span><br><span class="line">            j--;</span><br><span class="line">        }</span><br><span class="line">        a[j + <span class="number">1</span>] = key;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="合并排序"><a href="#合并排序" class="headerlink" title="合并排序"></a>合并排序</h3><ul>
<li><p>思想</p>
<ul>
<li>分解（Divide）：将n个元素分成各含n/2个元素的子序列；</li>
<li>解决（Conquer）：用合并排序法对两个子序列递归地排序；</li>
<li>合并（Combine）：合并两个已排序的子序列以得到排序结果。</li>
</ul>
</li>
<li><p>辅助过程<strong>MERGE</strong>：（合并两个子数组A[p…q], A[q+1…r]）</p>
<ul>
<li>把两个子数组复制到两个新的数组L和R，合并两数组</li>
</ul>
</li>
</ul>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/合并排序1.png" alt="合并排序1"></p>
<ul>
<li><p>递归过程：MERGE-SORT</p>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/合并排序2.png" alt="合并排序2" style="zoom:60%;"></p>
</li>
<li><p>递归式分析代价：<script type="math/tex">T(n)= \begin{cases} \Theta(1)& \text{n=1}\\ 2T(n/2)+\Theta(n) & \text{n>1} \end{cases}</script></p>
<ul>
<li><p>递归树</p>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/合并排序3.png" alt="合并排序3" style="zoom:60%;"></p>
</li>
</ul>
</li>
<li><p>时间开销：$\Theta(nlgn)$</p>
</li>
<li><p>空间开销：非原址</p>
</li>
</ul>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> mid, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> n1 = mid - left + <span class="number">1</span>, n2 = right - mid;</span><br><span class="line">    vector&lt;int&gt; L(n1), R(n2);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n1; i++)</span><br><span class="line">        L[i] = a[left + i];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n2; i++)</span><br><span class="line">        R[i] = a[mid + <span class="number">1</span> + i];</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> k = left; k &lt;= right; k++)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (i &lt; n1 &amp;&amp; j &lt; n2)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (L[i] &lt;= R[j])</span><br><span class="line">                a[k] = L[i++];</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                a[k] = R[j++];</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (i == n1)</span><br><span class="line">            a[k] = R[j++];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            a[k] = L[i++];</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MergeSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span> (left &lt; right)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">        MergeSort(a, left, mid);</span><br><span class="line">        MergeSort(a, mid+<span class="number">1</span>, right);</span><br><span class="line">        Merge(a, left, mid, right);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><ul>
<li><p>思想：反复交换相邻的未按次序排序的元素</p>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/冒泡排序1.png" alt="冒泡排序1" style="zoom:67%;"></p>
</li>
<li><p>时间开销：$\Theta(n^2)$</p>
</li>
</ul>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BubbleSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.<span class="built_in">size</span>() - <span class="number">1</span>; i++)   <span class="comment">// 已到位数字数目</span></span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; a.<span class="built_in">size</span>() - i - <span class="number">1</span>; j++)   <span class="comment">//遍历剩余的相邻项</span></span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(a[j] &gt; a[j + <span class="number">1</span>])</span><br><span class="line">            {</span><br><span class="line">                <span class="keyword">int</span> temp = a[j];</span><br><span class="line">                a[j] = a[j + <span class="number">1</span>];</span><br><span class="line">                a[j + <span class="number">1</span>] = temp;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><h4 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h4><p>一个近似的完全二叉树，除了最底层外，该树是完全充满的。</p>
<ul>
<li>$A.length$：数组长度</li>
<li>$A.heap-size$：当前有效元素个数</li>
<li>根节点：$A[1]$</li>
<li>给定一个节点下标$i$，计算父节点、左孩子、右孩子的下标：</li>
</ul>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/堆1.png" alt="堆1" style="zoom: 80%;"></p>
<p><strong>堆的性质：</strong></p>
<ul>
<li>最大堆性质：除了根节点以外所有节点$i$满足：$A[PARENT(i)]\geq A[i]$</li>
<li>最小堆性质：除了根节点以外所有节点$i$满足：$A[PARENT(i)]\leq A[i]$</li>
<li>树的高度：一个包含$n$个元素的堆的高度为$\Theta(lgn)$</li>
</ul>
<h4 id="维护堆的性质"><a href="#维护堆的性质" class="headerlink" title="维护堆的性质"></a>维护堆的性质</h4><ul>
<li>输入：堆$A$，节点$i$，<u>假设此时以左右子节点为根的子树都已是最大堆</u></li>
<li>输出：当节点$i$违背最大堆性质（值小于其子节点时），让其值在最大堆中逐级下降，从而使得<strong>以$i$为根节点的子树</strong>重新遵循最大堆的性质。</li>
<li>在父节点、左孩子、右孩子中选出最大的，若最大的不是父节点，则将该子节点与父节点交换，对该子节点递归地调用函数。</li>
<li>代价：$O(lgn)$</li>
</ul>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/维护堆的性质.png" alt="维护堆的性质"></p>
<h4 id="建堆"><a href="#建堆" class="headerlink" title="建堆"></a>建堆</h4><p>从最后一个非叶节点到根节点，依次调用MAX-HEAPIFY方法</p>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/建堆.png" alt="建堆"></p>
<ul>
<li>代价：$O(n)$</li>
</ul>
<h4 id="堆排序-1"><a href="#堆排序-1" class="headerlink" title="堆排序"></a>堆排序</h4><p>首先调用BUILD-MAX-HEAP构造最大堆，此时最大元素在$A[1]$中，交换$A[1]$和$A[n]$，此时新的根节点可能违背了最大堆性质，调用MAX_HEAPIFY(A,1)，从而在$A[1…n-1]$上构造一个新的最大堆。以此类推，直到堆的大小降到2。</p>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/堆排序.png" alt="堆排序"></p>
<ul>
<li><p>时间开销：$O(lgn)$</p>
</li>
<li><p>空间开销：原址的</p>
</li>
</ul>
<h4 id="最大优先队列"><a href="#最大优先队列" class="headerlink" title="最大优先队列"></a>最大优先队列</h4><ul>
<li>$MAXIMUM(S)$：返回$S$中最大键字的元素。（return A[1]）<ul>
<li>$\Theta(1)$</li>
</ul>
</li>
<li>$EXTRACT-MAX(S)$：去掉并返回S中具有最大键字的元素。<ul>
<li>交换A[1]和A[n]，调用MAX_HEAPIFY(A,1)</li>
<li>$O(lgn)$</li>
</ul>
</li>
<li>$INCREASE(S,x,k)$：将元素x的关键字值增加到k（假设k不小于x节点原关键字值）<ul>
<li>将新关键字值不断与父节点进行比较，大于则交换</li>
<li>$O(lgn)$</li>
</ul>
</li>
<li>$INSERT(S,x)$：把元素x插入到S中。<ul>
<li>在最后增加一个大小为$-\infin$节点，调用$INCREASE(A,A.heapsize,key)$</li>
<li>$O(lgn)$</li>
</ul>
</li>
</ul>
<h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><p>PARTITION：将数组$A[p…r]$划分成两个子数组$A[p…q-1]$和$A[q+1…r]$，使得$A[p…q-1]$中每个元素都小于等于$A[q]$，$A[q]$也小于等于$A[q+1…r]$中每个元素。</p>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/快速排序.png" alt="快速排序" style="zoom:80%;"></p>
<ul>
<li><p>PARTITION过程</p>
<ul>
<li>选择$x=A[r]$作为主元</li>
<li>$A[p…i]$：小于等于主元的部分</li>
<li>$A[i+1…j-1]$：大于主元的部分</li>
<li>$A[j…r-1]$：尚未考虑的部分</li>
<li>复杂度：$\Theta(n)$，其中$n=r-p+1$</li>
</ul>
</li>
<li><p>时间开销：最坏$\Theta(n^2)$，最好$O(nlgn)$</p>
</li>
</ul>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> x = a[right];   <span class="comment">// 主元</span></span><br><span class="line">    <span class="keyword">int</span> i = left - <span class="number">1</span>;       <span class="comment">// i标记小于x、大于x的分界点（a[i]是最后一个小于主元的元素，a[i+1]是第一个大于主元的元素）</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = left; j &lt;= right - <span class="number">1</span>; j++)     <span class="comment">// j遍历每个除主元外的元素</span></span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span>(a[j] &lt;= x)</span><br><span class="line">        {</span><br><span class="line">            i ++;        </span><br><span class="line">            swap(a[i], a[j]);        <span class="comment">//将新的a[j]与第一个大于主元的元素交换</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    swap(a[i + <span class="number">1</span>], a[right]);    <span class="comment">// 放置主元</span></span><br><span class="line">    <span class="keyword">return</span> i + <span class="number">1</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">QuickSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span> (left &lt; right)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">int</span> pivot = Partition(a, left, right);</span><br><span class="line">        QuickSort(a, left, pivot - <span class="number">1</span>);</span><br><span class="line">        QuickSort(a, pivot + <span class="number">1</span>, right);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h3><p>假设$n$个输入元素都是在0到$k$区间内的一个整数。对于每一个输入元素$x$，确定小于x的元素个数，利用这一信息，就可以直接把x放到它在输出数组中的位置上了。例如有17个元素小于x，则x就应该在第18个输出位置上。</p>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/计数排序.png" style="zoom:80%;"></p>
<ul>
<li>代价：当$k=O(n)$时，排序的运行时间为$\Theta(n)$</li>
</ul>
<h3 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h3><p>从最低有效位到最高有效位进行排序。</p>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/基数排序2.png" alt="基数排序2" style="zoom:80%;"></p>
<ul>
<li>代价：给定$n$个$d$位数，其中每一个数位可能有$k$个可能的取值，若使用的稳定排序方法耗时$n+k$，那么它就可以在$\Theta(d(n+k))$时间内将这些数排好序。</li>
</ul>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/基数排序.png" alt="基数排序" style="zoom:80%;"></p>
<h3 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h3><p>假设数据服从$[0,1)$上的$均匀分布</p>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/桶排序.png" alt="桶排序" style="zoom:80%;"></p>
<ul>
<li>时间代价：$O(n)$</li>
</ul>
<h2 id="中位数和顺序统计量"><a href="#中位数和顺序统计量" class="headerlink" title="中位数和顺序统计量"></a>中位数和顺序统计量</h2><h3 id="同时找到最大值和最小值"><a href="#同时找到最大值和最小值" class="headerlink" title="同时找到最大值和最小值"></a>同时找到最大值和最小值</h3><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/最大值最小值.png" alt="最大值最小值" style="zoom: 67%;"></p>
<h3 id="期望时间为线性的选择算法"><a href="#期望时间为线性的选择算法" class="headerlink" title="期望时间为线性的选择算法"></a>期望时间为线性的选择算法</h3><p>一种分治算法，用到了快速排序中的RANDOMIZED-SELECT算法，但快排会递归处理划分的两边，这里只处理一边。</p>
<ul>
<li>算法返回数组$A[p…r]$中第$i$小的元素</li>
<li>期望运行时间：$\Theta(n)$</li>
<li>最坏运行时间：$\Theta(n^2)$</li>
</ul>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/随机选择.png" alt="随机选择" style="zoom:80%;"></p>
<figure class="highlight c++"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">//LeetCode 215: 在未排序的数组中找到第 k 个最大的元素。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">int</span> x = a[right];</span><br><span class="line">        <span class="keyword">int</span> i = left - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = left; j &lt;= right - <span class="number">1</span>; j++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (a[j] &gt;= x)    <span class="comment">// 注意这里改成了&gt;=</span></span><br><span class="line">            {</span><br><span class="line">                i++;</span><br><span class="line">                swap(a[i], a[j]);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        swap(a[i+<span class="number">1</span>], a[right]);</span><br><span class="line">        <span class="keyword">return</span> i + <span class="number">1</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Select</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> left, <span class="keyword">int</span> right, <span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span> (left == right)</span><br><span class="line">            <span class="keyword">return</span> a[left];</span><br><span class="line">        <span class="keyword">int</span> x = Partition(a, left, right);</span><br><span class="line">        <span class="keyword">int</span> k = x - left + <span class="number">1</span>;   <span class="comment">// 当前的x是第几个数字</span></span><br><span class="line">        <span class="keyword">if</span> (i == k)</span><br><span class="line">            <span class="keyword">return</span> a[x];</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (i &lt; k)</span><br><span class="line">            <span class="keyword">return</span> Select(a, left, x - <span class="number">1</span>, i);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> Select(a, x + <span class="number">1</span>, right, i - k);</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findKthLargest</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> Select(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>()<span class="number">-1</span>, k);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure>
<h3 id="最坏情况为线性的选择算法"><a href="#最坏情况为线性的选择算法" class="headerlink" title="最坏情况为线性的选择算法"></a>最坏情况为线性的选择算法</h3><p>与随机选择相比，保证对数组能做出一个较好的划分：</p>
<p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/select.png" alt="select" style="zoom: 67%;"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法导论</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>《人生的智慧》摘抄（不断更新）</title>
    <url>/2020/09/03/%E4%BA%BA%E7%94%9F%E7%9A%84%E6%99%BA%E6%85%A7/</url>
    <content><![CDATA[<p><code>摆在书架上很久的书，心血来潮拿下来觉得该读一读了。这本书与“幸福论”有关，教导人们如何尽量称心、愉快地度过一生这样一门艺术。在此摘抄下一些觉得有道理的句子，以备将来不时之需。</code><a id="more"></a></p>
<h2 id="第1章-基本的划分"><a href="#第1章-基本的划分" class="headerlink" title="第1章 基本的划分"></a>第1章 基本的划分</h2><p>我认为决定凡人命运的根本差别在于三项内容，它们是：<br> （1）人的自身，即在最广泛意义上属于人的个性的东西。因此，“人的自身”包括健康、力量、外貌、气质、道德品格、精神智力及其潜在发展。<br> （2）人所拥有的身外之物，亦即财产和所有意义上的占有物。<br> （3）人向其他人所展示的样子，众所周知的就是人在其他人眼中所呈现的样子，亦即人们对它的看法。他人的看法又可分为名誉、地位和名声。</p>
<hr>
<p>每一个人到底生活在何样的世界，首先取决于这个人对这个世界的理解，这因个人头脑的差异而相应不同：是贫瘠的、浅薄的和肤浅的，抑或是丰富多彩、趣味盎然和充满意义的。例如，不少人羡慕他人在生活中发现和经历饶有趣味的事情，其实他们应该羡慕后者理解事物的禀赋才对，因为正是因为由于理解事物的禀赋，他们经历过的事情，在其描绘中才是那样耐人寻味。这是因为在一个思想丰富的人看来是意味深长的事情，由一个头脑肤浅、平庸的人理解的话，那不过是平凡世界里乏味的一幕而已。</p>
<hr>
<p>每个人都囿于自己的意识，正如每个人都囿于自己的皮囊，并且只是直接活在自己的意识之中。</p>
<hr>
<p>苏格拉底在看到摆卖的奢侈品时，说道：我不需要的东西，可真不少啊！</p>
<h2 id="第2章-人的自身"><a href="#第2章-人的自身" class="headerlink" title="第2章 人的自身"></a>第2章 人的自身</h2><h3 id="关于愉快心情"><a href="#关于愉快心情" class="headerlink" title="关于愉快心情"></a>关于愉快心情</h3><p>当愉快心情到来之时，我们应该敞开大门欢迎，因为它的到来永远不会不合时宜。</p>
<hr>
<p>高兴的心情直接使我们获益。它才是幸福的现金，而其他别的都只是兑现幸福的支票，因为高兴的心情在当下直接给人以愉快。所以，对于我们的生存，它是一种无与伦比的恩物，因为我们生存的真实性就体现在无法割裂的此时此刻，连接着两段无尽的时间。据此，我们应把获得和促进愉快的心情放在各种追求的首位。</p>
<hr>
<p>只需泛泛浏览一下生活，就可知道：人类幸福的两个死敌就是痛苦和无聊。还有我们成功远离了上述其中一个死敌的时候，也就在同等程度上接近了另一个死敌，反之亦然。这样，我们的生活确实就是在这两者之间或强或弱地摇摆。这是因为痛苦和无聊是处于双重的对立关系。一重是外在的，或说客体的；另一重是内在的，或说主体的。也就是说，外在的一重对立关系就是生活的艰辛和匮乏造成了痛苦，而丰裕和安定就产生了无聊。</p>
<h3 id="关于无聊和空虚"><a href="#关于无聊和空虚" class="headerlink" title="关于无聊和空虚"></a>关于无聊和空虚</h3><p><code>以前就听陈铭老师说过这句话，没想到出处是这里。</code></p>
<p><img src="/2020/09/03/%E4%BA%BA%E7%94%9F%E7%9A%84%E6%99%BA%E6%85%A7/1.jpg" alt="1" style="zoom:30%;"></p>
<p>内在空虚就是无聊的真正根源，这种人无时无刻不在向外面寻求刺激，试图借助某事某物使他们的精神和情绪活动起来。…能让我们免于这种痛苦的可靠手段，莫过于拥有丰富的内在，即丰富的精神思想。因为人的精神思想财富越优越和显著，留给无聊的空间就越小。这些人头脑里面的思想活泼，奔涌不息，不断更新；这些人玩味和摸索着内在世界和外部世界的多种现象；还有把这些思想进行各种组合的冲动和能力——所有这些，除了精神松弛下来的个别时候，都使卓越的头脑远离了无聊。但在另一方面，突出的智力以敏锐的感觉为直接前提，以激烈的意欲，亦即强烈的冲动和激情为根基。这些素质结合在一起极大地提高了情感的强烈程度，提高了对精神痛苦，甚至肉体痛苦的敏感性。对任何不如意的事情，甚至细微的骚动，都会感到更加不耐烦。</p>
<hr>
<p>一个人的自身拥有越丰富，他对身外之物的需求也就越少，别人对他来说就越不重要。所以，卓越的精神思想会导致一个人不喜与他人交往。…在独处的时候，每个人都返回到自身，这个人的自身拥有就会暴露无遗。</p>
<hr>
<p>愚蠢的人饱受无聊之苦。——塞内加</p>
<hr>
<p>我们大致上可以发现：一个人对与人交往的热衷程度，与他贫乏的思想和总体的平庸成正比。人们在这个世界上要么选择独处，要么选择庸俗，除此之外，再没有更多别的选择了。</p>
<h3 id="关于闲暇"><a href="#关于闲暇" class="headerlink" title="关于闲暇"></a>关于闲暇</h3><p>人们辛苦挣来的闲暇，就是人的一生的果实和收获，因为这闲暇让人能够自由地享受自己的意识和个性所带来的乐趣。除此闲暇以外，人的整个一生就只是辛苦和劳作而已。但闲暇给大多数人带来了什么呢？如果不是声色享受和胡闹，就是无聊和浑噩。人们消磨闲暇的方式显示出闲暇对于他们是何等的没有价值。他们的闲暇也就是阿里奥斯托所说的“一无所知者的无聊“。常人考虑的只是如何去打发时间，而略具才华的人却考虑如何利用时间。头脑思想狭隘的人容易受到无聊的侵袭，其原因就是他们的智力纯粹服务于他们的意欲，是发现动因的手段。</p>
<hr>
<p>归根到底，每个人都孑然独立，最关键的就是他到底是个什么样的人。</p>
<hr>
<p>幸福属于那些自得其乐的人。——亚里士多德</p>
<p>这是因为幸福和快乐的外在于安全，就其本质而言，都是机器不保险、不确定、为时短暂和受制于偶然的。因此，甚至在形势大好的情况下，这些外在源泉仍会轻易终结。的确，只要这些外在源泉不在我们的控制之下，这种情形就是不可避免的。</p>
<hr>
<p>我们这个世界乏善可陈，到处充斥着匮乏和痛苦，对于那些侥幸逃过匮乏和痛苦的人们来说，无聊却正在每个角落等待着它们。</p>
<hr>
<p>生活在这样一个世界里，一个拥有丰富内在的人，就像在冬月的晚上，在漫天冰雪当中拥有一间明亮、温暖、愉快的圣诞小屋。因此，能够拥有了优越、丰富的个性，尤其是深邃的精神思想，无疑是在这地球上得到的最大幸运。</p>
<h3 id="关于精神生活"><a href="#关于精神生活" class="headerlink" title="关于精神生活"></a>关于精神生活</h3><p>一般来说，每个无事可做的人都会根据自己的强项能力而挑选一种能够运用此能力的消遣。</p>
<hr>
<p>每个人都会根据自己身上所突出的或这或那的能力而选择相应的异类快乐。第一类是机体新陈代谢能力所带来的快乐。第二类是发挥肌肉力量所带来的快乐。第三类为施展感觉能力方面的快乐。…</p>
<p>感觉能力比人的另外两种生理能力更为优越，因为人在感觉方面的明显优势就是人优胜于动物之处，而人的另外两种生理基本能力在动物身上也同样存在，甚至超过人类。感觉能力隶属于人的认知能力，因此，卓越的感觉力使我们有能力享受到属于认知的，亦即所谓精神思想上的快乐。</p>
<hr>
<p>一个具有思想天赋的人除了个人生活之外，还过着另一种精神的生活，精神的生活逐渐成为了他的唯一目标，而个人生活只是实现自己目标的一种手段而已。但对于芸芸众生来说，只有这一浅薄、空虚和充满烦恼的存在才必须是生活的目标。精神卓越的人首要关注的是精神生活。随着他们对事物的洞察和认识持续地加深和增长，他们的精神生活获得了某种连贯性和持续提升，越来越完整和完美，就像一件逐步变得完美的艺术品。与这种精神生活相比，那种纯粹以追求个人自身安逸为目标的实际生活则显得可悲——这种生活增加的只是长度而不是深度。正如我已经说过的，这种现实生活对于大众就是目的，但对于精神卓越者而言，那只是手段而已。</p>
<hr>
<p>孤身独处正是他们求之不得的，闲暇则是至高的礼物，其他的别的一切好处都是可有可无的。</p>
<hr>
<p>虽然如此，我们却要考虑到一个具有优异思想禀赋的人由于头脑超常的神经活动，对形形色色的痛苦的感受力大大加强了。另外，他拥有这些思想禀赋的前提条件，亦即那激烈的气质，以及与此密不可分的头脑中那些更加生动、更加完美的表象，都会让透过这些表象而刺激起来的激动情绪更增加了烈度。总的来说，这些激动情绪是痛苦多于愉快。最后就是巨大的精神思想禀赋使拥有这些禀赋的人疏远了其他人及其追求。因为自身的拥有越丰富，他在别人身上所能发现得到的就越少。其他人引以为乐的、花样繁多的事情，在他眼里既乏味又浅薄。</p>
<h3 id="菲利斯特人"><a href="#菲利斯特人" class="headerlink" title="菲利斯特人"></a>菲利斯特人</h3><p>在这里，我们不会不提及这样一类人：由于仅仅具备了常规的、有限的智力配给，所以，他们并没有精神思想上的需求，他们也就是德语里所说的“菲利斯特人”。…菲利斯特人就是一个没有精神需求的人。</p>
<hr>
<p>对这种人来说，真正的快乐只能是感官上的快乐，他们就通过这些补偿自己。</p>
<hr>
<p>在与他人的交往中，他们会寻求那些能满足自己生理上的需要，而不是精神上的需求的人。因此，在他们对别人的诸多要求中，最不重要的就是别人必须具备一定的头脑思想。</p>
<hr>
<p>菲利斯特人的巨大痛苦就是在于任何观念性的东西都无法带给他们愉快。为了逃避无聊，他们不断需要现实性的东西。但由于现实性的东西一来很快就会被穷尽，一旦这样，它们不但不再提供快乐，反而会使人厌烦；二来还会带来各种祸殃。相比较而言，观念性的东西却是不可穷尽的，它们本身既无邪也乌海。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>叔本华</tag>
      </tags>
  </entry>
  <entry>
    <title>《计算模型导引》复习笔记</title>
    <url>/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/</url>
    <content><![CDATA[<h2 id="第一章-递归函数"><a href="#第一章-递归函数" class="headerlink" title="第一章 递归函数"></a>第一章 递归函数</h2><h3 id="1-1-数论函数（P1）"><a href="#1-1-数论函数（P1）" class="headerlink" title="1.1 数论函数（P1）"></a>1.1 数论函数（P1）<a id="more"></a></h3><ul>
<li><p>数论函数</p>
<ul>
<li><p>k元数论全函数</p>
</li>
<li><p>部分数论函数</p>
</li>
</ul>
</li>
<li><font color="red">本原函数（$\mathcal{IF}$）</font>

<ul>
<li><p>零函数$Z$</p>
</li>
<li><p>后继函数$S$</p>
</li>
<li><p>投影函数$P_i^n$</p>
</li>
</ul>
</li>
<li><p>常用数论函数（P2）</p>
<ul>
<li>前驱函数pred</li>
<li>加法函数add</li>
<li><strong>算术差函数sub</strong></li>
<li>绝对差函数diff</li>
<li>乘法函数mul</li>
<li>除法函数div</li>
<li>求余函数rs</li>
<li>指数函数pow</li>
<li>平方函数sq</li>
<li>$E(x)=x-\lfloor \sqrt{x}\rfloor$</li>
<li>max、min</li>
<li>最大公约数函数gcd、最小公倍数函数lcm</li>
<li><strong>素数枚举函数$P$</strong></li>
<li><strong>$ep(n,x)$：x的素因子分解式中第n个素数的指数</strong></li>
<li>$eq(x,y)$：相等时等于0，否则为1</li>
<li><strong>$N$：否定，x=0时为1，否则为0</strong></li>
<li><strong>$N^2$：x=0时为0，否则为1</strong></li>
</ul>
</li>
<li><p>函数的复合（P4）</p>
</li>
<li>有界迭加算子$\sum$</li>
<li>有界迭乘算子$\prod$</li>
<li>有界$\mu-$算子</li>
<li>有界$max-$算子</li>
<li><font color="red">基本函数类$\mathcal{BF}$</font>（P6）<ul>
<li>$\mathcal{IF} \subseteq \mathcal{BF}$</li>
<li>$\mathcal{BF}$对于复合封闭</li>
</ul>
</li>
</ul>
<h3 id="1-2-配对函数（P7）"><a href="#1-2-配对函数（P7）" class="headerlink" title="1.2 配对函数（P7）"></a>1.2 配对函数（P7）</h3><ul>
<li>配对函数、配对函数组</li>
<li>Godel编码</li>
<li>若配对函数组$\{pg,K,L\}$使pg穷尽一切自然数，则称该配对函数组是一一对应的<ul>
<li>康托编码（P9）</li>
</ul>
</li>
<li>多元配对函数（P11）</li>
<li>Godel $\beta-$函数（P13）<ul>
<li>定理1.11：有穷数列的编码和解码（中国剩余定理）</li>
</ul>
</li>
</ul>
<h3 id="1-3-初等函数（P14）"><a href="#1-3-初等函数（P14）" class="headerlink" title="1.3 初等函数（P14）"></a>1.3 初等函数（P14）</h3><ul>
<li><font color="red">初等函数类$\mathcal{EF}$</font><ul>
<li>定义<ul>
<li>$\mathcal{IF} \subseteq \mathcal{EF}$</li>
<li>$x+y,x−y(绝对差),x×y,⌊x/y⌋∈\mathcal{EF}$<ul>
<li>加、乘、除可省</li>
</ul>
</li>
<li>$\mathcal{EF}$对于复合，有界迭加算子$∑[⋅]$和有界迭乘算子$∏[⋅]$封闭</li>
</ul>
</li>
<li>性质<ul>
<li>$\mathcal{EF}$对于有界$\mu-$算子和max-算子封闭</li>
</ul>
</li>
</ul>
</li>
<li>数论谓词<ul>
<li>数论谓词的特征函数：真为0，假为1</li>
<li>初等数论谓词：若谓词$P$的特征函数属于$\mathcal{EF}$，则称$P$是初等的</li>
</ul>
</li>
<li>数论集合<ul>
<li>数论集合的特征函数：属于集合为0，不属于为1</li>
<li>初等数论集合：若数论集合$S$的特征函数属于$\mathcal{EF}$，则称$S$是初等的</li>
</ul>
</li>
<li>重要的初等函数（P19）<ul>
<li>$x^y$</li>
<li>$\lfloor \sqrt[y] x \rfloor$</li>
<li>余数$rs(x,y)$</li>
<li>$\tau (x)$：x因子的数目</li>
<li>$prime(x)$：判定x是否为素数（数论谓词）</li>
<li>$\pi (x)$：不超过x的素数个数</li>
<li>素数枚举函数$P(n)$=第n个素数</li>
<li>$ep(n,x)$</li>
</ul>
</li>
<li>控制函数（P24）<ul>
<li>$\mathcal{EF}$的控制函数G（P22）<ul>
<li>控制函数不属于$\mathcal{EF}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="1-4-原始递归函数（P25）"><a href="#1-4-原始递归函数（P25）" class="headerlink" title="1.4 原始递归函数（P25）"></a>1.4 原始递归函数（P25）</h3><ul>
<li>原始递归算子<ul>
<li>带参原始递归算子</li>
<li>无参原始递归算子</li>
</ul>
</li>
<li><font color="red">原始递归函数类$\mathcal{PRF}$</font><ul>
<li>定义<ul>
<li>$\mathcal{IF} \subseteq \mathcal{PRF}$</li>
<li>$\mathcal{PRF}$对于复合、带参原始递归算子、无参原始递归算子封闭</li>
</ul>
</li>
<li>重要的原始递归函数<ul>
<li>add、pred、sub、diff、mul、sq（平方）、N、$N^2$、sqrt、E</li>
</ul>
</li>
</ul>
</li>
<li>原始复迭算子It​、弱原始复迭算子​Itw​ (P30)</li>
<li><font color="red">原始复迭函数类$\mathcal{ITF}$</font><ul>
<li>定义<ul>
<li>$\mathcal{IF} \subseteq \mathcal{ITF}$</li>
<li>$\mathcal{ITF}$对于复合、原始复迭算子It[·]、弱原始复迭算子Itw[·]封闭</li>
</ul>
</li>
<li>$\mathcal{ITF=PRF}$</li>
</ul>
</li>
<li>若干形式不同的递归式化归到原始递归式（P34）<ul>
<li>串值递归</li>
<li>联立递归</li>
<li>变参递归</li>
<li>多重递归</li>
</ul>
</li>
<li>$\mathcal{PRF}$的控制函数<ul>
<li>Ackermann函数：$Ack(m,n)$（P38）<ul>
<li>不是原始递归函数</li>
</ul>
</li>
</ul>
</li>
<li>$\mathcal{EF} \subset \mathcal{PRF}$ (P41)<ul>
<li>真包含：$\mathcal{EF}$的控制函数G属于$\mathcal{PRF}$</li>
</ul>
</li>
</ul>
<h3 id="1-5-递归函数（P42）"><a href="#1-5-递归函数（P42）" class="headerlink" title="1.5 递归函数（P42）"></a>1.5 递归函数（P42）</h3><ul>
<li>正则函数（全函数）、正则$\mu-$算子（区别于有界$\mu-$算子）</li>
<li><font color="red">一般递归函数类$\mathcal{GRF}$</font><ul>
<li>定义<ul>
<li>$\mathcal{IF} \subseteq \mathcal{GRF}$</li>
<li>$\mathcal{GRF}$对于复合和原始递归算子（带参和无参）封闭</li>
<li><u>$\mathcal{GRF}$对于正则$\mu-$算子封闭</u></li>
</ul>
</li>
</ul>
</li>
<li>$\mu-$算子（部分函数）</li>
<li><font color="red">部分递归函数类$\mathcal{RF}$</font> （P43）<ul>
<li>定义<ul>
<li>$\mathcal{IF} \subseteq \mathcal{RF}$</li>
<li>$\mathcal{RF}$对于复合和原始递归算子（带参和无参）封闭</li>
<li><u>$\mathcal{RF}$对于$\mu-$算子封闭</u></li>
</ul>
</li>
<li>显然$\mathcal{GRF} \subset \mathcal{RF}$ ：正则$\mu-$算子是$\mu-$算子的特例</li>
</ul>
</li>
<li>递归数论谓词、$\mu-$谓词、递归集</li>
<li>$\mathcal{PRF} \subset \mathcal{GRF}$ <ul>
<li>法1：利用控制函数，证明Ackermann函数是一般递归函数但不是原始递归函数（P45）</li>
<li>法2：利用通用函数（P47）</li>
</ul>
</li>
<li>$\mathcal{GRF} \subset 全数论函数\mathcal{NTF}$ （P48）<ul>
<li>存在数论全函数f，f不是一般递归函数 </li>
</ul>
</li>
</ul>
<h3 id="1-6-结论（P48）"><a href="#1-6-结论（P48）" class="headerlink" title="1.6 结论（P48）"></a>1.6 结论（P48）</h3><script type="math/tex; mode=display">本原函数类\mathcal{IF} \subset 基本函数类\mathcal{BF} \subset 初等函数类\mathcal{EF} \subset 原始递归函数类\mathcal{PRF} = 原始复迭函数类\mathcal{ITF} \subset 一般递归函数类\mathcal{GRF} \subset 部分递归函数类\mathcal{RF}</script><h2 id="第三章-lambda-演算"><a href="#第三章-lambda-演算" class="headerlink" title="第三章 $\lambda-$演算"></a>第三章 $\lambda-$演算</h2><h3 id="3-1-lambda-演算的语法（P72）"><a href="#3-1-lambda-演算的语法（P72）" class="headerlink" title="3.1 $\lambda-$演算的语法（P72）"></a>3.1 $\lambda-$演算的语法（P72）</h3><ul>
<li>变量：小写字母，表示一个参数（形参）或者一个值（实参）</li>
<li>抽象：$\lambda x.M$，绑定变量x于该函数抽象的函数体M，简单来说就是表示一个<u>形参</u>为x的函数M。</li>
<li>作用：$MN$，表示将函数M应用于参数N，简单来说就是给函数M输入<u>实参</u>N。<ul>
<li>函数作用是左结合的，即：<code>M N P</code>意为<code>(M N) P</code>而非<code>M (N P)</code></li>
</ul>
</li>
<li>自由变元和约束变元（P74）<ul>
<li>在函数抽象中，形参绑定于函数体，即形参是约束变元，相对应地，不是约束变元的自然就是自由变元</li>
<li>闭$\lambda-$项：没有自由变元的项，$\Lambda ^ \circ$表示全体闭$\lambda-$项的集合</li>
<li>约束变元改名后仍等价</li>
<li>自由变元的替换：$M[x:=N]$，$x$为自由变元</li>
</ul>
</li>
</ul>
<h3 id="3-2-转换（P76）"><a href="#3-2-转换（P76）" class="headerlink" title="3.2 转换（P76）"></a>3.2 转换（P76）</h3><ul>
<li>形式理论$\lambda \beta$ (P76)<ul>
<li>标准组合子$I,K,K^*,S$</li>
</ul>
</li>
<li>形式理论$\lambda \beta + ext$ 和$\lambda \beta \eta$ (P79)</li>
</ul>
<h3 id="3-3-归约（P80）"><a href="#3-3-归约（P80）" class="headerlink" title="3.3 归约（P80）"></a>3.3 归约（P80）</h3><ul>
<li>$\to_R$（一步$R-$归约）、$\twoheadrightarrow_R$ （$R-$归约）、$=_R$（$R-$转换）</li>
<li>二元关系$\beta$、$\alpha$、$\eta$、$\beta \eta$<ul>
<li>$M =_\beta N \Leftrightarrow \lambda \beta \vdash M=N$</li>
</ul>
</li>
<li>$\beta-$可约式、$\beta-$范式（$\beta-$nf）、$\beta-$范式的集合（$NF_R$）、$M$有$\beta-$nf</li>
</ul>
<h3 id="3-4-Church-Rosser定理（P85）"><a href="#3-4-Church-Rosser定理（P85）" class="headerlink" title="3.4 Church-Rosser定理（P85）"></a>3.4 Church-Rosser定理（P85）</h3><ul>
<li>CR性质：$P \twoheadrightarrow M \bigwedge P \twoheadrightarrow N \Rightarrow \exists T.(M \twoheadrightarrow T \bigwedge N \twoheadrightarrow T)$<ul>
<li>对$\twoheadrightarrow_\beta$、$\twoheadrightarrow_\eta$、$\twoheadrightarrow_{\beta \eta}$成立</li>
</ul>
</li>
<li>con($\lambda \beta$)：存在推不出的公式</li>
</ul>
<h3 id="3-5-不动点定理（P93）"><a href="#3-5-不动点定理（P93）" class="headerlink" title="3.5 不动点定理（P93）"></a>3.5 不动点定理（P93）</h3><ul>
<li>不动点定理：对于任何的$F \in \Lambda$，存在$Z \in \Lambda$，使得$FZ =_\beta Z$</li>
<li>不动点组合子<ul>
<li>$Y$： 对于任何的$F \in \Lambda$，$F(YF)=_\beta YF$</li>
<li>$\Theta$：对于任何的$F \in \Lambda ^ \circ$，$\Theta F \twoheadrightarrow_\beta F(\Theta F)$</li>
</ul>
</li>
<li>$([M_1,…,M_n])^n_i \twoheadrightarrow_\beta M_i$</li>
</ul>
<h3 id="3-6-递归函数的-lambda-可定义性（P95）"><a href="#3-6-递归函数的-lambda-可定义性（P95）" class="headerlink" title="3.6 递归函数的$\lambda-$可定义性（P95）"></a>3.6 递归函数的$\lambda-$可定义性（P95）</h3><ul>
<li>Church数项：$\ulcorner n \urcorner \equiv \lambda fx.f^nx$</li>
<li>$\lambda-$可定义性：$F \ulcorner n_1 \urcorner …  \ulcorner n_k \urcorner =_\beta  \ulcorner f(n_1,…,n_k) \urcorner$</li>
<li>$D\ulcorner n \urcorner MN$ = if (n = 0) then M else N​ (P97)</li>
<li><strong>一般/部分递归函数是$\lambda-$可定义的</strong></li>
</ul>
<h3 id="3-7-与递归论对应的结果（P100）"><a href="#3-7-与递归论对应的结果（P100）" class="headerlink" title="3.7 与递归论对应的结果（P100）"></a>3.7 与递归论对应的结果（P100）</h3><ul>
<li>$\lambda-$项的编码：对每个$M \in \Lambda$ ，都有唯一的自然数$\sharp M$与之对应 （$\lambda-$项 $\rightarrow$ $\mathbb{N}$）</li>
<li>$\lambda-$项的内部编码：$M$的内部编码定义为$\ulcorner M \urcorner \equiv \ulcorner \sharp M \urcorner$ （$\lambda-$项 $\rightarrow$ $\lambda-$项)</li>
<li>枚举子$E$，对于任何$M \in \Lambda^ \circ$，有$E\ulcorner M \urcorner =_\beta M$</li>
<li>第二不动点定理：$\forall F. \exists Z. F \ulcorner Z \urcorner =_ \beta Z$</li>
<li><strong>不可判定性</strong>（P102）<ul>
<li>若自然数集S的特征函数$\mathcal{X}_s \in \mathcal{GRF}$，则称$S$是可判定的</li>
<li>若$\lambda-$项集合$\mathcal{A} \subseteq  \Lambda$的编码集合是可判定的，则称$\mathcal{A}$是可判定的</li>
<li>设$\mathcal{A} \subseteq  \Lambda$非平凡、$\mathcal{A}$对$=_\beta$封闭，则$\mathcal{A}$不可判定</li>
<li>$=_\beta$关系不可判定</li>
<li>集合$\mathcal{N}=\{M:M有\beta-nf\}$不可判定</li>
</ul>
</li>
</ul>
<h2 id="第五章-Turing机"><a href="#第五章-Turing机" class="headerlink" title="第五章 Turing机"></a>第五章 Turing机</h2><h3 id="5-1-Turing机的形式描述（P121）"><a href="#5-1-Turing机的形式描述（P121）" class="headerlink" title="5.1 Turing机的形式描述（P121）"></a>5.1 Turing机的形式描述（P121）</h3><ul>
<li>Turing机定义：$M=(d,p,s)$，论域$Dom(M)$</li>
<li>Turing可计算的定义：存在机器M计算函数f</li>
<li>基本机器：<ul>
<li>零函数：$\boxed{Z}$</li>
<li>后继函数：$\boxed{S}$</li>
<li>投影函数：$\boxed{I}$、$\boxed{K}$、$\boxed{L}$</li>
<li>常数函数：$\boxed{C^k_l}$</li>
<li>前驱函数：$\boxed{pred}$</li>
<li>加法函数：$\boxed{add}$</li>
<li>乘法函数：$\boxed{multi}$ (习题5.3)</li>
<li>幂函数$2^x$：习题5.4</li>
<li>平方根函数：习题5.7</li>
</ul>
</li>
</ul>
<h3 id="5-2-Turing机的计算能力（P127）"><a href="#5-2-Turing机的计算能力（P127）" class="headerlink" title="5.2 Turing机的计算能力（P127）"></a>5.2 Turing机的计算能力（P127）</h3><ul>
<li>常用机器（P128）<ul>
<li>$f(x)=2x$：$\boxed{double}$</li>
<li>$\boxed{copy_1}$（习题5.2）、$\boxed{copy_2}$、$\boxed{copy_k}$、$\boxed{copy_k}^k$</li>
<li>$\boxed{compress}$</li>
<li>$\boxed{erase}$</li>
<li>$\boxed{shiftr}$、$\boxed{shiftl}$</li>
</ul>
</li>
<li>Turing可计算函数类对于复合算子、原始递归算子、正则$\mu-$算子封闭</li>
<li>若f是一般/部分递归函数，则f是Turing-可计算的</li>
</ul>
<h3 id="5-3-可判定性与停机问题（P138）"><a href="#5-3-可判定性与停机问题（P138）" class="headerlink" title="5.3 可判定性与停机问题（P138）"></a>5.3 可判定性与停机问题（P138）</h3><ul>
<li>可判定性：设$A \subseteq \mathbb{N}$，$A$是可判定的指$A$的特征函数$\mathcal{X}_A$是Turing-可计算的（可构造出机器）</li>
<li>Turing机的编码<ul>
<li>从#M可反向求出M</li>
</ul>
</li>
<li>自停机问题$K=\{\sharp M:M对于输入\overline{\sharp M}停机\}$：不可判定</li>
<li>停机问题$\hat{K}=\{\sharp M:M对于一切输入皆停机\}$：不可判定</li>
</ul>
<h3 id="5-4-通用Turing机（P141）"><a href="#5-4-通用Turing机（P141）" class="headerlink" title="5.4 通用Turing机（P141）"></a>5.4 通用Turing机（P141）</h3><ul>
<li>带位置编码</li>
<li>标准输入编码和解码：$\boxed{code}$、$\boxed{decode}$</li>
<li>计算后继带位置函数STP、TS</li>
<li>通用图灵机$U$的定义（P146）</li>
</ul>
<h3 id="5-5-Church-Turing论题（P147）"><a href="#5-5-Church-Turing论题（P147）" class="headerlink" title="5.5 Church-Turing论题（P147）"></a>5.5 Church-Turing论题（P147）</h3><h2 id="题型"><a href="#题型" class="headerlink" title="题型"></a>题型</h2><ul>
<li><p>问答题</p>
<ul>
<li>什么是配对函数组？什么是配对函数？请构造一例。（P7 定义1.10，如引理1.14构造）</li>
<li>什么是一般递归函数？（P42 定义1.31）</li>
<li>什么是部分递归函数？（P43 定义1.33）</li>
<li><font color="red">什么是$\lambda \beta$系统的CR性质？（P85）</font></li>
<li>什么是Turing 机？（P122）</li>
<li>什么是Church-Turing Thesis？你认可它吗？/你拥护吗？（P148）</li>
<li>为什么算法和Turing 机概念在可以构成“思维机器”的现代观点中占有如此核心的地位？（因为图灵机的概念为现在的思维机器观点提供了抽象模型，是现代计算机的起源。）是否在原则上存在一个算法可达到绝对极限呢？（未知，自由发挥）</li>
<li>什么是Halting Problem？它可判定吗？（P141）</li>
<li>什么是Turing 机的通用性(universality)？什么是通⽤Turing 机？(P146)</li>
</ul>
</li>
<li><p>判断函数类（第二大题）</p>
<ul>
<li>A：$\mathcal{EF}$<ul>
<li>Godel的$\beta-$函数</li>
<li>向下取整</li>
<li>$\pi,e$的十进制展开中的第n个数字</li>
<li>$\lambda-$项呈形…</li>
<li>组合数个数</li>
<li>数列求和</li>
</ul>
</li>
<li>B：$\mathcal{PRF} - \mathcal{EF}$<ul>
<li>形如$G(x)=2^{2^{\cdots^x}}$</li>
<li>Ack(5,n)</li>
<li>变参递归(不确定？)</li>
</ul>
</li>
<li>C：$\mathcal{GRF} - \mathcal{PRF}$<ul>
<li>Ackermann函数</li>
<li>Ack(m,5)</li>
<li>$\beta_0,\beta_{x+1}$ （不确定？）</li>
</ul>
</li>
<li>D：$\mathcal{RF} - \mathcal{GRF}$<ul>
<li>处处无定义的函数</li>
<li>存在无定义的函数</li>
</ul>
</li>
<li><font color="red">E：不可计算的数论函数类 </font><ul>
<li>停机问题</li>
<li>$M有\beta-nf$、$M =_\beta N$</li>
</ul>
</li>
</ul>
</li>
<li><p>证明集合$S$的可判定性</p>
<ul>
<li>证$\mathcal{X}_s \in \mathcal{GRF}$</li>
</ul>
</li>
<li><p>证明初等函数</p>
<ul>
<li><p>根据定义用常用函数表示（Q1.2，Q1.3，1.11，1.12）</p>
</li>
<li><p>出现根号、负数等：用已知形式表示（转化为整数、分开表示等）</p>
</li>
<li><p>取整</p>
<ul>
<li>$f(n)=\lfloor e \cdot n \rfloor$ (Q1.8)</li>
<li>$f(n)=\lfloor (\frac{\sqrt{5}+1}{2})^n\rfloor$ (2019 四，1.19(2) 两种方法）</li>
<li>$f(n)=\lfloor (\sqrt{6} +\sqrt{5})^{2n} \rfloor$ (2019+ 四)</li>
<li>$\lfloor n! \cdot cos(1)\rfloor$</li>
<li>$\lfloor n! \cdot 2^n \cdot \sqrt{e} \rfloor$</li>
<li>$\lfloor log_{10}n \rfloor$</li>
<li>$\lfloor (n+1+\frac{1}{n+1})^{n+1} \rfloor$</li>
</ul>
</li>
<li><font color="red">十进制展开式第n位</font>

<blockquote>
<p>step1. 泰勒展开，分开整数项和小数项，证$\lfloor n!·α\rfloor \in \mathcal{EF}$</p>
<p>step2. 证$\lfloor n \cdot \alpha \rfloor \in \mathcal{EF}$</p>
<p>step3. $f(n) = \lfloor 10^n \alpha \rfloor - \lfloor 10^{n-1}\alpha \rfloor \cdot 10  \in \mathcal{EF},(n \geq 1)$</p>
</blockquote>
<ul>
<li>$e$ （2019+ 七，5.18，类似Q1.8）</li>
<li>$\pi$（1.25，一般递归函数）</li>
<li>$\frac{e^2+1}{2e}$ (2019 七)</li>
<li>$sinh(1)=(e-e^{-1})/2$ (2018 七)，$sin(1),cos(1)$</li>
<li>$\sqrt{e}$</li>
<li>证明原始递归函数</li>
</ul>
</li>
<li><p>串值递归（Q1.5，Q1.6，1.15）</p>
</li>
<li>变参递归（Q1.7）</li>
</ul>
</li>
<li><p>对给定的数论函数$f(x)$，构造$F∈Λ^∘$其$λ−$定义$f(x)$</p>
<ul>
<li>判断奇偶函数：$l(x)=N^x(0)$ （2017 三）</li>
<li>add（2016 三）、$f(x,y)=x+y$ （3.16）</li>
<li>$f(x)=2x$ ，$f(x)=3x$（Q2.3，3.17）</li>
<li>$g(x)=2^x$ （Q2.7，3.20）<ul>
<li>倒推法、Rosser引理$\ulcorner n^m \urcorner = \ulcorner m \urcorner \ulcorner n \urcorner$</li>
</ul>
</li>
<li>$f(x)=\lfloor \frac{x}{2} \rfloor$</li>
</ul>
</li>
<li><p>在已有的公理系统中加入一个额外公理</p>
<ul>
<li>$\lambda xy.xy = \lambda xy.yx$ (2017)</li>
<li>$\lambda x.x = \lambda x.xxx$ (2018)</li>
<li>$\lambda x.x = \lambda x.xx$ （2016）</li>
<li>$\lambda xy.x = \lambda xy.y$ （3.13）</li>
<li>$\lambda xyz.x(yz)=\lambda xyz.(xy)z$</li>
<li>$I=K$ （2019）</li>
<li>$I=S$（2019+）</li>
</ul>
</li>
<li><p>构造图灵机</p>
<ul>
<li>$f(x)=x^3$ （2019 六）、$f(x)=x^4$（2019+ 六）</li>
<li>$g(x)=2^x$ （2016 六、习题5.4、Q3.5）</li>
<li>$f(x)=\lfloor \frac{x}{2} \rfloor$</li>
<li>满足给定的输入输出（2018、2017 五）</li>
</ul>
</li>
<li><p>求图灵机运行后的输出</p>
<ul>
<li>记得写状态、箭头</li>
</ul>
</li>
<li><p>证明停机问题不可判定（2018、 2017 六）</p>
</li>
</ul>
<h2 id="Hint"><a href="#Hint" class="headerlink" title="Hint"></a>Hint</h2><ul>
<li>减号写点、除号写取整</li>
<li>$N$, $N^2$ 表示if-else</li>
<li><p>Godel编码</p>
<ul>
<li>P,ep</li>
</ul>
</li>
<li>标准组合子$I,K,K^*,S,U^3_1,…$</li>
<li>$y^nz=(\lambda fx.f^nx)yz = \ulcorner n \urcorner yz$</li>
</ul>
<p><img src="/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/1.png" alt="1" style="zoom:60%;"></p>
<p><img src="/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/2.png" alt="2" style="zoom:70%;"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>复习笔记</category>
      </categories>
      <tags>
        <tag>计算模型</tag>
      </tags>
  </entry>
  <entry>
    <title>【复习笔记】自然语言处理</title>
    <url>/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h2 id="1-基于规则的传统自然语言处理方法"><a href="#1-基于规则的传统自然语言处理方法" class="headerlink" title="1 基于规则的传统自然语言处理方法"></a>1 基于规则的传统自然语言处理方法</h2><h3 id="什么是自然语言处理"><a href="#什么是自然语言处理" class="headerlink" title="什么是自然语言处理"></a>什么是自然语言处理<a id="more"></a></h3><ul>
<li>语⾔<ul>
<li>自然语⾔（⼈类语⾔）</li>
<li>程序语⾔（⼈⼯语⾔）</li>
<li>是⼀种由三部分组成的符号交流系统：记号，意义和连接两者的符码。</li>
<li>语⾔视作由组合语法规则制约、旨在传达语义的记号形式系统。</li>
<li>⼀种口头或符号上的⼈类交流系统。</li>
</ul>
</li>
<li><strong>自然语言处理（Natural Language Processing, NLP）</strong><ul>
<li><strong>利用计算机为⼯具对自然语言进⾏各种加⼯处理、信息提取及应⽤的技术。</strong></li>
<li><strong>自然语言理解：强调对语言<u>含义和意图的深层次解释</u></strong></li>
<li>计算语⾔学：强调可计算的语⾔理论</li>
</ul>
</li>
</ul>
<h3 id="自然语言处理的实现方法"><a href="#自然语言处理的实现方法" class="headerlink" title="自然语言处理的实现方法"></a>自然语言处理的实现方法</h3><ul>
<li><strong>基于知识工程的理性方法（ Rationalist approach）</strong><ul>
<li>以<strong>规则</strong>形式表达语言知识。</li>
<li>基于规则进行符号推理 ，从而实现语言信息处理。</li>
<li>强调人对语言知识的理性整理（受 Chomsky 主张的<u>人具有先天语言能力观点</u>的影响 ）。</li>
</ul>
</li>
<li><strong>基于数据的经验方法（ Empiricist approach）</strong><ul>
<li>以大规模语料库为语言知识基础。</li>
<li>利用统计学习 和基于神经网络的深度学习方法自动获取隐含在语料库中的知识，学习到的知识体现为一系列模型参<br>数。 （训练基于学习到的参数和相应的模型进行语言信息处理。）</li>
</ul>
</li>
<li><p><strong>混合方法</strong></p>
<ul>
<li><font color="red"><strong>理性方法的优、缺点</strong></font><ul>
<li>相应的语言学理论基础好</li>
<li>语言知识描述精确</li>
<li>处理效率高（确定性推理）</li>
<li>知识获取困难（需要专业人员，高级劳动）</li>
<li>系统鲁棒性差：不完备的规则系统将导致推理的失败</li>
<li>知识扩充困难，并且很难保证规则之间的一致性</li>
</ul>
</li>
<li><font color="red"><strong>经验方法的优、缺点</strong></font><ul>
<li>知识获取容易（低级劳动）</li>
<li>系统鲁棒性好（概率大的作为结果）</li>
<li>知识扩充容易、一致性容易维护</li>
<li>相应的语言学理论基础差（可解释性差）</li>
<li>缺乏对语言学知识的深入描述和利用，过于机械</li>
<li>处理效率低（大数据、高维度计算）</li>
</ul>
</li>
</ul>
</li>
<li><p><font color="red"><strong>自然语言处理的难点</strong></font></p>
<ul>
<li><strong>歧义处理</strong><ul>
<li>自然语言充满了大量的歧义（为什么？）</li>
<li><u>有限的词汇和规则表达复杂、多样的对象</u></li>
</ul>
</li>
<li>语言知识的表示、获取和运用</li>
<li>成语和惯用型的处理</li>
<li><strong>对语言的<u>灵活性和动态性</u>的处理</strong><ul>
<li>灵活性：同一个意图的不同表达，甚至包含错误的语法等</li>
<li>动态性：语言在不断的变化，如：新词等</li>
</ul>
</li>
<li>对常识等与语言无关的知识的利用和处理（上下⽂和世界知识（语⾔⽆关）的利⽤和处理）</li>
</ul>
</li>
</ul>
<h3 id="基于规则的自然语言处理方法（理性方法，传统方法）"><a href="#基于规则的自然语言处理方法（理性方法，传统方法）" class="headerlink" title="基于规则的自然语言处理方法（理性方法，传统方法）"></a>基于规则的自然语言处理方法（理性方法，传统方法）</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><ul>
<li>以 规则 形式表达语言知识。</li>
<li>基于规则进行 符号推理 ，从而实现语言信息处理。</li>
<li>强调人对语言知识的理性整理（知识工程）。</li>
<li>受计算语言学理论指导。</li>
<li>语言处理规则作为数据，它与程序分离，程序体现为规则语言的解释器。</li>
</ul>
<h4 id="词法分析"><a href="#词法分析" class="headerlink" title="词法分析"></a>词法分析</h4><ul>
<li>形态还原（针对英语、德语、法语等）<ul>
<li>把句子中的词还原成它们的基本词形（原形）。</li>
</ul>
</li>
<li>词性标注<ul>
<li>为句子中的词标上预定义类别集合中的类。</li>
<li>方法：词典和规则提供候选词性，消歧规则进行消歧</li>
</ul>
</li>
<li>命名实体识别<ul>
<li>识别出句子中的人名、地名、机构名等。</li>
</ul>
</li>
<li>分词（针对汉语、日语等）<ul>
<li>识别出句子中的词。</li>
<li>一般通过分词词典和分词规则库进行分词。</li>
<li><font color="red">中文分词</font><ul>
<li>任务描述：分词是指根据 某个分词规范 ，把一个字串划分成词串。</li>
<li>难点问题<ul>
<li>切分歧义<ul>
<li>分类<ul>
<li>交集型歧义<ul>
<li>ABC 切分成 AB/C 或 A/BC</li>
<li>如： 和平等</li>
</ul>
</li>
<li>组合型歧义<ul>
<li>AB 切分成 AB 或 A/B</li>
<li>如： 马上</li>
</ul>
</li>
<li>混合型歧义<ul>
<li>由交集型歧义和组合型歧义嵌套与交叉而成<ul>
<li>如：“得到达”（交集型、组合型）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>一般通过<u>分词词典</u>和<u>分词规则库</u>进行分词。主要方法有：<ul>
<li>正向最大匹配 (FMM）或逆向最大匹配 (RMM)</li>
<li>双向最大匹配（能发现交集型歧义）</li>
<li>正向最大、逆向最小匹配（发现组合型歧义）</li>
<li>逐词遍历匹配<ul>
<li>在全句中取最长的词，去掉之，对剩下字符串重复该过程</li>
</ul>
</li>
<li>设立切分标记<ul>
<li>收集词首字和词尾字，把句子分成较小单位，再用某些方法切分</li>
</ul>
</li>
<li>全切分<ul>
<li>获得所有可能的切分，选择最大可能的切分</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="句法分析"><a href="#句法分析" class="headerlink" title="句法分析"></a>句法分析</h4><ul>
<li>确定句子的组成<ul>
<li>词、短语以及它们之间的关系</li>
</ul>
</li>
<li>句法分析任务的类型<ul>
<li>组块分析（浅层句法分析、部分句法分析）：<ul>
<li>识别基本短语（非递归的核心成分）</li>
</ul>
</li>
<li><strong>组成分分析（结构分析，完全句法分析）</strong><ul>
<li>识别词如何构成短语、短语如何构成句子</li>
<li>句法分析的目的<ul>
<li>判断句子的合法性（句子识别）</li>
<li>确定句子的结构（句子中单词相互关联的方式）</li>
</ul>
</li>
<li>基于上下文无关语法（ CFG ）的句法分析<ul>
<li>CFG 能描述大部分的自然语言结构</li>
<li>可以构造高效的基于 CFG 的句法分析器</li>
</ul>
</li>
<li>通常采用树形结构来表示句法分析的结果</li>
</ul>
</li>
<li>依存分析<ul>
<li>识别词之间的依赖（或支配）关系</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="语义分析"><a href="#语义分析" class="headerlink" title="语义分析"></a>语义分析</h4><ul>
<li><p>语义分析的目的是给出语言表达的含义或意义 ( 。</p>
</li>
<li><p>语义分析包括</p>
<ul>
<li>词义分析（词义表示及多义词消歧等）</li>
<li>句义分析（句义表示及句义计算等）</li>
<li>篇章语义分析（指代、实体关系等）</li>
</ul>
</li>
<li><p>词汇的语义表示：</p>
<ul>
<li><p>义项（义位）</p>
</li>
<li><p>语义类</p>
</li>
<li><p>义素组合</p>
</li>
</ul>
</li>
</ul>
<h3 id="往年考题"><a href="#往年考题" class="headerlink" title="往年考题"></a>往年考题</h3><ul>
<li>语言、自然语言、自然语言处理</li>
<li><p>你认为自然语言理解和自然语言处理概念本身的区别？</p>
</li>
<li><p>相比于程序设计等人工语言，为什么自然语言的处理和理解更难？结合一些示例说明</p>
</li>
<li><p>中文分词的任务描述、难点问题以及解决问题的技术和方法</p>
</li>
<li><p>规则、统计、神经网络方法的优缺点（结合文本分类）</p>
<ul>
<li><p>统计和机器学习方法的特点是，对特征工程的依赖比较大。这是个双刃剑：当训练数据量比较小的时候，我们可以凭着对业务和数据的理解，为模型添加大量先验知识，让它站在我们的肩膀上；当然，这样做会消耗我们的时间和精力。</p>
</li>
<li><p>神经网络的突出优势是：</p>
<p>(1)结构灵活。可以基于常用结构方便地设计出各式各样的结构，以处理各式各样的任务。</p>
<p>(2)信息容量大。神经网络是众多机器学习模型中，效果上限最高的之一。神经网络可以拟合任意复杂的函数。</p>
<p>(3)文本表示自动化。神经网络方法大部分情况下会采用词token的分布式表示作为输入特征——这种特征工程方法的泛化性比较强，不需要太多背景知识就可以得到比较好的特征。“万物皆可嵌入”这句口号还是有点道理的。</p>
</li>
</ul>
</li>
<li><p>比尔盖茨说语言理解是人工智能皇冠上的明珠。给出对于这句话的理解。</p>
</li>
</ul>
<blockquote>
<p>在以计算、记忆为基础的「运算智能」之上，是以听觉、视觉、触觉为代表的「感知智能」，反映在人工智能技术上为语音识别和图像识别。再之上则是「认知智能」，包含语言、知识和推理。金字塔的顶端，则是「创造智能」，想象一些不存在的事情包括理论、方法、技术，通过实验加以验证，然后提出新的理论。</p>
<p>语言智能是人工智能皇冠上的明珠，如果语言智能能够突破，与他同属认知智能的知识和推理就会得到长足的发展，整个人工智能体系就会得到很好的推进，也有更多的场景可以落地。</p>
</blockquote>
<h2 id="2-语言模型"><a href="#2-语言模型" class="headerlink" title="2 语言模型"></a>2 语言模型</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><ul>
<li>定义：语言模型是用来刻画一个句子（词串序列）存在可能性的概率模型</li>
</ul>
<h3 id="N-Gram-语言模型"><a href="#N-Gram-语言模型" class="headerlink" title="N-Gram 语言模型"></a>N-Gram 语言模型</h3><h4 id="N-Gram模型"><a href="#N-Gram模型" class="headerlink" title="N-Gram模型"></a>N-Gram模型</h4><ul>
<li><p>基于历史的模型，有限视野假设(Limited Horizon)：当前词出现的概率只和它前面的k个词相关 </p>
<script type="math/tex; mode=display">
P(W)=P(w_{1}w_{2} \ldots w_{n})= \prod _{i=1..n}P(w_{i}|W_{i-k} \cdots W_{i-1})</script><ul>
<li>二元语言模型：（k=1）1阶马尔可夫链 $ P(W)=P(w_{1}w_{2} \cdots w_{n})=P(w_{1}) \prod _{i=2..n}P(w_{i}|w_{i-1}) $ </li>
<li>三元语言模型：（k=2）2阶马尔可夫链 $ P(W)=P(w_{1}w_{2} \ldots w_{n})=P(w_{1})P(w_{2}|w_{1}) \prod _{i=3..n}P(w_{i}|w_{i-2}w_{i-1}) $ </li>
</ul>
</li>
<li><p>N-Gram参数估计：相对频率（最大似然）估计</p>
<script type="math/tex; mode=display">
P(w_i|w_{i-1})=\dfrac{P(w_{i-1}w_i)}{P(w_{i-1})}=\dfrac{Count(w_{i-1}w_i)}{\sum_wCount(w_{i-1}w)}=\dfrac{Count(w_{i-1}w_i)}{Count(w_{i-1})}</script></li>
<li><p>Zipf Law：如果以词频排序，词频和排位的乘积是一个常数。</p>
<ul>
<li>Zipf法则隐含的意义：大部分的词都稀有</li>
<li>语言中频繁出现的事件是有限的，不可能搜集到足够的数据来得到稀有事件的完整概率分布。</li>
<li>词（一元）如此，对于二元、三元模型更加严重</li>
<li>零概率还会向下传播，一个2元或者3元文法的零概率，会导致整个句子的零概率</li>
</ul>
</li>
<li>数据稀疏（零概率）：没有足够的训练数据，对于未观测到的数据，出现零概率现象<ul>
<li>解决方案<ul>
<li>构造等价类</li>
<li>参数平滑</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="参数平滑"><a href="#参数平滑" class="headerlink" title="参数平滑"></a>参数平滑</h4><ul>
<li>平滑是指给没观察到的N元组合赋予一个概率值，以保证词序列总能通过语言模型得到一个概率值。约束总该率和为1。</li>
<li><p>思想：稍微减少已观察到的事件概率的大小，同时把少量概率分配到没有看到过的事件上，折扣法，使整个事件空间的概率分布曲线更加平滑。改进模型的整体效果。</p>
</li>
<li><p>“劫富济贫”：高概率调低点，小概率或者零概率调高点。</p>
</li>
<li><p>1、加法平滑方法：简单，效果不好</p>
<script type="math/tex; mode=display">P_{add- \delta }(x)= \dfrac{c(x)+ \delta }{ \sum _{x^{ \prime }}(c(x^{ \prime })+ \delta )}</script><p>加一平滑前：</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2-1.png" alt="image-20210107200039583"></p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2-2.png" alt="image-20210107200108334"></p>
<p>加一平滑后：</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2-3.png" alt="2-3" style="zoom:60%;"></p>
</li>
<li><p>2、Laplace平滑</p>
</li>
<li><p>3、简单线性插值平滑</p>
</li>
</ul>
<h4 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h4><ul>
<li>困惑度(Perplexity)：对测试集存在的概率</li>
</ul>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2-4.png" alt="2-4" style="zoom: 80%;"></p>
<h4 id="N-gram的缺点"><a href="#N-gram的缺点" class="headerlink" title="N-gram的缺点"></a>N-gram的缺点</h4><p>从语言具有的特性看，显得过于简单和幼稚</p>
<ul>
<li>数据稀疏、参数空间过大</li>
<li>基于词的，无长距离依赖，无结构、语义支持</li>
<li>泛化能力差</li>
</ul>
<h3 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h3><ul>
<li>对比N-gram和FNNLM的优缺点 </li>
<li>FNNLM：复杂，自动提取特征、泛化能力强</li>
</ul>
<h3 id="往年考题-1"><a href="#往年考题-1" class="headerlink" title="往年考题"></a>往年考题</h3><ul>
<li>自左向右定义一个n-gram，得到一个三元文法，也可以按照相反的顺序自右向左定义得到一个三元文法，证明二者等价。</li>
<li></li>
</ul>
<h2 id="3-文本分类"><a href="#3-文本分类" class="headerlink" title="3 文本分类"></a>3 文本分类</h2><h3 id="朴素贝叶斯模型"><a href="#朴素贝叶斯模型" class="headerlink" title="朴素贝叶斯模型"></a>朴素贝叶斯模型</h3><p>基本思想：<strong>利用特征项和类别的<u>联合概率</u>来估计给定文档的<u>类别概率</u></strong></p>
<p>D为待分类的文档，$c_k$指第k个类别</p>
<script type="math/tex; mode=display">
\begin{align}
argmax_{c_k}P(c_k|D)&=arg \max _{c_{k}}\dfrac{P(D|c_k)P(c_k)}{P(D)}\\&=arg \max _{c_{k}}P(D|c_k)P(c_k) \\
\end{align}</script><h4 id="伯努利文档模型"><a href="#伯努利文档模型" class="headerlink" title="伯努利文档模型"></a>伯努利文档模型</h4><ul>
<li>一个文档被表示成01向量，向量中每一个元素表示相应的单词是否在文档中出现了</li>
<li>令$D_i$表示第i个文档的01向量；$D_{it}$表示第i个文档的01向量中第t个元素的值（是一个随机变量），即单词$w_t$是否在文档i中出现了</li>
<li>$P(w_t|c_k)$表示单词$w_t$在$c_k$类文档中出现的概率，估计：$P(w_t|c_k)=\dfrac{c_k中w_t出现的文档个数}{c_k中所有文档的个数}$ </li>
<li>估计：$ P(c_{k})= \dfrac{c_k类文档数}{所有文档数} $ </li>
<li>$c_k$类文档中特征$it$出现的条件估计：$P(D_{it}|c_k)=D_{it}P(w_t|c_k)+(1-D_{it})(1-P(w_t|c_k))$ <ul>
<li>$D_{it}=0$时，$P(D_{it}|c_k)=1-P(w_t|c_k)$</li>
<li>$D_{it}=1$时，$P(D_{it}|c_k)=P(w_t|c_k)$</li>
</ul>
</li>
<li>$P(D_i|C_k)=\prod_{t=1}^{|V|}P(D_{it}|c_k)$</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
argmax_{c_k}P(c_k|D)&=arg \max _{c_{k}}\dfrac{P(D|c_k)P(c_k)}{P(D)}\\&=arg \max _{c_{k}}P(D|c_k)P(c_k) \\
&=arg \max _{c_{k}}P(c_{k}) \prod _{i=1}^{W_{t}} \left[ D_{jt}P(w_{t}|c_{k})+(1-D_{jt})(1-P(w_{t}|c_{k})) \right]
\end{align}</script><p>例：PPT 13-15 </p>
<h4 id="多项式文档模型（Multinomial-document-model）"><a href="#多项式文档模型（Multinomial-document-model）" class="headerlink" title="多项式文档模型（Multinomial document model）"></a>多项式文档模型（Multinomial document model）</h4><ul>
<li><p>一个文档被表示成整数向量，向量中每一个元素表示相应的单词在文档中出现了多少次</p>
<script type="math/tex; mode=display">
P(D_{i}|c_{k})= \frac{n_{i}!}{ \prod _{i=1}^{|V|}D_{it}!} \prod _{i=1}^{|V|}P(w_{t}|c_{k})^{D_{it}}</script></li>
<li><p>估计单词$w_i$在$c_k$类文档出现的概率：$ \widehat{P}(w_{i}|c_{k})= \dfrac{ \sum _{i=1}^{N}D_{ij}z_{ik}}{ \sum _{s=1}^{|V|} \sum _{i=1}^{N}D_{is}z_{ik}} $</p>
</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
argmax_{c_k}P(c_k|D)&=arg \max _{c_{k}}\dfrac{P(D|c_k)P(c_k)}{P(D)}\\&=arg \max _{c_{k}}P(D|c_k)P(c_k) \\
&=arg \max _{c_{k}}P(c_{k}) \frac{n_{i}!}{ \prod _{i=1}^{|V|}D_{it}!} \prod _{i=1}^{|V|}P(w_{t}|c_{k})^{D_{it}} \\ &=arg \max _{c_{k}}P(c_{k})\prod _{i=1}^{|V|}P(w_{t}|c_{k})^{D_{it}}
 \\ &=arg \max _{c_{k}}P(c_{k}) \prod _{h=1}^{len(D_{i})}P(u_{h}|c_{k}) \end{align}</script><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><ul>
<li><p>训练句向量</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/3-1.png" alt="3-1" style="zoom:60%;"></p>
</li>
<li><p>每个特征具有一个权重</p>
</li>
<li>定义一个线性函数为句子的类别打分：$ score(y,w)=w^{T}f(y) $ </li>
</ul>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/3-2.png" alt="3-2" style="zoom:70%;"></p>
<ul>
<li><p>预测</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/3-3.png" alt="3-3" style="zoom:60%;"></p>
</li>
<li><p>朴素贝叶斯模型其实属于线性模型</p>
</li>
<li><p>权重学习</p>
<ul>
<li>Perceptron: 0-1 loss</li>
<li>Logistic Regression ( Maximum Entropy ): log-loss</li>
<li>SVM: hinge-loss</li>
</ul>
</li>
</ul>
<h3 id="文本表示"><a href="#文本表示" class="headerlink" title="文本表示"></a>文本表示</h3><h4 id="特征选择方法"><a href="#特征选择方法" class="headerlink" title="特征选择方法"></a>特征选择方法</h4><ul>
<li><p>停词删除</p>
</li>
<li><p>基于文档频率(DF)的特征提取法<br>从训练预料中统计出包含某个特征的文档的频率(个数),然后根据设定的阈值，当该特征项的DF值小于某个阈值时，从特征空间中去掉该特征项，因为该特征项使文档出现的频率太低，没有代表性；当该特征项的DF值大于另外一个阈值时，从特征空间中也去掉该特征项，因为该特征项使文档出现的频率太高，没有区分度</p>
</li>
<li><p><strong>互信息</strong>（计算？ PPT 39-41）</p>
<ul>
<li>互信息越大，特征$t_i$和类别$C_j$共现的程度越大</li>
</ul>
<script type="math/tex; mode=display">
I(X;Y)= \sum _{y \in Y} \sum _{Y \in X}p(x,y) \log ( \frac{p(x,y)}{p(x)p(y)})</script></li>
</ul>
<ul>
<li><p>$ \chi^{2}(Chi-Square) $ 检验</p>
<ul>
<li>衡量特征$t_i$和类别$C_j$之间的相关联程度</li>
</ul>
</li>
</ul>
<h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><ul>
<li>TF(词频)<ul>
<li>出现次数越多的词越重要</li>
<li>标准化后：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
TF=\dfrac{某个词在文章中的出现次数}{文章中出现最多词的个数}</script><ul>
<li><p>IDF(逆文档频率)</p>
<ul>
<li>出现在许多不同文档中的词对主题的指示性较差</li>
<li>文档频率：包括了该词的文档个数/文档总数</li>
<li>逆文档频率：文档频率倒数</li>
</ul>
<script type="math/tex; mode=display">
IDF=\log \dfrac{语料库的文档总数}{包含该词的文档数+1}</script></li>
<li><p>tf-idf</p>
<ul>
<li>衡量词$i$在文档$j$中表示的重要程度/区分能力的强弱</li>
</ul>
<script type="math/tex; mode=display">
W_{ij}=tf_{ij}\cdot idf_{i}=tf_{ij} \cdot \log _{2}(N/df_{i})</script></li>
</ul>
<h3 id="文档分类性能评价"><a href="#文档分类性能评价" class="headerlink" title="文档分类性能评价"></a>文档分类性能评价</h3><ul>
<li>二分类：准确率、查全率、查准率、F1-score</li>
<li>微平均和宏平均</li>
</ul>
<h2 id="4-情感分类"><a href="#4-情感分类" class="headerlink" title="4 情感分类"></a>4 情感分类</h2><ul>
<li>情感分析中如何表示词和句子，才有可能得到更好的情感分类？</li>
</ul>
<h2 id="5-表示学习"><a href="#5-表示学习" class="headerlink" title="5 表示学习"></a>5 表示学习</h2><p>文本表示的作用就是将这些非结构化的信息转化为结构化的信息。</p>
<h3 id="符号编码（Symbolic-Encoding）"><a href="#符号编码（Symbolic-Encoding）" class="headerlink" title="符号编码（Symbolic Encoding）"></a>符号编码（Symbolic Encoding）</h3><h4 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h4><ul>
<li><p>独热编码（One-hot encoding）</p>
<ul>
<li>一位有效编码</li>
<li>假设词之间没有相似性，所有向量正交</li>
</ul>
</li>
<li><p>词袋模型：文档中所有单词独热编码的和</p>
<ul>
<li>优点：简单</li>
<li><font color="red"> <strong>缺点：</strong></font><ul>
<li><strong>忽略了词的相似性（同义词、多义词）</strong></li>
<li><strong>维度灾难（可用特征选择方法减轻）</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>N-gram的词袋模型</p>
<ul>
<li><p>Vocab = set of all n-grams in corpus</p>
</li>
<li><p>Document = n-grams in document w.r.t vocab with multiplicity </p>
</li>
<li><p>例：</p>
<ul>
<li><p>Sentence 1: “The cat sat on the hat” </p>
<p>Sentence 2: “The dog ate the cat and the hat” </p>
<p>Vocab = { the cat, cat sat, sat on, on the, the hat, the dog, dog ate, ate the, cat and, and the} </p>
<p>Sentence 1: { 1, 1, 1, 1, 1, 0, 0, 0, 0, 0} </p>
<p>Sentence 2 : { 1, 0, 0, 0, 0, 1, 1, 1, 1, 1} </p>
</li>
</ul>
</li>
<li><p>维度过高</p>
<ul>
<li>特征工程、特征过滤</li>
</ul>
</li>
</ul>
</li>
<li><p>它没有考虑词与词之间内在的联系性，如何表达词之间的相似性？</p>
<ul>
<li>借助词分类相关的外部知识如WordNet</li>
</ul>
</li>
<li><p>能否用一个<strong>连续的稠密向量去刻画一个word的特征?</strong></p>
</li>
</ul>
<h3 id="潜在语义索引（Latent-Semantic-Index）"><a href="#潜在语义索引（Latent-Semantic-Index）" class="headerlink" title="潜在语义索引（Latent Semantic Index）"></a>潜在语义索引（Latent Semantic Index）</h3><ul>
<li><p>刻画单词的分布式语义表示</p>
</li>
<li><p>“单词-文档”共现矩阵</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/5-1.png" alt="5-1" style="zoom:60%;"></p>
</li>
<li><p>奇异值分解：$ X=U \sum V^{T} $ </p>
</li>
<li><p>对角阵$\sum$中的奇异值由大到小排列，由于值越大的奇异值越能代表矩阵的主要特征，因此可以忽略较小的奇异值，从而压缩空间。忽略后重新计算三个矩阵的乘积，得到新的”单词-文档”共现矩阵$ X_{k}=U_{k} \sum _{k}V_{k}^{T} $ (<strong>降维</strong>)</p>
</li>
<li><p>词相似度矩阵：$XX^T$</p>
</li>
<li><p>文档相似度矩阵：$X^TX$</p>
</li>
<li><p>优点</p>
<ul>
<li>剔除噪声</li>
<li>为单词和文档之间建立语义关联<ul>
<li>两篇具有相似词分布的文档可以被认为是有着相近的主题。</li>
<li><strong>上下文环境相似的两个词有着相近的语义</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>计算开销大</li>
<li>线性模型，不能处理非线性依赖</li>
<li>k值难设置</li>
<li>未考虑词的顺序</li>
<li>难以增加新的词和文档</li>
</ul>
</li>
</ul>
<h3 id="词嵌入向量（WordEmbedding）"><a href="#词嵌入向量（WordEmbedding）" class="headerlink" title="词嵌入向量（WordEmbedding）"></a>词嵌入向量（WordEmbedding）</h3><ul>
<li><p>WordEmbedding矩阵给每个单词分配一个固定长度的向量表示，这个长度可以自行设定，比如300，实际上会远远小于字典长度（比如10000）。而且两个单词向量之间的夹角值可以作为他们之间关系的一个衡量。通过简单的余弦函数，我们就可以计算两个单词之间的相关性，简单高效：</p>
<script type="math/tex; mode=display">
similarity= \cos ( \theta )= \frac{A \cdot B}{||A||_{2}||B||_{2}}</script></li>
<li><p>优势</p>
<ul>
<li>维度小</li>
<li>语意相似的词在向量空间上也会比较相近。</li>
<li>通用性很强，可以用在不同的任务中。</li>
</ul>
</li>
<li>应用<ul>
<li>计算相似度，比如man和woman的相似度比man和apple的相似度高；</li>
<li>在一组单词中找出与众不同的一个，例如在如下词汇列表中：[dog, cat, chicken, boy]，利用词向量可以识别出boy和其他三个词不是一类；</li>
<li>直接进行词的运算，例如经典的：woman+king-man =queen；</li>
<li>由于携带了语义信息，还可以计算一段文字出现的可能性，也就是说，这段文字是否通顺。</li>
</ul>
</li>
</ul>
<h4 id="NNLM"><a href="#NNLM" class="headerlink" title="NNLM"></a>NNLM</h4><ul>
<li><p>一个简单的前向反馈神经网络$ f(w_{t-n+1}, \ldots ,w_{t}) $ 来拟合一个词序列的条件概率$ p(w_{t}|w_{1},w_{2}, \ldots ,w_{t-1}) $ </p>
<ul>
<li>首先是一个线性的Embedding层。它将输入的N−1个one-hot词向量，通过一个共享的D×V的矩阵C，映射为N−1个分布式的词向量（distributed vector）。其中，V是词典的大小，D是Embedding向量的维度（一个先验参数）。<strong>C矩阵里存储了要学习的word vector。</strong></li>
<li>其次是一个简单的前向反馈神经网络g。它由一个tanh隐层和一个softmax输出层组成。通过将Embedding层输出的N−1个词向量映射为一个长度为V的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估</li>
</ul>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/5-2.png" alt="5-2" style="zoom:80%;"></p>
</li>
</ul>
<h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><ul>
<li><p>CBOW (Continuous Bag-of-Words Model)</p>
<p>通过上下文来预测当前值。相当于一句话中扣掉一个词，让你猜这个词是什么。</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/MyBlog\source\_drafts\【复习笔记】自然语言处理\5-4.png" alt="5-4" style="zoom:80%;"></p>
<p>过程简单介绍如下：<br>（1）输入为C个V维的vector。其中C为上下文窗口的大小，V为原始编码空间的规模。例如，示例中的C=2，V=4.两个vector分别为4维的He和is的one-hot编码形式；<br>（2）激活函数相当简单，在输入层和隐藏层之间，每个input vector分别乘以一个VxN维度的矩阵，得到后的向量各个维度做平均，得到隐藏层的权重。隐藏层乘以一个NxV维度的矩阵，得到output layer的权重；<br>（3）隐藏层的维度设置为理想中压缩后的词向量维度。示例中假设我们想把原始的4维的原始one-hot编码维度压缩到2维，那么N=2；<br>（4）输出层是一个softmax层，用于组合输出概率。所谓的损失函数，就是这个output和target之间的的差（output的V维向量和input vector的one-hot编码向量的差），该神经网络的目的就是最小化这个loss；<br>（5）优化结束后，隐藏层的N维向量就可以作为Word-Embedding的结果。如此一来，便得到了既携带上下文信息，又经过压缩的稠密词向量。</p>
</li>
<li><p>Skip-gram  (Continuous Skip-gram Model)</p>
<p>用当前词来预测上下文。相当于给你一个词，让你猜前面和后面可能出现什么词。</p>
</li>
</ul>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/5-3.png" alt="5-3" style="zoom:60%;"></p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><ul>
<li><p>LTSM：缓解梯度爆炸、消失问题</p>
</li>
<li><p>GRU（Gated Recurrent Unit）</p>
</li>
</ul>
<h3 id="往年考题-2"><a href="#往年考题-2" class="headerlink" title="往年考题"></a>往年考题</h3><ul>
<li>（1）tf-idf的计算公式。<br>（2）词的分布式表示word embedding相比较于one-hot表示的优势。</li>
</ul>
<h2 id="6-词性标注与隐马尔科夫模型"><a href="#6-词性标注与隐马尔科夫模型" class="headerlink" title="6 词性标注与隐马尔科夫模型"></a>6 词性标注与隐马尔科夫模型</h2><h3 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h3><ul>
<li>定义及任务描述<ul>
<li>词性又称词类，是指词汇基本的语法属性。</li>
<li>划分词类的依据：词的形态、词的语法功能、词的语法意义</li>
</ul>
</li>
<li>词性标注的问题－ 标注歧义（兼类词）<ul>
<li>一个词具有两个或者两个以上的词性</li>
<li>对兼类词消歧</li>
</ul>
</li>
<li>词性标注之重要性<ul>
<li>句法分析的预处理</li>
<li>机器翻译</li>
<li>Text – Speech （record）</li>
</ul>
</li>
<li>词性标注方法<ul>
<li>规则方法：<ul>
<li>词典提供候选词性</li>
<li>人工整理标注规则</li>
</ul>
</li>
<li>基于错误驱动的方法<ul>
<li>错误驱动学习规则</li>
<li>利用规则重新标注词性</li>
</ul>
</li>
<li>统计方法<ul>
<li>问题的形式化描述</li>
<li>建立统计模型<ul>
<li>HMM方法</li>
<li>最大熵方法</li>
<li>条件随机场方法</li>
<li>结构化支持向量机方法</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>词性标注的性能指标<ul>
<li>性能指标：标注准确率</li>
<li>当前方法正确率可以达到97%<ul>
<li>正确率基线(Baseline)可以达到90%</li>
<li>基线的做法：<ul>
<li>给每个词标上它最常见的词性</li>
<li>所有的未登录词标上名词词性</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>形式化为一个分类问题<ul>
<li>词串：$x_1x_2…x_n$; 词性串：$y_1y_2…y_n$</li>
</ul>
</li>
<li>决定一个词词性的因素<ul>
<li>从语言学角度：由词的用法以及在句中的语法功能决定</li>
<li>统计学角度：<ul>
<li>和上下文的词性（前后词的标注）相关</li>
<li>和上下文单词（前后词）相关</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="隐马尔科夫模型（HMM）"><a href="#隐马尔科夫模型（HMM）" class="headerlink" title="隐马尔科夫模型（HMM）"></a>隐马尔科夫模型（HMM）</h3><h4 id="马尔科夫模型"><a href="#马尔科夫模型" class="headerlink" title="马尔科夫模型"></a>马尔科夫模型</h4><ul>
<li><p>一个系统有$N$个有限状态$S=\{s_1,s_2,…s_N\}$，$Q=(q_1,q_2,…q_T)$是一个随机变量序列。随机变量的取值<br>为状态集$S$中的某个状态。</p>
</li>
<li><p>系统在时间$t$处于状态$s_j$的概率取决于其在时间$1,2,…,t-1$的状态，该概率为：</p>
<script type="math/tex; mode=display">
P(q_t=s_j|q_{t-1}=s_i,q_{t-2}=s_k,...,q_1=s_h)</script></li>
<li><p>假设1：有限视野假设</p>
</li>
<li><p>假设2：时间独立性(No change over time)</p>
</li>
<li><p>示例： 天气预报</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-2.png" alt="6-2" style="zoom:60%;"></p>
<ul>
<li><p>预测：计算未来天气（序列的概率）</p>
<ul>
<li><p>晴-雨-晴-雨-晴-多云-晴，未来七天天气是这种情况的概率</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-3.png" alt="6-3" style="zoom:60%;"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="隐马尔科夫模型"><a href="#隐马尔科夫模型" class="headerlink" title="隐马尔科夫模型"></a>隐马尔科夫模型</h4><ul>
<li><p>词串W，词性串S</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-1.png" alt="6-1" style="zoom:40%;"></p>
</li>
<li><p>HMM是一阶马尔可夫模型的扩展</p>
<ul>
<li>隐藏的状态序列满足一阶马尔可夫模型</li>
<li>观察值与状态之间存在概率关系</li>
</ul>
</li>
<li><p>相对于markov模型的又一假设：输出独立性</p>
<p>$ P(O_{1}, \ldots O_{T}|S_{1}, \ldots S_{T})= \prod _{t}P(O_{t}|S_{t}) $ </p>
</li>
<li><p>HMM模型用于词性标注</p>
<ul>
<li>$S$：状态集<ul>
<li>预先定义的词性标注集</li>
</ul>
</li>
<li>$V$：观察集<ul>
<li>文本中的词汇</li>
</ul>
</li>
<li>$A$：词性之间的转移概率</li>
<li>$B$：某个词性生成某个词的概率<ul>
<li>例，P(我|“代词”)</li>
</ul>
</li>
<li>$π$：初始概率</li>
<li>可见的观察序列为$w_1w_2…w_T$</li>
</ul>
</li>
</ul>
<h3 id="隐马尔科夫模型的三个基本问题"><a href="#隐马尔科夫模型的三个基本问题" class="headerlink" title="隐马尔科夫模型的三个基本问题"></a>隐马尔科夫模型的三个基本问题</h3><h4 id="求解观察序列的概率"><a href="#求解观察序列的概率" class="headerlink" title="求解观察序列的概率"></a>求解观察序列的概率</h4><p>给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$,计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$</p>
<ul>
<li>暴力算法：枚举所有可能的状态序列，依次算出其生成观察序列$O$的概率，求和<ul>
<li>指数爆炸</li>
</ul>
</li>
</ul>
<h5 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h5><ul>
<li><p>定义前向概率:</p>
<p>给定隐马尔科夫模型$\lambda$,定义到时刻$t$部分观测序列为$o_1,o_2,…,o_t$且状态为$s_i$的概率为前向概率,记作</p>
<script type="math/tex; mode=display">
\alpha_t(i)=P(o_1,o_2,...,o_t,q_t=s_i|\lambda)</script></li>
<li><p>迭代计算前向概率</p>
<script type="math/tex; mode=display">
\alpha_{t+1}(j)=\left[ \sum_{i=1}^N\alpha_t(i)a_{ij}\right]b_j(o_{t+1}), i=1,2,...N</script><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-4.png" alt="6-4" style="zoom:75%;"></p>
</li>
<li><p>$P(O|\lambda)$可由前向概率计算而得：$P(O|\lambda)=\sum_{i=1}^{N}\alpha_T(i)$</p>
<ul>
<li>在时间$T$，HMM输出了序列$o_1,o_2,…,o_T$，且位于每个状态$s_i$的概率之和</li>
</ul>
</li>
</ul>
<blockquote>
<p>输入:隐马尔科夫模型$\lambda$,观测序列$O$</p>
<p>输出:观测序列概率$P(O|\lambda)$</p>
<p>(1) 初值：初始位于$s_i$的概率$\times s_i$生成$o_1$的概率</p>
<script type="math/tex; mode=display">\alpha_1(i)=\pi_ib_i(o_1), i=1,2,...,N</script><p>(2) 递推　对t=1,2,…,T-1</p>
<script type="math/tex; mode=display">\alpha_{t+1}(j)=\left[ \sum_{i=1}^N\alpha_t(i)a_{ij}\right]b_j(o_{t+1}), i=1,2,...N</script><p>(3) 终止</p>
<script type="math/tex; mode=display">P(O|\lambda)=\sum_{i=1}^{N}\alpha_T(i)</script></blockquote>
<ul>
<li>复杂度：$O(N^2T)$</li>
</ul>
<h5 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h5><ul>
<li>定义后向概率：给定隐马尔科夫模型$\lambda$,定义在时刻$t$状态为$q_i$的条件下，从$t+1$到$T$的部分观测序列为$o_{t+1},o_{t+2},…,o_T$的概率为后向概率，记作</li>
</ul>
<script type="math/tex; mode=display">
\beta_t(i)=P(o_{t+1},o_{t+2},...,o_T|q_t=s_i,\lambda)</script><blockquote>
<p>输入:隐马尔可夫模型$\lambda$,观测序列$O$</p>
<p>输出:观测序列概率$P(O|\lambda)$</p>
<p>(1) 初值</p>
<script type="math/tex; mode=display">\beta_T(i)=1,i=1,2,...,N</script><p>(2)递推：对$t=T-1,T-2,…,1$</p>
<script type="math/tex; mode=display">\beta_t(i)=\sum_{j=1}^{N}a_{ij}b_j(o_{t+1})\beta_{t+1}(j),i=1,2...N</script><p>(3)求和终结</p>
<script type="math/tex; mode=display">P(O|\lambda)=\sum_{i=1}^N\pi_ib_i(o_1)\beta_1(i)</script></blockquote>
<h4 id="求解最优状态序列"><a href="#求解最优状态序列" class="headerlink" title="求解最优状态序列"></a>求解最优状态序列</h4><p>已知模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$,求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,…,i_T)$.即给定观测序列，求最有可能的对应的状态序列。</p>
<p><strong>维特比算法</strong></p>
<ul>
<li><p>定义维特比变量：在时间$t$时，HMM沿着某一条路径到达状态$s_i$，并输出观察序列$O_1O_2…O_t$的最大概率：</p>
<script type="math/tex; mode=display">
\delta _{t}(i)=\max _{q_1,q_2,...,q_{t-1}}P(q_1,q_2,...,q_t=s_{i},O_{1}O_{2} \cdots O_{t}| \mu )</script></li>
<li><p>与前向算法类似的递推关系：</p>
<script type="math/tex; mode=display">
\delta _{t+1}(i)= \max_j \left[ \delta _{i}(j) \cdot a_{ij} \right] \cdot b_{i}(O_{t+1})</script></li>
</ul>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-5.png" alt="6-5" style="zoom:67%;"></p>
<h4 id="HMM参数估计"><a href="#HMM参数估计" class="headerlink" title="HMM参数估计"></a>HMM参数估计</h4><p>已知观测序列$O=(o_1,o_2,…,o_T)$，估计模型$\lambda=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大。即用极大似然估计的方法估计参数。</p>
<h5 id="有指导学习模型参数——从标注语料中学习"><a href="#有指导学习模型参数——从标注语料中学习" class="headerlink" title="有指导学习模型参数——从标注语料中学习"></a>有指导学习模型参数——从标注语料中学习</h5><ul>
<li><p>最大似然估计</p>
<script type="math/tex; mode=display">
\bar{\pi}_i=\delta(q_1,s_i)</script><script type="math/tex; mode=display">
\begin{aligned}
    \bar{a}_{ij}&=\dfrac{Q中从状态q_i转移到q_j的次数}{Q中所有从状态q_i转移到另一状态(包括q_i自身)的次数}\\
    &=\dfrac{\sum_{t=1}^{T-1}\delta(q_t,s_i)*\delta(q_{t+1},s_j)}{\sum_{t=1}^{T-1}\delta(q_t,s_i)}
\end{aligned}</script><script type="math/tex; mode=display">
\bar{b}_j(k)=\dfrac{Q中从状态q_j输出符号v_k的次数}{Q到达q_j的次数}</script></li>
</ul>
<h5 id="无指导学习模型参数——Welch-Baum-算法（前向后向算法）"><a href="#无指导学习模型参数——Welch-Baum-算法（前向后向算法）" class="headerlink" title="无指导学习模型参数——Welch-Baum 算法（前向后向算法）"></a>无指导学习模型参数——Welch-Baum 算法（前向后向算法）</h5><p>由于HMM中的状态序列Q是观察不到的(隐变量),因此，这种最大似然估计的方法不可行。所幸的是，期望最大化(expectation maximization,EM)算法可以用于含有隐变量的统计模型的参数最大似然估计。</p>
<p>基本思想：随机给出模型参数的初始化值，得到最初的模型λ0，然后利用初始模型λ0得到某一状态转移到另一状态的期望次数，然后利用期望次数对模型进行重新估计，由此得到模型λ1，如此循环迭代，重新估计，直至模型参数收敛（模型最优）。</p>
<h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><ul>
<li>与HMM比较</li>
</ul>
<h3 id="往年考题-3"><a href="#往年考题-3" class="headerlink" title="往年考题"></a>往年考题</h3><ul>
<li>HMM，“隐”在何处？适合处理哪一类问题？维特比算法的内容？<ul>
<li>我们不知道模型所经过的状态序列，只知道状态的概率函数，观察到的事件是状态的随机函数。</li>
</ul>
</li>
<li>给定一个HMM模型<br>（1）计算某个序列存在的概率<br>（2）画个图，描述一下HMM的转移过程</li>
<li>天气晴雨</li>
</ul>
<h2 id="7-句法分析"><a href="#7-句法分析" class="headerlink" title="7 句法分析"></a>7 句法分析</h2><h3 id="句法分析简介"><a href="#句法分析简介" class="headerlink" title="句法分析简介"></a>句法分析简介</h3><ul>
<li>语言视作由组合语法规则制约、旨在传达语义的记号形式系统</li>
<li>句法：指一门语言里支配句子结构，决定词、短语、从句等句子成分如何组成其上级成分，直到组成句子的规则或过程。[1]这一概念常常与语法混淆，盖因语法研究里面很大一部分都是关于句法的内容，但语法不仅关注句子结构的形成，也关注句子成分的语法功能和语法意义。</li>
<li>形式语言：给出了语言的语法规则和分类的形式化方法（形式文法），通过对字母、单词、句子的定义，我们用精确的、数学可描述或机器可处理的公式来定义语言</li>
<li>上下文无关文法<ul>
<li>示例 PPT12</li>
<li>歧义<ul>
<li>程序设计语言<ul>
<li>Very constrained grammars</li>
<li>LL(1) grammar, LR Grammar</li>
<li>推导或规约过程中可以通过一些约束选择正确的产生式规则</li>
</ul>
</li>
<li>自然语言<ul>
<li>难以避免的歧义</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="概率上下文无关文法（PCFG）"><a href="#概率上下文无关文法（PCFG）" class="headerlink" title="概率上下文无关文法（PCFG）"></a>概率上下文无关文法（PCFG）</h3><ul>
<li>Input: 文法$G$，输入串$string$（句子）</li>
<li>Output: 推导序列或语法树</li>
<li>概率上下文无关文法$G=(N,T,S,R,P)$<ul>
<li>$N$是非终结符号集合</li>
<li>$T$是终结符号集合</li>
<li>$S$是开始符号</li>
<li>$R$是产生式规则，均形如$X→Y_1Y_2…Y_n$，for $n≥0$, $X∈V,Yi ∈(V∪T)$</li>
<li>$P(R)$ 每条产生式规则赋予的概率</li>
</ul>
</li>
<li>句法结构（子树）的概率=每个节点概率的乘积</li>
</ul>
<h3 id="PCFG的三个基本问题"><a href="#PCFG的三个基本问题" class="headerlink" title="PCFG的三个基本问题"></a>PCFG的三个基本问题</h3><h4 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h4><p>给定一个句子$W=w_1w_2…w_n$和文法$G$,如何快速计算概率$P(W|G)$</p>
<h5 id="内向算法"><a href="#内向算法" class="headerlink" title="内向算法"></a><font color="red">内向算法</font></h5><ul>
<li><p>利用动态规划算法计算非终结符$A$推导出$W$中子串$w_iw_{i+1}…w_j$的概率$a_{ij}(A)$</p>
</li>
<li><p>则有：$ P(W|G)=P(S \Rightarrow W|G)= \alpha _{1n}(S) $ </p>
</li>
<li><p>有递推公式如下:</p>
<script type="math/tex; mode=display">
a_{ii}(A)=P(A\rightarrow w_i)</script><script type="math/tex; mode=display">
a_{ij}(A)=\sum_{B,C}\sum_{i\le k\le j}P(A\rightarrow BC)\cdot a_{ik}(B)\cdot a_{(k+1)j}(C)</script><p>算法:</p>
<blockquote>
<p>输入:PCFG $G(S)$和句子$W=w_1w_2…w_n$</p>
<p>输出:$a_{ij}(A),1\le i \le j\le n$</p>
<p>步1 初始化:$a_{ii}(A)=P(A\rightarrow w_i),1\le i\le n$</p>
<p>步2 归纳计算:$j=1…n,i=1…n-j$,重复下列计算:</p>
<script type="math/tex; mode=display">
   a_{i(i+j)}(A)=\sum_{B,C}\sum_{i\le k \le i+j-1}P(A\rightarrow BC)*a_{ik}(B)*a_{(k+1)(i+j)}(C)</script><p>步3 终结:$P(S\rightarrow w_1w_2…w_n)=a_{1n}(S)$</p>
</blockquote>
</li>
</ul>
<h5 id="外向算法"><a href="#外向算法" class="headerlink" title="外向算法"></a>外向算法</h5><ul>
<li><p>定义外向变量$\beta_{ij}(A)$为初始非终结符$S$在推导出语句$W=w_1w_2…w_n$的过程中，产生符号串$w_1…w_{i-1}Aw_{j+1}…w_n$的概率（隐含着$A \rightarrow_* w_i…w_j$</p>
</li>
<li><p>有如下递推公式:</p>
<script type="math/tex; mode=display">
\beta_{1n}(A)=\left\{
\begin{array}{rcl}
1 & & {A=S}\\
0 & & {A\neq S}\\
\end{array} \right.</script><script type="math/tex; mode=display">
\begin{aligned} % requires amsmath; align* for no eq. number
\beta_{ij}(A) & =\sum_{B,C}\sum_{k\gt j}P(B\rightarrow AC)\alpha_{(j+1)k}(C)\beta_{ik}(B) \\
   & \ \ \ +\sum_{B,C}\sum_{k\lt i}P(B\rightarrow CA)\alpha_{k(i-1)}(C)\beta_{kj}(B)\\
\end{aligned}</script></li>
</ul>
<h4 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h4><p>给定一个句子$W=w_1w_2…w_n$和文法$G$,如何选择该句子的最佳结构？即选择句法结构树$t$使其具有最大概率:$argmax_tP(t|W,G)$</p>
<h5 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a><font color="red">维特比算法</font></h5><ul>
<li><p>定义维特比变量$ \gamma _{i,j}(A) $ :由给定非终结符号$A$生成子串$W_{ij}$的最大概率。</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/7-1.png" alt="7-1" style="zoom:75%;"></p>
</li>
</ul>
<h5 id="自底向上分析器-CKY算法"><a href="#自底向上分析器-CKY算法" class="headerlink" title="自底向上分析器(CKY算法)"></a>自底向上分析器(CKY算法)</h5><ul>
<li>空间复杂度：|symbols|*n^2 doubles</li>
<li>时间复杂度：|rules|*n^3</li>
</ul>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/7-2.png" alt="7-2" style="zoom: 67%;"></p>
<ul>
<li>例：PPT44,书190</li>
</ul>
<h4 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h4><p>给定PCFG G和句子$W=w_1w_2…w_n$,如何调节$G$的概率参数，使句子的概率最大?即求解$argmax_GP(W|G)$</p>
<ul>
<li><p>有监督：从树库中计数得到</p>
<ul>
<li><p>相对频率：$ P(A \rightarrow \alpha )= \dfrac{Numbcr(A \rightarrow \alpha )}{ \sum _{ \gamma }Number(A \rightarrow \gamma )} $ </p>
<ul>
<li>例如：</li>
</ul>
<p>$P(VP \rightarrow Vi)= \dfrac{Number(VP \rightarrow Vi)}{Number(VP \rightarrow Vi)+Number(VP \rightarrow VT\quad NP)+Number(VP \rightarrow VP\quad P)} $ </p>
</li>
</ul>
</li>
<li><p>无监督：没有树库</p>
</li>
<li><p>EM估计– 利用向内向外概率</p>
</li>
<li><p>其他</p>
<ul>
<li>文法来源<ul>
<li>树库中抽取文法规则</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="句法分析的性能评价"><a href="#句法分析的性能评价" class="headerlink" title="句法分析的性能评价"></a>句法分析的性能评价</h3><ul>
<li><p>三元组：[ label, start, end ]</p>
</li>
<li><p>计算precision和recall</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/7-3.png" alt="7-3" style="zoom:70%;"></p>
</li>
</ul>
<h3 id="往年考题-4"><a href="#往年考题-4" class="headerlink" title="往年考题"></a>往年考题</h3><ul>
<li>Cut the envelope with sciors。使用内向算法，PCFG进行分析。</li>
<li>给出一个上下文无关文法，基于该PCFG，画出句子的最优句法分析树。从语义的角度判断句子的最优句法分析是否合理？如果不合理，该如何解决？</li>
</ul>
<h2 id="8-信息抽取"><a href="#8-信息抽取" class="headerlink" title="8 信息抽取"></a>8 信息抽取</h2><ul>
<li>信息抽取（Information Extraction，简称IE，⼜译信息截取技术）主要是从⼤量⽂字资料中⾃动抽取特定消息（Particular Information），以作为数据库访问（Database Access）之⽤的技术。</li>
<li>⾮结构化⽂本→结构化数据 →知识库（知识）</li>
</ul>
<h3 id="实体识别"><a href="#实体识别" class="headerlink" title="实体识别"></a>实体识别</h3><p>○ ⼈名、地名、机构名…<br>○ 药物名、蛋⽩质名、基因名…<br>○ 领域专有名词</p>
<ul>
<li>规则方法</li>
<li>统计学习方法：<br>○ 任务描述<br>○ 任务形式化<br>○ 模型<br>○ 特征<br>○ 评价<br>○ ……</li>
</ul>
<h3 id="关系抽取"><a href="#关系抽取" class="headerlink" title="关系抽取"></a>关系抽取</h3><p>○ 给定头实体和尾实体，识别两者的关系</p>
<h4 id="封闭域关系抽取"><a href="#封闭域关系抽取" class="headerlink" title="封闭域关系抽取"></a>封闭域关系抽取</h4><ul>
<li>有限的实体类型，有限的关系类型</li>
<li>有监督统计学习方法：<br>○ 任务描述及任务形式化<br>○ 训练数据<br>○ 模型<br>○ 特征<br>○ 评价<br>○ ……</li>
</ul>
<h4 id="开放域关系抽取"><a href="#开放域关系抽取" class="headerlink" title="开放域关系抽取"></a>开放域关系抽取</h4><ul>
<li>包含数以千计的关系类型，百万千万级的实体</li>
<li>样本不平衡性和稀疏性</li>
<li>语⾔表达多样性和有限性</li>
</ul>
<h4 id="基于远程监督方法的关系分类"><a href="#基于远程监督方法的关系分类" class="headerlink" title="基于远程监督方法的关系分类"></a>基于远程监督方法的关系分类</h4><ul>
<li><p>关系分类的任务描述</p>
<ul>
<li>预测实体之间的关系</li>
<li>例：<ul>
<li>Sentence : Obama was born in Honolulu, Hawaii, USA as he has always said.</li>
<li>Relation : born-in</li>
<li>Fact triplet : <obama, born-in,="" usa=""></obama,></li>
</ul>
</li>
</ul>
</li>
<li><p>传统监督学习算法</p>
<ul>
<li>依赖大量标注的数据</li>
</ul>
</li>
<li><p>基于远程监督的关系分类方法 （Distant Supervision for Relation Classification）</p>
<ul>
<li><p>目标：自动生成大量标注数据</p>
</li>
<li><p>假设：如果两个实体在知识库中有关系，所有包含这两个实体的句子都会表达这个关系。</p>
<p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/8-1.png" alt="image-20210109234117470"></p>
</li>
</ul>
</li>
<li><p>挑战：数据中的噪音</p>
<ul>
<li>假设过强：<ul>
<li>误报（False Positives）：不是每个包含两个实体的句子都在知识库中提到相同的关系。</li>
<li>False Negatives：由于知识库中没有关系事实，两个实体被错误地标记为无关系(NA)，尽管它们表达了目标关系。</li>
</ul>
</li>
<li>解决<ul>
<li>抑制噪声（Suppress Noise）:<ul>
<li>de-emphasize the false positive sentences.</li>
<li>至少有一个假设:至少有一个提到这两个实体的句子表达了他们的关系。</li>
<li>多实例学习:将所有提到同一实体对的句子放入一个包中，并以包为单位训练模型。标签只分配给实例的袋子。实例没有标签。<ul>
<li>在二分类情况下，如果袋子中至少有一个实例是正的，那么袋子被标记为正，如果袋子中所有的实例都是负的，那么袋子被标记为负的</li>
</ul>
</li>
<li>缺点：不能处理所有的句子并非描述同一关系的袋子，也不能处理句子级别的预测。</li>
</ul>
</li>
<li>消除噪声(Remove Noise)<ul>
<li>消除False Positives</li>
<li>强化学习<ul>
<li>从一个句子包中识别并去除错误标记的句子，用清理过的数据集训练关系分类器。</li>
<li>由于没有直接信号表明句子是否被标错，本文提出了一种强化学习方法来解决这一问题</li>
<li>通过强化学习来学习agent，以识别每种关系类型的误报，并重新分配训练数据集。</li>
<li>奖励是通过关系分类器的性能变化来反映的，之前工作的奖励是通过预测的可能性来计算的。</li>
</ul>
</li>
<li>缺点：只针对False Positives，未考虑False Negatives</li>
</ul>
</li>
<li>纠正噪声（Rectify Noise）<br>■ 纠正False Positives和False Negatives<ul>
<li>以往的工作只是抑制或消除了误报，得到了次优决策边界。</li>
<li>False Negatives表达了与 positive数据相似的语义信息，为目标关系提供了证据。</li>
<li>因此，对含噪句子的标签进行修正有助于获得最优决策边界。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="知识库填充"><a href="#知识库填充" class="headerlink" title="知识库填充"></a>知识库填充</h3><p>○ 实体消歧，实体链接等</p>
<h3 id="事件抽取"><a href="#事件抽取" class="headerlink" title="事件抽取"></a>事件抽取</h3><h2 id="9-机器翻译"><a href="#9-机器翻译" class="headerlink" title="9 机器翻译"></a>9 机器翻译</h2><h3 id="基于规则的机器翻译（since-1950s）"><a href="#基于规则的机器翻译（since-1950s）" class="headerlink" title="基于规则的机器翻译（since 1950s）"></a>基于规则的机器翻译（since 1950s）</h3><ul>
<li>由语⾔学⽅⾯的专家进⾏规则的制订<br>– ⼀般包含词典和⽤法等组成部分</li>
<li>需要语⾔学家大量的⼯作；维护难度大；翻译规则容易发生冲突</li>
</ul>
<h3 id="基于实例的机器翻译（since-1980s）"><a href="#基于实例的机器翻译（since-1980s）" class="headerlink" title="基于实例的机器翻译（since 1980s）"></a>基于实例的机器翻译（since 1980s）</h3><ul>
<li>从语料库中学习翻译实例<ul>
<li>查找接近的翻译实例，并进⾏逐词替换进⾏翻译</li>
<li>利⽤类⽐思想analogy，避免复杂的结构分析</li>
</ul>
</li>
</ul>
<h3 id="统计机器翻译（since-1990s）"><a href="#统计机器翻译（since-1990s）" class="headerlink" title="统计机器翻译（since 1990s）"></a>统计机器翻译（since 1990s）</h3><ul>
<li>从双语平⾏语料中⾃动进⾏翻译规则的学习和应⽤</li>
<li>词对齐<ul>
<li>–没有⼤规模标记数据，采⽤⽆监督⽅法学习</li>
<li>从词语的共现中发掘翻译关系</li>
</ul>
</li>
<li>机器翻译的自动评价<ul>
<li>评价以何为标准？<ul>
<li>⼈⼯翻译的结果作为参考译⽂</li>
<li>使⽤多个参考译⽂增强评价结果的鲁棒性</li>
</ul>
</li>
<li>Word Error Rate (WER)：编辑距离(insertion, deletion, substitution)<ul>
<li>$ WER= \dfrac{I+D+S}{N} $ </li>
<li>对流畅性把握较好</li>
<li>对充分性把握较差</li>
</ul>
</li>
<li>Position-Independent WER (PER)<ul>
<li>WER对顺序有很强的敏感性，但没有考虑可能发⽣的整体顺序偏移</li>
<li>PER：忽略顺序，只考虑单词的匹配(unigram matching)</li>
</ul>
</li>
<li>BLEU<ul>
<li>Unigram Precision of a candidate translation: $\dfrac{C}{N}$<ul>
<li>N is number of words in the candidate,</li>
<li>C is the number of words in the candidate which are in at least one reference translation</li>
<li>存在问题</li>
</ul>
</li>
<li>⽤“Clipping”来进⾏修正：同一单词的count不能超过译文中该单词总数</li>
<li>长度惩罚因子</li>
</ul>
</li>
</ul>
</li>
<li><strong>总结统计机器翻译</strong><ul>
<li>可以⼀定程度上从数据中⾃动挖掘翻译知识</li>
<li>流程相对复杂，其中各个部分都不断被改进和优化</li>
<li>翻译性能遇到瓶颈，难以⼤幅度提升</li>
</ul>
</li>
</ul>
<h3 id="神经网络机器翻译"><a href="#神经网络机器翻译" class="headerlink" title="神经网络机器翻译"></a>神经网络机器翻译</h3><ul>
<li>从单词序列到单词序列的翻译⽅式<ul>
<li>简单直接的把句⼦看做单词序列</li>
<li>不再依赖⼤量从语料库中学习得到的有噪⾳规则（不需要建模规则的组合关系）</li>
</ul>
</li>
<li>神经网络的引⼊从统计稀疏性和建模两个⽅⾯提升了机器翻译系统</li>
<li>神经网络机器翻译是⼀种能够更加充分发挥机器⻓处的⾃动翻译⽅法</li>
</ul>
<h2 id="往年考题-5"><a href="#往年考题-5" class="headerlink" title="往年考题"></a>往年考题</h2><ul>
<li><p>建模题</p>
<ul>
<li><p>给定任意多个字，拼成一个合理的句子，如何用语言模型描述和解决，概率如何估计？</p>
</li>
<li><p>机器已经可以作对联，作诗，甚至写新闻稿，为机器作对联任务进行建模，给出可行的解决方案。</p>
</li>
<li><p>设计一个输入法（音字转换）方案，从建模、参数学习、解码这几个方面阐述你的方案</p>
<ul>
<li><p>比如<strong>语音识别</strong>，给你一段音频数据，需要识别出该音频数据对应的文字。这里音频数据就是观测变量，文字就是隐藏变量。我们知道，对单个文字而言，虽然在不同语境下有轻微变音，但大致发音是有统计规律的。另一方面，当我们说出一句话时，文字与文字之间也是有一些转移规律的。比如，当我们说出“比”这个字时，下一个大概率的字一般是“如”“较”等。虽然文字千千万，但文字与文字之间的转移却是有章可循的。有了文字的发音特征，以及文字与文字之间的转移规律，那么从一段音频中推测出对应的文字也就可以一试了。插一句，在当前深度学习一统江湖的时代，已经很少有人还在用HMM做语音识别了。</p>
<p>除了语音识别，你可能已经想到了另一个与之相近的例子了，<strong>输入法</strong>。就拿中文拼音输入法来说，给你一段从键盘输入的字符，你需要从中推测出用户输入的文字是什么，如果有多种可能的文字，你甚至需要给出每段候选文字的概率是多少。这里输入字符序列就是观测变量，要推断的输入文字就是隐藏变量。我们知道，对单个文字而言，与之对应的字符输入序列是有统计规律的。比如，我要打“张”这个字，一般可能的输入是“zh”、“zhang”、“zhagn”等。另一方面，和语音识别的例子一样，文字与文字之间也是有一些转移规律的。利用单个文字的输入统计规律、以及文字与文字之间的转移规律这两方面的信息，从一段字符序列推断对应的输入文字也不是什么难事了。对HMM而言，一般观测序列越长，推断越准。比如，我想输入“从一段字符序列推断对应的输入文字”这句话，当我输入“cong”时，输入法给我的候选字很多，</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>隐马尔科夫模型和维特比算法</p>
<p>4.计算题<u>隐马尔科夫模型</u>，viterbi算法</p>
<p>4.<u>隐马尔科夫模型</u>，给出了状态间的转移概率。 (1)计算当前序列X = abbbaba 存在的概率。 (2)给出观察序列对应的隐藏状态序列，计算生成X的最优状态序列</p>
</li>
<li><p>PCFG内向算法、最优句法分析树</p>
<p>给一个<u>PCFG</u> 利用内向算法 计算句子概率</p>
<p>2.给一个<u>PCFG</u> 画出最优句法分析树。 从语义角度看是否合理？</p>
</li>
<li><p>朴素贝叶斯法则</p>
<p>文本分类和机器翻译中如何引入<u>朴素贝叶斯法则</u>的，其背后的<u>基本思想和目的</u>分别是什么？结合符号和公式说明</p>
</li>
<li><p>文本表示</p>
<p>5.(1)写出<u>TF-IDF</u> 计算公式  (2)简述word embedding 比 one-hot 表示的优势</p>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>复习笔记</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>【复习笔记】神经网络及其应用</title>
    <url>/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<h2 id="一、生物神经系统与人工神经网络"><a href="#一、生物神经系统与人工神经网络" class="headerlink" title="一、生物神经系统与人工神经网络"></a>一、生物神经系统与人工神经网络</h2><h3 id="生物神经系统的结构"><a href="#生物神经系统的结构" class="headerlink" title="生物神经系统的结构"></a>生物神经系统的结构<a id="more"></a></h3><ul>
<li><p>什么是意识？</p>
<ul>
<li>意识是人脑对大脑内外表象的觉察</li>
<li>人的头脑对于客观物质世界的反映，也是感觉、思维等各种心理过程的总和</li>
<li>我：意识是一种信息</li>
</ul>
</li>
<li><p>人工神经网络</p>
<ul>
<li><p>能从输入数据中学习“信息”</p>
</li>
<li><p>信息以某种方式存储在神经网络这个“黑匣子”中</p>
</li>
<li><p>人工神经网络能利用学到的“信息”完成很多任务</p>
<p><u>人工神经网络在“信息”意义上已经具有了某种“意识”</u></p>
</li>
</ul>
</li>
<li><p>意识是如何产生的？</p>
<ul>
<li>二元论：心理和身体是两种不同的物质，他们独立存在——违背了能量守恒定律</li>
<li>一元论：宇宙只是由一种物质构成<ul>
<li>唯物主义：所有存在的物质都是物质的、有形的</li>
<li>唯心主义：没有心理去感知，物理世界将不会存在</li>
<li>同一性观点：心理过程和某些大脑活动过程是一样的，只不过用不同的术语来描述</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="人工神经网络简介"><a href="#人工神经网络简介" class="headerlink" title="人工神经网络简介"></a>人工神经网络简介</h3><ul>
<li>神经网络：模拟人类大脑的模型<ul>
<li>大规模并行分布处理器，由一些简单的处理单元（神经元）组成，能够保存经验知识，并且能够利用这些经验知识完成一些任务</li>
<li>通过“学习”来从环境中积累知识，知识被保存在神经元之间的连接上</li>
</ul>
</li>
<li><strong>有两个方面类似于人脑</strong>：<ol>
<li>知识由网络通过学习过程得到；</li>
<li>神经元间的互联，用来存储知识的已知的联合权值</li>
</ol>
</li>
<li><strong>神经网络的特点</strong><ul>
<li>面向神经元和联结性</li>
<li>信息的分布表示<ul>
<li>并行、分布处理结构</li>
<li>一个处理单元的输出可以被任意分枝，且大小不变</li>
<li>记忆效应</li>
</ul>
</li>
<li>运算的全局并行和局部操作<ul>
<li>处理单元完全的局部操作</li>
<li>集体效应</li>
</ul>
</li>
<li>处理的非线性性<ul>
<li>可以模拟任意的数学模型</li>
</ul>
</li>
<li>自组织性</li>
</ul>
</li>
<li><strong>神经网络的能力</strong><ul>
<li>学习能力<ul>
<li>学习从环境中获取的信息</li>
</ul>
</li>
<li>泛化能力<ul>
<li>对从未学习过的输入产生可信的输出</li>
</ul>
</li>
<li>容错能力<ul>
<li>系统受到局部损伤时仍然能够正常工作</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="二、人工神经元"><a href="#二、人工神经元" class="headerlink" title="二、人工神经元"></a>二、人工神经元</h2><h3 id="MP神经元"><a href="#MP神经元" class="headerlink" title="MP神经元"></a>MP神经元</h3><ul>
<li><p>结构（f为阶跃函数）</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-1.png" alt="2-1" style="zoom:60%;"></p>
</li>
<li><p>特征</p>
<ul>
<li>二值网络（binary：fire-1, or not fire-0）：自变量及其函数的值、向量分量的值只取0和1函数、向量。</li>
<li>由有方向的、带权重的路径联系</li>
<li>权为正：刺激；权为负：抑制</li>
<li>全或无：每个神经元有一个固定的阈值，如果输入大于阈值，就fires</li>
<li>绝对抑制：阈值被设为使得抑制为绝对的，即，非0的抑制输入将阻止该神经元兴奋。</li>
<li>花费一个时间单位使得信号通过一个连接</li>
<li>任意命题逻辑函数都可由一两层的MP模型计算</li>
<li><font color="red">所有的命题逻辑函数都可以用MP AND逻辑门、MP OR逻辑门、MP NOT逻辑门予以表达和实现（*重点）</font> </li>
<li>MP模型具有神经计算模型一般和普遍的特性，可表达一般人工神经网络的赋权联结和相对抑制</li>
</ul>
</li>
<li><p>与</p>
<ul>
<li>W = 1，threshold = -2</li>
</ul>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-2.png" alt="2-2" style="zoom:50%;"></p>
</li>
<li><p>或</p>
<ul>
<li><p>W = 1，threshold = -1</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-6.png" alt="2-6" style="zoom:40%;"></p>
</li>
</ul>
</li>
<li><p>非</p>
<ul>
<li><p>W=-1,threshold=0</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-7.png" alt="2-7" style="zoom: 50%;"></p>
</li>
</ul>
</li>
<li><p>与非</p>
<ul>
<li>W = 2, p = 1, threshold = 2</li>
</ul>
</li>
<li><p>异或</p>
<ul>
<li><p>X1 XOR x2 &lt;-&gt; (x1 AND NOT x2) OR (x2 AND NOT x1)</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-8.png" alt="2-8" style="zoom:50%;"></p>
</li>
<li><p>X1 XOR x2 &lt;-&gt; NOT (NOT x1 OR x2) OR NOT(x1 OR NOT x2)</p>
</li>
</ul>
</li>
</ul>
<h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><h4 id="最小二乘法（-重点）"><a href="#最小二乘法（-重点）" class="headerlink" title="最小二乘法（*重点）"></a><font color="red">最小二乘法（*重点）</font></h4><p>假设$h(x)=wx+b$，均方误差$ E= \frac{1}{n} \sum _{i=1}^{n} \left[ h(x_{i})-y_{i} \right] ^{2} $ ，问题就被化为求$ =argmin_{w,b}\frac{1}{n} \sum _{i=1}^{n}(wx_{i}+b-y_{i})^{2} $ 。令导数为0，解得：</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-3.png" alt="2-3" style="zoom:50%;"></p>
<p>神经元表示：</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-4.png" alt="2-4" style="zoom:67%;"></p>
<h3 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h3><p>线性分类需要更复杂的神经元来解决，两分类问题可用一个神经元来表示，但不同于线性回归，神经元中对数据加权求和后需要在进行一步非线性操作，即用单位阶跃函数对求和结果进行映射。输出𝑦也不再是一个常数，两分类中用0,1表示不同的类别</p>
<h3 id="感知器神经元"><a href="#感知器神经元" class="headerlink" title="感知器神经元"></a>感知器神经元</h3><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><ul>
<li><p>功能：将神经元的加权输入线性/非线性转换成一个输出的激活函数</p>
</li>
<li><p>阶跃式激活函数</p>
<ul>
<li>Identity function (同一函数)</li>
<li>Threshold function (阈值函数)<ul>
<li>f(x)=1, if x&gt;=阈值</li>
<li>f(x)=-1, if x&lt;阈值</li>
</ul>
</li>
<li>Piecewise linear function(分段线性函数)<ul>
<li>f(x)=1, if x&gt;=阈值</li>
<li>f(x)=x, if -阈值&lt;x&lt;阈值</li>
<li>f(x)=-1, if x&lt;=-阈值</li>
</ul>
</li>
</ul>
</li>
<li><p>非线性激活函数：保持在上限和下限之间的非线性的连续函数<br>（1）非线性：函数的输出随输入做非线性的变化，<br>（2）连续函数：函数中没有顶点或者中断，可以从始至终进行微分</p>
<p>$ Sigmoid (z)= \frac{1}{1+e^{-z}} $ </p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-5.png" alt="2-5" style="zoom:50%;"></p>
</li>
</ul>
<h3 id="FT神经元"><a href="#FT神经元" class="headerlink" title="FT神经元"></a>FT神经元</h3><p>启发来源：神经元A接收到来自神经元B的刺激信号后的响应，不仅取决于神经元B的轴突传递强度，还依赖与神经元A的树突浓度，这与神经元A的记忆单元有关。</p>
<h2 id="三、人工神经元的学习"><a href="#三、人工神经元的学习" class="headerlink" title="三、人工神经元的学习"></a>三、人工神经元的学习</h2><h3 id="生物神经元的学习"><a href="#生物神经元的学习" class="headerlink" title="生物神经元的学习"></a>生物神经元的学习</h3><p>学习的两种具体类型</p>
<ul>
<li>经典条件作用<ul>
<li>由一个刺激或事件预示另一个刺激或事件的到来</li>
<li>巴浦洛夫的狗——巴浦洛夫条件作用</li>
<li>刺激泛化：反应自动扩展到与刺激相似的新刺激上</li>
</ul>
</li>
<li>操作性条件作用<ul>
<li>操作：自发产生的行为，可按照他作用于环境并使环境发生了可观察的结果来描述其特点</li>
<li>效果律：带来满意结果的反应出现的概率会越来越大，带来不满意结果的反应出现的概率越来越小</li>
<li>桑代克的迷笼（猫）——指向成功的特定冲动则因愉快的结果而保留下来 </li>
</ul>
</li>
</ul>
<h3 id="感知器学习方法"><a href="#感知器学习方法" class="headerlink" title="感知器学习方法"></a>感知器学习方法</h3><h4 id="随机学习"><a href="#随机学习" class="headerlink" title="随机学习"></a>随机学习</h4><ul>
<li><p>随机更新权矩阵𝑾和偏置矩阵𝑩，然后慢慢对赋值进行修正</p>
</li>
<li><p>尽管随机学习效率很低，但其思想简单，实现容易，且具有能找到全局最优解等特点，在对其搜索方法进行改进后，也能够具有一定的实际应用意义</p>
</li>
</ul>
<h4 id="Hebbian学习"><a href="#Hebbian学习" class="headerlink" title="Hebbian学习"></a>Hebbian学习</h4><ul>
<li>一个网络里的信息被储存在神经元之间的权值中；</li>
<li>假定两个神经元之间<u>的权值变换是与它们神经元的输出成比例</u>的；</li>
<li>假定随着通过重复和激励一组弱连接神经元而发生学习时，它们之间权值的强度和模式经历逐步增加改变，最终导致神经元形成强连接集合形式。</li>
<li>假设两个神经元有x和y的输出，如果x激励y，它们之间的连接强度就会增加。两个神经元之间的权值改变Δw与x和y成比例：$\Delta w= \beta x \cdot y$<ul>
<li>比例系数𝛽叫做“学习率”，决定学习发生的速度。𝛽越大，权值改变得越快。</li>
</ul>
</li>
</ul>
<h4 id="实例1：作为分类器的有监督学习的感知器（PPT-29-36，-会计算"><a href="#实例1：作为分类器的有监督学习的感知器（PPT-29-36，-会计算" class="headerlink" title="实例1：作为分类器的有监督学习的感知器（PPT 29-36， *会计算)"></a><font color="red">实例1：作为分类器的有监督学习的感知器（PPT 29-36， *会计算)</font></h4><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-9.png" alt="2-9" style="zoom:45%;"></p>
<h4 id="实例2：线性神经元预报器-PPT-38-42，-会计算"><a href="#实例2：线性神经元预报器-PPT-38-42，-会计算" class="headerlink" title="实例2：线性神经元预报器 (PPT 38-42， *会计算)"></a><font color="red">实例2：线性神经元预报器 (PPT 38-42， *会计算)</font></h4><p>上述方法尝试利用一个样本来更新w权重，为何有效？</p>
<ul>
<li>其本质是利用单个样本在平方差损失函数$ L(t,y)= \frac{1}{2}(y-t)^{2} $ 上的梯度信息，进行更新。</li>
</ul>
<h4 id="梯度下降法学习"><a href="#梯度下降法学习" class="headerlink" title="梯度下降法学习"></a><font color="red">梯度下降法学习</font></h4><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/3-1.png" alt="3-1" style="zoom: 50%;"></p>
<ul>
<li><p><strong>学习率调整方法：</strong></p>
<ul>
<li><p>（1）学习率衰减</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/3-2.png" alt="3-2" style="zoom: 60%;"></p>
</li>
<li><p>（2）学习率预热</p>
<p>为了让初始阶段的学习更加稳定：在初始的几轮，采用较小的学习率。梯度下降到一定程度之后，再恢复到初始设置的学习率</p>
</li>
<li><p>（3）周期学习率</p>
<p>循环学习率（Cyclic）是让学习率在一个区间内周期性的增大和缩小</p>
</li>
<li><p>（4）自适应调整学习率：AdaGrad，RMSprop，AdaDelta</p>
</li>
</ul>
</li>
<li><p><strong>梯度下降面临的困难</strong></p>
<ul>
<li>很难选择一个合适学习率</li>
<li>对于所有的参数，均使用相同的学习率。不同参数的梯度大小有差异。</li>
<li>在非凸函数的优化过程中，我们往往希望模型能够跳过那些局部极值点，去找一个更好的极值。实际问题中，鞍点问题很难解决。</li>
</ul>
</li>
<li><p>感知机学习算法</p>
<ul>
<li>算法的收敛性：证明经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。</li>
<li>感知机算法存在许多解，既依赖于初值，也依赖迭代过程中误分类点的选择顺序。</li>
<li>为得到唯一分离超平面，需要增加约束。</li>
<li>线性不可分数据集，迭代震荡。</li>
</ul>
</li>
</ul>
<h3 id="ADALINE-Adaptive-Linear-Element"><a href="#ADALINE-Adaptive-Linear-Element" class="headerlink" title="ADALINE: Adaptive Linear Element"></a>ADALINE: Adaptive Linear Element</h3><ul>
<li>结构<ul>
<li>与感知器网络结构非常相似</li>
<li>激活函数不同：<strong>线性函数</strong></li>
</ul>
</li>
<li>ADALINE的学习：LMS（Least-Mean-Square learning algorithm）</li>
<li><strong>ADALINE和感知器的比较</strong><ul>
<li>神经元模型不同<ul>
<li>感知器：非线性模型，只能输出两种可能的值，其激活函数是<strong>阈值函数</strong></li>
<li>ADALINE：线性模型，输出可以取任意值，其激活函数是线性函数</li>
</ul>
</li>
<li>功能不同<ul>
<li>Perceptron:感知器只能做简单的分类</li>
<li>LMS：还可以实现拟合或逼近</li>
</ul>
</li>
<li><strong>分类性能不同</strong><ul>
<li>LMS算法得到的分类边界往往处于两类模式的正中间</li>
<li>感知器学习算法在刚刚能正确分类的位置就停下来，使分类边界离一些模式距离过近，使系统对误差更敏感</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="四、神经元的连接"><a href="#四、神经元的连接" class="headerlink" title="四、神经元的连接"></a>四、神经元的连接</h2><h3 id="神经元的联结综述"><a href="#神经元的联结综述" class="headerlink" title="神经元的联结综述"></a>神经元的联结综述</h3><ul>
<li>神经元的连接方式<ul>
<li>层级结构和互联网型结构：通常同一层不具有连接、两个相邻层完全连接(每一层的每一个神经元到另一层的每个神经元)。</li>
<li>层（级）内联结模式：用来加强和完成层内神经元之间的竞争</li>
<li>互联网型连接模式：最典型的就是马可夫链和Hopfield网络(HN)。</li>
</ul>
</li>
<li>神经元计算方式<ul>
<li>循环联结模式</li>
<li>卷积计算模式</li>
</ul>
</li>
</ul>
<h3 id="宽度扩展：以多输出的单层感知器为例"><a href="#宽度扩展：以多输出的单层感知器为例" class="headerlink" title="宽度扩展：以多输出的单层感知器为例"></a>宽度扩展：以多输出的单层感知器为例</h3><p>算法思想：将单输出感知器的处理逐个地用于多输出感知器输出层的每一个神经元的处理。</p>
<ul>
<li><p>离散多输出感知器训练算法</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/3-3.png" alt="3-3" style="zoom:50%;"></p>
</li>
<li><p>连续多输出感知器训练算法</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/3-4.png" alt="3-4" style="zoom:50%;"></p>
</li>
</ul>
<h3 id="深度扩展：多层感知器"><a href="#深度扩展：多层感知器" class="headerlink" title="深度扩展：多层感知器"></a>深度扩展：多层感知器</h3><p>在输入层和输出层之间至少有一层（称之为隐藏层，hidden layer）</p>
<ul>
<li>和单层神经网络相比能解决更加复杂的问题</li>
<li>训练更加困难</li>
<li>有些情况下能解决单层不能解决的问题</li>
</ul>
<p>代表：多层感知器、CNN</p>
<ul>
<li><font color="red">手工搭建并设置单隐藏层网络模型：拟合sin函数 (*计算,PPT53-60)</font>

<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/4-1.png" alt="4-1"></p>
</li>
</ul>
<h3 id="其他连接方式"><a href="#其他连接方式" class="headerlink" title="其他连接方式"></a>其他连接方式</h3><ul>
<li>卷积神经网络（Convolutional Net）</li>
<li>竞争神经网络（Competitive Net ）</li>
<li>循环神经网络（Recurrent Net ）</li>
<li>其他类型网络</li>
</ul>
<h3 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h3><ul>
<li><p>阈值函数为什么不适合作为激活函数？</p>
<ul>
<li>理由，大多数激活函数都需要满足以下条件（当然也存在不完全满足的，比如relu函数）<ol>
<li>神经网络的激活函数需要满足的条件之一就是函数是<strong>连续可导</strong>的，这样才能使用梯度下降法更新网络参数，这里该函数在x=0处不是连续可导的</li>
<li>神经网络激活函数还需要满足函数是<strong>单调增/减</strong>的，这里也不满足</li>
</ol>
</li>
<li><u>不适合，因为它不连续、不光滑，导致在x=0处不可微，并且在其他地方的导数是零，无法使用基于梯度的参数优化方法。</u></li>
</ul>
</li>
<li><p>能否用一个神经元拟合二次曲线吗？如果能，请给出实例。如果不能，请说明至少需要多少个神经元才能拟合二次曲<br>线。</p>
<ul>
<li><p>能，考虑二次函数的一般形式 $y=ax^2 + bx + c$，可设计如下单个神经元结构就能拟合二次曲线。</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/4-2.png" alt="4-2" style="zoom: 33%;"></p>
</li>
</ul>
</li>
</ul>
<h2 id="五、多层感知器"><a href="#五、多层感知器" class="headerlink" title="五、多层感知器"></a>五、多层感知器</h2><h3 id="BP网络的训练—基本算法"><a href="#BP网络的训练—基本算法" class="headerlink" title="BP网络的训练—基本算法"></a>BP网络的训练—基本算法</h3><ul>
<li><font color="red">弱点：训练速度非常慢、局部极小点的逃离问题、算法不一定收敛</font>
</li>
<li><font color="red">优点：广泛的适应性和有效性</font>

<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/5-1.png" alt="5-1" style="zoom:80%;"></p>
</li>
</ul>
<font color="red">计算例子（PPT 21-24）</font>



<h3 id="BP算法的改进"><a href="#BP算法的改进" class="headerlink" title="BP算法的改进"></a>BP算法的改进</h3><ul>
<li><p>BP针对单个样例进行更新</p>
<ul>
<li>不同样例的更新的效果可能“抵消”。BP网络接受样本的顺序对训练结果有较大影响。它更“偏爱”较后出现的样本。给样本安排一个适当的顺序，是非常困难的。</li>
<li><strong>用多个样本的“总效果”修改权重</strong>$ \Delta w_{ij}^{(k)}= \sum _{p} \Delta w_{ij}^{(k)} $ </li>
</ul>
</li>
<li><p>消除样本顺序影响的BP算法:</p>
<ul>
<li>较好地解决了因样本的顺序引起的精度问题和训练的抖动问题</li>
<li>收敛速度：比较慢</li>
<li>偏移量：给每一个神经元增加一个偏移量来加快收敛速度</li>
<li>冲量：<u>联接权的本次修改要考虑上次修改的影响，以减少抖动问题</u></li>
</ul>
</li>
</ul>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/5-2.png" alt="5-2" style="zoom: 50%;"></p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/5-3.png" alt="5-3" style="zoom:55%;"></p>
<ul>
<li>设置冲量的BP算法</li>
</ul>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/5-4.png" alt="5-4" style="zoom:60%;"></p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/5-5.png" alt="5-5" style="zoom:94%;"></p>
<h3 id="算法的理论基础（-掌握）"><a href="#算法的理论基础（-掌握）" class="headerlink" title="算法的理论基础（*掌握）"></a><font color="red">算法的理论基础（*掌握）</font></h3><ul>
<li><font color="red">实例：拟合正弦波的第一个1/4周期波形(PPT 42-63)</font>
</li>
<li><p>局部极小点问题</p>
<ul>
<li>修改初值：从多个初始点开始进行搜索。</li>
<li>模拟退火：以一定概率接受比当前结果更差的解。</li>
<li><strong>随机梯度下降：计算梯度值时，加入了随机因素，极小值点处计算的梯度不为0，有机会跳出局部极小。</strong></li>
<li>遗传算法</li>
</ul>
</li>
<li>几个问题的讨论<ul>
<li>收敛速度问题</li>
<li>网络瘫痪问题<ul>
<li>在训练中，权可能变得很大，这会使神经元的网络输入变得很大，从而又使得其激活函数的导函数在此点上的取值很小。根据相应式子，此时的训练步长会变得非常小，进而将导致训练速度降得非常低，最终导致网络停止收敛</li>
</ul>
</li>
<li>稳定性问题<ul>
<li>用修改量的综合实施权的修改</li>
<li>连续变化的环境，它将变成无效的</li>
</ul>
</li>
<li>步长问题<ul>
<li>BP网络的收敛是基于无穷小的权修改量</li>
<li>步长太小，收敛就非常慢</li>
<li>步长太大，可能会导致网络的瘫痪和不稳定</li>
<li>自适应步长，使得权修改量能随着网络的训练而不断变化。[1988年，Wasserman]<ul>
<li>AdaGrad方法：在模型训练初期，参数变化快（学习率大），而在模型训练后期，参数变化慢且梯度更新值小。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="作业-1"><a href="#作业-1" class="headerlink" title="作业"></a>作业</h3><ul>
<li>进一步提高神经网络识别率的方法：</li>
</ul>
<ol>
<li>增加隐藏层数；</li>
<li>增加隐藏层神经元个数；</li>
<li>增加训练轮数；</li>
<li>获取更多训练数据；</li>
<li>使用卷积神经网络等其他网络结构</li>
</ol>
<h2 id="六、正则化、归一化和预训练"><a href="#六、正则化、归一化和预训练" class="headerlink" title="六、正则化、归一化和预训练"></a>六、正则化、归一化和预训练</h2><h3 id="数据、模型与特征工程"><a href="#数据、模型与特征工程" class="headerlink" title="数据、模型与特征工程"></a>数据、模型与特征工程</h3><ul>
<li>常见的数据问题<ul>
<li>不同维度差异过大（数据中心偏置）</li>
<li>正负例样本不均衡</li>
</ul>
</li>
<li>模型问题<ul>
<li>过/欠拟合</li>
<li>梯度消失/梯度爆炸</li>
<li>模型过大，难以重新训练等</li>
</ul>
</li>
<li>解决上述问题常用的手段有：<br>1、正则化，规一化<br>2、预训练和迁移学习<br>3、特征预处理（特征工程）</li>
</ul>
<h3 id="正则化与归一化"><a href="#正则化与归一化" class="headerlink" title="正则化与归一化"></a>正则化与归一化</h3><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ul>
<li><p>过拟合和欠拟合</p>
<ul>
<li>过拟合：模型过于复杂，参数过多或训练数据过少，噪声过多。</li>
<li>欠拟合：模型比较简单，特征维度过少。</li>
</ul>
</li>
<li><p>偏差和方差：定量分析</p>
<ul>
<li><p>偏差：衡量模型预测值和实际值之间的偏离关系，即模型在样本上拟合得好不好。</p>
</li>
<li><p>方差：描述模型在整体数据上表现的稳定情况，在训练集和验证集/测试集上表现是否一致</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/6-1.png" alt="6-1" style="zoom:60%;"></p>
</li>
</ul>
</li>
<li><p>正则化：<strong>通过限制模型的复杂度，避免过拟合，提高泛化能</strong></p>
</li>
<li><font color="red">不同的正则化方法</font>


</li>
</ul>
<p>1、<font color="red">L1和L2正则化（*掌握）</font></p>
<ul>
<li><p>l1正则化：$ argmin_{ \theta } \frac{1}{N}( \sum _{i}^{N} L(y_{i},f(x_{i}, \theta ))+ \lambda | \theta |) $ </p>
<ul>
<li>目的：<strong>使𝜃更容易为0，整体权重矩阵更为稀疏，抑制过拟合。</strong></li>
</ul>
</li>
<li><p>l2正则化：$ argmin_{ \theta } \frac{1}{N}( \sum _{i}^{N}L(y_{i},f(x_{i}, \theta ))+ \lambda \theta ^{2}) $ </p>
</li>
<li><p>目的：<strong>使得权重变得更小</strong></p>
</li>
</ul>
<ul>
<li>参数$\lambda$的意义<ul>
<li>$\lambda$：正则化项系数，用来控制正则化项的“力度”，<u>平衡损失函数和正则化项之间的关系</u>。<ul>
<li>若ƛ项越大，正则化项在损失函数中所占比例更大，因此损失函数更倾向于优化正则化项，对原始的损失函数V产生较小影响，易导致模型简单，发生欠拟合；</li>
</ul>
</li>
<li>若ƛ项越小，相比于原始的损失函数，正则化项在损失函数中所占比例很小，几乎<br>不起作用，容易发生过拟合问题。</li>
</ul>
</li>
</ul>
<p>  2、权重衰减法</p>
<ul>
<li><p>每次参数更新时，都先对参数进行一定衰减：$ \theta :=(1-w) \theta - \alpha d \theta $ （其中w为权重衰减系数）</p>
<p>3、丢弃法</p>
</li>
<li><p>在训练时，以概率p随机丢弃部分神经元。</p>
</li>
<li><p>1.相当于取平均的作用，取每次丢弃后子网络的平均结果。</p>
<ul>
<li>2.降低神经元之间的敏感度，增加整体鲁棒性。</li>
</ul>
<p>4、提前停止</p>
</li>
<li><p>思路：提早结束训练</p>
</li>
<li><p>验证集错误率基本不下降时或有反增趋势时，可以提前停止训练。</p>
<p>5、数据增强</p>
</li>
<li><p>在不实质增加数据的情况下，对当前数据执行一些操作达到数据增加的效果。</p>
<ul>
<li>图像数据：翻转、旋转、镜像、裁剪、增加高斯白噪声等</li>
<li>文本数据：同义词替换、随机插入、随机交换、随机删除等</li>
</ul>
</li>
</ul>
<h4 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h4><ul>
<li><p>为什么要归一化？</p>
<ul>
<li>提升模型的收敛速度<ul>
<li>所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛</li>
</ul>
</li>
</ul>
</li>
<li><font color="red">最常用的归一化方法</font>

<ul>
<li><p>Min-max归一化：将结果映射到[0,1]之间$ \widehat{x}= \frac{x- \min (x)}{ \max (x)- \min (x)} $ </p>
</li>
<li><p>Z-score归一化（标准归一化）：$ \widehat{x}= \frac{x- \mu }{ \sigma } $ </p>
</li>
<li><p><strong>批归一化（Batch Normalization，BN）</strong></p>
<ul>
<li>思路：逐层归一化方法，对神经网络中任意的中间层进行归一化操作。使得净输入的分布一致（例如正态分布），一般应用在激活函数之前。</li>
<li>加快了模型的收敛速度，而且更重要的是在一定程度缓解了深层网络中“梯度弥散”的问题，从而使得训练深层网络模型更加容易和稳定。</li>
<li>BN的输出服从什么分布？<ul>
<li>标准正态分布，N(0,1)：均值为0，方差为1.</li>
</ul>
</li>
<li><strong>Batch normalization为什么归一化后还有放缩（γ ）和平移（β ）？</strong><ul>
<li>平移和放缩是变换的反操作。通过反操作将标准化后的数据尽可能恢复。如果批标准化没有发挥作用，通过放缩和平移可以抵消一部分标准化的作用。防止网络表达能力下降，恢复数据。弥补归一化后模型损失的表征能力。</li>
<li>规范化操作让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是我们通过变换操作改变了原有数据的信息表达，使得底层网络学习到的参数信息丢失。另一方面，通过让每一层的输入分布均值为0，方差为1，会使得输入在经过sigmoid或tanh激活函数时，容易陷入非线性激活函数的线性区域。放缩和平移这两个参数对规范化后的数据进行线性变换，是为了<u>恢复数据本身的表达能力，保留原始输入特征的分布信息。</u></li>
</ul>
</li>
<li>算法</li>
</ul>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/6-2.png" alt="6-2" style="zoom: 60%;"></p>
</li>
<li><p>层归一化（Layer Normalization，LN）</p>
<ul>
<li>思路：对中间层的所有神经元进行归一化</li>
</ul>
</li>
<li><p>实例归一化（Instance Normalization，IN）</p>
<ul>
<li>主要用于依赖于某个图像实例的任务。</li>
</ul>
</li>
<li><p>N：batch维度，C：特征通道维度，H、W：特征图高和宽维度。</p>
<ul>
<li>对每个样本的H和W的数据求均值和标准化，保留N、C维度。</li>
</ul>
</li>
<li><p>组归一化（Group Normalization，GN）</p>
<ul>
<li>把特征通道分为G组，每组有C/G个特征通道，在组内归一</li>
</ul>
</li>
<li><p>可转换归一化（Switchable Normalization，SN）</p>
<ul>
<li>将批归一化、层归一化、实例归一化结合起来的方法，使网络自适应学习如何组合起来的权重。</li>
</ul>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/6-3.png" alt="6-3" style="zoom:80%;"></p>
</li>
</ul>
</li>
</ul>
<h3 id="初始化、预训练和迁移学习"><a href="#初始化、预训练和迁移学习" class="headerlink" title="初始化、预训练和迁移学习"></a>初始化、预训练和迁移学习</h3><h4 id="网络参数初始化"><a href="#网络参数初始化" class="headerlink" title="网络参数初始化"></a>网络参数初始化</h4><ul>
<li><p>不同的初始化参数，计算得到的“全局最小”差距很大。好的初始化值能够帮助网络更快地计算得到最优值，更容易收敛到目标函数。</p>
</li>
<li><p>目前没有发现一种初始化方式可以适用于任何网络结构。初始化需要避免“对称权重”现象（唯一确知的特性）</p>
</li>
<li><p>权重矩阵的初始化</p>
<ul>
<li>均匀分布初始化<ul>
<li>主要思想：在区间(-r,r)的均匀分布U(-r,r)中随机选取所有网络权值</li>
<li>如何确定区间范围？尽量保持梯度能够在多层网络中传播</li>
<li>规则1：对于任意网络权值$w_{ij}$，从如下分布中随机选取初始值：$ w_{ij} \sim U(- \frac{1}{ \sqrt{n_{i}}}, \frac{1}{ \sqrt{n_{i}}}) $ (其中$n_i$表示第i层的神经元数量。)</li>
<li>规则2：Xavier 初始化</li>
</ul>
</li>
<li>高斯分布初始化<ul>
<li>主要思想：在固定均值和固定方差的高斯分布中随机选取所有网络权值</li>
<li>缺陷：深层模型会非常难以收敛</li>
</ul>
</li>
<li>稀疏初始化<ul>
<li>主要思想：稀疏初始化降低连接数量，使得初始化的数值不会太小</li>
</ul>
</li>
<li>正交初始化<ul>
<li>主要思想：正交初始化可以避免训练开始时就出现梯度消失或梯度爆炸现象</li>
</ul>
</li>
</ul>
</li>
<li><p>偏置矩阵初始化</p>
<ul>
<li>偏置矩阵通常不需要考虑破坏对称性的问题，通常我们可以把偏置矩阵初始化为<u>全0矩阵</u>；</li>
<li>除了一些例外情况：<ul>
<li>偏置作为输出单元，初始化偏置以获得正确的输出边缘的统计是有利的；</li>
<li>需要选择偏置以避免初始化引起的太大饱和</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>不合理的初始化会导致梯度消失或爆炸现象</strong><br>（1）大的梯度会使网络十分不稳定，会导致权重成为一个特别大的值，最终导致溢出而无法学习<br>（2）小的梯度传过多层网络，达到靠近输入的隐藏层后会越来越小，导致隐藏层无法正常地进行学习。</p>
</li>
</ul>
<h4 id="网络预训练和迁移学习"><a href="#网络预训练和迁移学习" class="headerlink" title="网络预训练和迁移学习"></a>网络预训练和迁移学习</h4><ul>
<li>网络预训练：采用相同结构的，并且已经训练好的网络权值作为初始值，在当前任务上再次进行训练</li>
<li>为什么使用网络预训练？<ul>
<li>为了能够在更短时间内训练得到更好的网络性能</li>
<li>相似的任务之间，训练好的神经网络可以复用，通常作为特征提取器</li>
</ul>
</li>
<li>预训练方法<ul>
<li>无监督预训练<ul>
<li>玻尔兹曼机</li>
<li>自编码器</li>
</ul>
</li>
<li>有监督预训练：迁移学习<ul>
<li>主要思想：通过大量带标签的数据集，大幅减少网络收敛的训练时间。站在巨人的肩膀上，复用已经得到的研究成果</li>
<li>使用预训练模型的方式：<ul>
<li>（1）<strong>直接作为特征提取网络</strong>：即将网络直接用于数据的特征提取，将输出作为特征，根据任务目标进行后续的分类、回归等操作，不再对这些预训练层进行进一步的学习，可以看作预训练模型“冻结（frozen）”了；</li>
<li>（2）<strong>作为初始化模型进行微调</strong>：部分或全部使用这个模型作为初始化模型，根据手头的数据对这个模型进行再次训练，这个过程被称为“精调（fine-tune）”。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="七、神经动力学系统"><a href="#七、神经动力学系统" class="headerlink" title="七、神经动力学系统"></a>七、神经动力学系统</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul>
<li><p>神经动力学：研究神经系统随时间的变化过程和规律</p>
<ul>
<li><p>确定性神经动力学：神经网络有确定的行为。用一组非线性微分方程描述。</p>
</li>
<li><p>统计性神经动力学：神经网络受到噪声扰动，在数学上采用随机性的非线性微分方程来描述系统的行为，方程的解用概率表示</p>
</li>
</ul>
</li>
<li><p>动力学系统</p>
<ul>
<li>大型的非线性动力学系统的动力特性可用下面的微分方程表示： $\frac{d}{dt}V(t)=F(V(t)) $</li>
<li>如果一个非线性动力系统的向量函数$F(V(t))$不隐含地依赖于时间$t$，则此系统称为自治系统，否则不是自治的。</li>
</ul>
</li>
</ul>
<h3 id="离散Hopfield网络（重点）"><a href="#离散Hopfield网络（重点）" class="headerlink" title="离散Hopfield网络（重点）"></a><font color="red">离散Hopfield网络（重点）</font></h3><ul>
<li><p>离散Hopfield网络是单层全互连的，其表现形式有两种</p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/7-1.png" alt="7-1" style="zoom:60%;"></p>
<ul>
<li>神经元可取二值{0/1}或{−1/1}</li>
<li><strong>条件</strong>：神经元之间联接是对称的，即$ W_{ij}=W_{ji} $ ；神经元自身无联接,即$W_{ii} = 0$</li>
<li>每个神经元都将其输出通过突触权值传递给其它的神经元，同时每个神经元又都接收其它神经元传来的信息</li>
<li>对每个神经元来说，其输出信号经过其它神经元后又有可能反馈给自己，所以Hopfield网络是一种<strong>反馈神经网络</strong>。</li>
<li>输出计算：$ u_{i}(t)= \sum _{j=1,j\neq i}^{n}w_{ij}v_{j}(t)+b_{i} $ , $ v_{i}(t+1)=f(u_{i}(t)) $ <ul>
<li>其中的激励函数$f(\cdot)$可取阶跃函数或符号函数</li>
</ul>
</li>
</ul>
</li>
<li><p>运行规则：Hopfield网络的工作方式主要有两种形式</p>
<ul>
<li><p><font color="red"><strong>串行（异步）工作方式</strong></font>：在任一时刻$t$，只有某一神经元$i$（随机的或确定的选择）依上式变化，而其他神经元的状态不变。</p>
<blockquote>
<p>第一步：对网络进行初始化<br>第二步：从网络中随机选取一个神经元$i$<br>第三步：求出该神经元$i$的输入 $ u_{i}(t)= \sum _{j=1,j\neq i}^{n}w_{ij}v_{j}(t)+b_{i} $</p>
<p>第四步：求出该神经元$i$的输出$ v_{i}(t+1)=f(u_{i}(t)) $ ，此时网络中的其它神经元的输出保持不变</p>
<p>第五步：判断网络是否达到稳定状态，若达到稳定状态或满足给定条件，则结束；否则转到第二步继续运行。这里网络的稳定状态定义为：若网络从某一时刻以后，状态不再发生变化，则称网络处于稳定状态。</p>
</blockquote>
<ul>
<li><p>Hopfield网络“能量函数”（Lyapunov函数）的“能量”在网络运行过程中应不断地降低，<strong>最后达到稳定的平衡状态</strong></p>
<ul>
<li>能量函数：$ E=- \frac{1}{2} \sum _{i=1,i\neq j}^{n} \sum _{j=1,j\neq i}^{n}w_{ji}v_{j}v_{j}+ \sum _{j=1}^{n}b_{i}v_{i} $ </li>
<li><p>上式所定义的“能量函数”值应单调减小。所以在满足参数条件下，Hopfield网络状态是向着能量函数减小的方向演化。由于能量函数有界，所以系统必然会趋于稳定状态，该稳定状态即为Hopfield网络的输出</p>
</li>
<li><p>能量函数的变化曲线含有全局最小点和局部最小点。将这些极值点作为记忆状态，可将Hopfield网络用于<u>联想记忆</u>；将能量函数作为代价函数，全局最小点看成最优解，则Hopfield网络可用于<u>最优化计算</u></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>并行（同步）工作方式：在任一时刻t，部分神经元或全部神经元的状态同时改变。</p>
</li>
</ul>
</li>
</ul>
<h3 id="连续Hopfield网络"><a href="#连续Hopfield网络" class="headerlink" title="连续Hopfield网络"></a>连续Hopfield网络</h3><p>激励函数为连续函数</p>
<h3 id="联想记忆"><a href="#联想记忆" class="headerlink" title="联想记忆"></a>联想记忆</h3><ul>
<li><p>人工神经网络的联想就是指系统在给定一组刺激信号的作用下，该系统能联系出与之相对应的信号。联想是以记忆为前提的，即首先信息存储起来，再按某种方式或规则将相关信息取出。联想记忆的过程就是信息的存取过程。</p>
</li>
<li><p>所谓的联想记忆也称为<strong>基于内容的存取</strong>（Content-addressed memory），信息被分布于生物记忆的内容之中，而不是某个确定的地址。</p>
<ul>
<li>（1）信息的存贮是按内容存贮记忆的(content addressable memory CAM)，而传统的计算机是基于地址存贮的（Addressable Memory）即一组信息对应着一定的存储单元。<br>（2）信息的存贮是分布的，而不是集中的。</li>
</ul>
</li>
<li><p>联想记忆的分类：联想记忆可分为自联想与异联想。Hopfield网络属于自联想。</p>
<ul>
<li><strong>自联想</strong>记忆(Auto-AssociativeMemory)<br>自联想能将网络中输入模式映射到存贮在网络中不同模式中的一种。联想记忆网络不仅能将输入模式映射为自己所存贮的模式，而且还能对具有缺省/噪音的输入模式有一定的容错能力。<ul>
<li>设在学习过程中给联想记忆网络存入$M$个样本,若给联想记忆网络加以输入$ X^{ \prime }=X^{m}+V $ ，其中$X^{m}$是$M$个学习样本之一，$V$是偏差项（可代表噪声、缺损与畸变等），通过自联想联想记忆网络的输出为$X^{m}$，即使之复原（比如，破损照片→完整照片）</li>
</ul>
</li>
<li><strong>异联想</strong>网络：在受到具有一定噪音的输入模式激发时，能通过状态的演化联想到原来样本的模式对(如从破损照片得到某人的姓名)</li>
</ul>
</li>
<li><p>联想记忆的工作过程</p>
<ul>
<li><p>（1）<strong>记忆阶段</strong>：在记忆阶段就是通过设计或学习网络的权值，使网络具有<u>若干个稳定的平衡状态</u>，这些稳定的平衡状态也称为<strong>吸引子（Attractor）</strong></p>
<ul>
<li>吸引子有一定的吸引域（Basin of Attraction），吸引子的吸引域就是能够稳定该吸引子的<u>所有初始状态的集合</u>，吸引域的大小用吸引半径来描述，吸引半径可定义为：吸引域中所含所有状态之间的最大距离或吸引子所能吸引状态的最大距离</li>
<li>吸引子也就是<u>联想记忆网络能量函数的极值点</u>，记忆过程就是将<u>要记忆和存储的模式设计或训练成网络吸引子的过程</u></li>
</ul>
</li>
<li><p>（2）<strong>联想阶段</strong></p>
<ul>
<li>联想过程就是给定输入模式，联想记忆网络通过动力学的演化过程达到稳定状态，即收敛到吸引子，回忆起已存储模式的过程。</li>
</ul>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/7-2.png" alt="7-2" style="zoom:40%;"></p>
<ul>
<li>吸引子的数量代表着AM的记忆容量（Memory Capacity）或存储容量（Storage Capacity），存储容量就是在一定的联想出错概率容限下，网络中存储互不干扰样本的最大数目</li>
<li>吸引子具有一定的吸引域，吸引域是衡量网络容错性的指标，<u>吸引域越大网络的容错性能越好，或者说网络的联想能力就越强</u></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Hopfield联想记忆网络</strong></p>
<ul>
<li><p>将Hopfield网络作为AM需要设计或训练网络的权值，使吸引子存储记忆模式。常用的设计或学习算法有：外积法（OuterProductMethod）、投影学习法（ProjectionLearningRule）、伪逆法（PseudoInverseMethod）以及特征结构法（EigenStructureMethod）等</p>
</li>
<li><font color="red">Hopfield联想记忆网络运行步骤</font>

<ul>
<li><p>第一步：设定记忆模式。将欲存储的模式进行编码，得到取值为1和−1的记忆模式($m&lt;n$)：</p>
<p>$ U_{k}= \left[ u_{1}^{k},u_{2}^{k}, \cdots ,u_{i}^{k}, \cdots ,u_{n}^{k} \right] ^{T}k=1,2, \cdots ,m $ </p>
</li>
<li><p>第二步：设计网络的权值</p>
<p>$ w_{ij}= \begin{cases} \frac{1}{N} \sum _{ \mu =1}^{M}u_{i}^{k}u_{j}^{k},j \neq i \\\\ 0,j=i \end{cases} $ </p>
</li>
<li><p>第三步：初始化网络状态。将欲识别模式$ U^{ \prime }= \left[ u_{1}’,u_{2}’, \cdots ,u_{i}’, \cdots ,u_{n}’\right] ^{T} $ 设为网络状态的初始状态，即：$ \nu _{i}(0)=u_{i} $ ，是网络中任意神经元$i$在$t=0$时刻的状态</p>
</li>
<li><p>第四步：迭代收敛，随机地更新某一神经元的状态$ \nu _{i}(t+1)=Sgn \left[ \sum _{j=1}^{N}w_{ij}x_{j}(n) \right] $ 。反复迭代直至网络中所有神经元的状态不变为止</p>
</li>
<li><p>第五步：网络输出，这时的网络状态（稳定状态），即为网络的输出$y=v_i(T)$</p>
</li>
</ul>
</li>
<li><p>Hopfield联想记忆网络的记忆容量：就是在一定的联想出错概率容限下，网络中存储互不干扰样本的最大数目。记忆容量$\alpha$反映所记忆的模式$m$和神经元的数目$N$之间的关系：$\alpha=m/N$。记忆$m$个模式所需的神经元数$N=m/\alpha$，联接权值数目为$(m/\alpha)^2$，若$\alpha$增加一倍，联接权值数目降为原来的$1/4$，这是一对矛盾。在技术实现上也是很困难的。实验和理论研究表明Hopfield联想记忆网络记忆容量的上限为$0.15N$。</p>
</li>
<li><p>Hopfield AM网络存在伪状态（Spurious States），伪状态是指除记忆状态之外网络多余的稳定状态。</p>
</li>
</ul>
</li>
</ul>
<h3 id="最优化计算"><a href="#最优化计算" class="headerlink" title="最优化计算"></a>最优化计算</h3><p>将Hopfield神经网络应用于求解组合优化问题，就是把目标函数转化为网络的能量函数，把问题的变量对应于网络的状态，当网络的能量函数收敛于极小值时，网络的状态就对应于问题的最优解。</p>
<h2 id="八、随机神经网络"><a href="#八、随机神经网络" class="headerlink" title="八、随机神经网络"></a>八、随机神经网络</h2><h3 id="基本的非确定方法"><a href="#基本的非确定方法" class="headerlink" title="基本的非确定方法"></a>基本的非确定方法</h3><ul>
<li><p>非确定的方法：生物神经网络按照概率运行（别称：统计方法（Statistical Method））</p>
</li>
<li><p>既可以用于训练，又可以用于运行</p>
</li>
<li><p>基本思想</p>
<ul>
<li>从所给的网络中“随机地选取一个联接权”</li>
<li>对该联接权提出一个“伪随机调整量”</li>
<li>当用此调整量对所选的联接权进行修改后<ul>
<li>如果“被认为”修改改进了网络的性能，则保留<br>此调整；</li>
<li>否则放弃本次调整。</li>
</ul>
</li>
</ul>
</li>
<li><p>算法1 基本统计训练算法</p>
<p>1、从样本集$S$中取一样本$(X,Y)$；</p>
<p>2、将$X$输入到网络中，计算出实际输出$O$；</p>
<p>3、求出网络关于$Y$、$O$的误差测度$E$；</p>
<p>4、随机地从$W ^ { ( 1 ) } W ^ { ( 2 ) }, \ldots, W ^ { ( L ) }$中选择一个联接权$ w_{ij}^{(p)} $ ；</p>
<p>5、生成一个小随机数$ \Delta w_{ij}^{(p)} $ ；</p>
<p>6、用$ \Delta w_{ij}^{(p)} $ 修改$ w_{ij}^{(p)} $ ；</p>
<p>7、用修改后的$W ^ { ( 1 ) } W ^ { ( 2 ) }, \ldots, W ^ { ( L ) }$重新计算$X$对应的实际输出$O’$；<br>8、求出网络关于$Y$、$O’$的误差测度$E’$ ；<br>9、如果$E’&lt;E$，则保留本次对$W ^ { ( 1 ) } W ^ { ( 2 ) }, \ldots, W ^ { ( L ) }$的修改，否则，根据概率判断本次修改是否有用，如果认为有用，则保留本次对$W ^ { ( 1 ) } W ^ { ( 2 ) }, \ldots, W ^ { ( L ) }$的修改，如果认为本次修改无用，则放弃它；<br>重复上述过程，直到网络满足要求。</p>
</li>
</ul>
<h3 id="模拟退火（重点）"><a href="#模拟退火（重点）" class="headerlink" title="模拟退火（重点）"></a><font color="red">模拟退火（重点）</font></h3><ul>
<li>金属中原子的能量与温度有关。原子能量高的时候，有能力摆脱其原来的能量状态而最后达到一个更加稳定的状态——全局极小能量状态</li>
<li>在金属的退火过程中，能量的状态分布$ P(E) \propto exp(- \frac{E}{kT}) $ <ul>
<li>$P(E)$——系统处于具有能量$E$的状态的概率；</li>
<li>$k$——Boltzmann常数；</li>
<li>$T$——系统的绝对温度(Kelvin)</li>
</ul>
</li>
<li>高温情况下：$T$足够大，对系统所能处的任意能量状态$E$，有$exp(- \frac{E}{kT}) $ 将趋于1</li>
<li>中温情况下：$T$比较小，$E$的大小对$P(E)$有较大的影响，设$E_1&gt;E_2,P(E_2)&gt;P(E_1)$，即，系统处于高能量状态的可能性小于处于低能量状态的可能性</li>
<li>低温情况下：$T$非常小，$E$的大小对影响非常大，设$E_1&gt;E_2,P(E_2)&gt;&gt;P(E_1)$，即，当温度趋近于0时，系统几乎不可能处于高能量状态</li>
</ul>
<h4 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h4><ul>
<li>首先在高温下进行搜索，此时各状态出现概率相差不大，可以很快进入“热平衡状态”，这时进行的是一种“粗搜索”，也就是大<u>致找到系统的低能区</u></li>
<li>随着温度的逐渐降低，各状态出现概率的差距逐渐被扩大，搜索精度不断提高。这就可以<u>越来越准确地找到网络能量函数的全局最小点</u></li>
</ul>
<h4 id="模拟退火与传统迭代最优算法的比较"><a href="#模拟退火与传统迭代最优算法的比较" class="headerlink" title="模拟退火与传统迭代最优算法的比较"></a>模拟退火与传统迭代最优算法的比较</h4><p>（1）当系统在非零温度下时，从局部最优中跳出是非常可能的，因此不会陷入局部最优。<br>（2）系统最终状态的总特征可以在较高温度下看到，而状态的好的细节却在低温下表现，因此，模拟退火是自适应的。</p>
<h4 id="模拟退火原理"><a href="#模拟退火原理" class="headerlink" title="模拟退火原理"></a>模拟退火原理</h4><p>1.Metropolis抽样过程</p>
<ul>
<li>ΔE表示系统从状态vi转移至状态vj所引起的能量差。如果能量差ΔE为负，这种转移就导致状态能量的降低，这种转移就被接受。接下来，新状态作为算法下一步的起始点。</li>
<li>若能量差为正，算法在这一点进行概率操作。首先，选定一个在[0,1]内服从均匀分布的随机数ξ。如果$ξ&lt;e^{-ΔE/T}$，则接受这种转移，否则，拒绝这种转移；即在算法的下一步中拒绝旧的状态。如此反复，达到系统在此温度下的热平衡。</li>
</ul>
<p>2.退火过程（降温过程）</p>
<ul>
<li>Metropolis抽样过程中温度$T$缓慢地降低。</li>
<li>初始温度值：初始温度值$T_0$要选得足够高，保证模拟退火算法中所有可能的转移都能被接受。</li>
<li>温度的下降：原先使用指数函数实现温度的下降。但是这种方法使降温幅度过小，从而延长搜索时间。在实际中，通常使用下式：$ T_{k}= \lambda T_{k-1},k=1,2, \cdots $ </li>
<li>终止温度：如果在连续的若干个温度下没有可接受的新状态，系统冻结或退火停止。</li>
</ul>
<h3 id="模拟退火算法用于组合优化问题"><a href="#模拟退火算法用于组合优化问题" class="headerlink" title="模拟退火算法用于组合优化问题"></a>模拟退火算法用于组合优化问题</h3><p>第一步：初始化。依据所要解决的组合优化问题，确定代价函数𝐶(﹒)的表达式，随机选择初始状态𝑉=𝑉(0)，设定初始温度$T_0$，终止温度$T_{final}$，概率阈值ξ。</p>
<p>第二步：Metropolis抽样过程<br>（1）在温度𝑇下依据某一规定的方式，根据当前解所处的状态𝑉，产生一个近邻子集𝑁(𝑉)（可包括𝑉，也可不包括𝑉），在𝑁(𝑉)内随机寻找一个新状态𝑆’作为下一个当前解的候选解，计算Δ𝐶’=𝐶(𝑉’)−𝐶(𝑉)。</p>
<p>（2）若Δ𝐶’&lt;0,则𝑉=𝑉’，作为下一状态；若Δ𝐶’&gt;0，则计算概率$ e^{- \Delta C^{ \prime }/T} $ ，若其大于给定概率阈值ξ，则取下一状态为𝑉=𝑉’，否则，保留这一状态。<br>（3）<u>按某一给定的收敛算法检查算法在温度T下是否应停止，若符合收敛条件则表示已达到热平衡，转向第三步的退火过程，若不符合收敛条件，则转向（1）继续迭代，直至在此温度下收敛。</u><!--0,则𝑉=𝑉’，作为下一状态；若Δ𝐶’--></p>
<p>第三步：退火过程。<br>按照一定的降温方法得到一个新的温度𝑇，检查𝑇是否小于给定的温度终止阈值$T_{final}$。若小于，则退火过程结束，当前状态𝑉即为算法最终输出解。若温度𝑇大于等于给定阈值，则转至Metropolis抽样过程，在新的温度下搜索状态。</p>
<h3 id="玻尔兹曼机"><a href="#玻尔兹曼机" class="headerlink" title="玻尔兹曼机"></a>玻尔兹曼机</h3><h4 id="随机神经网络与其他网络的比较"><a href="#随机神经网络与其他网络的比较" class="headerlink" title="随机神经网络与其他网络的比较"></a>随机神经网络与其他网络的比较</h4><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/8-1.png" alt="8-1" style="zoom: 67%;"></p>
<ul>
<li>BP网络是一种“贪心”算法，容易陷入局部最小点。</li>
<li>Hopfield网络很难避免出现伪状态，网络是严格按照能量减小的方向运行的，容易陷入局部极小点，而无法跳出。</li>
<li>所以，在用BP网络和Hopfield网络进行最优化的计算时，由于限定条件的不足，往往会使网络稳定在误差或能量函数的局部最小点，而不是全局最小点，即所得的解不是最优解。</li>
</ul>
<h4 id="随机神经网络的基本思想"><a href="#随机神经网络的基本思想" class="headerlink" title="随机神经网络的基本思想"></a>随机神经网络的基本思想</h4><font color="red">网络向误差或能量函数减小方向运行的概率大，同时向误差或能量函数增大方向运行的概率存在，这样网络跳出局部极小点的可能性存在，而且向全局最小点收敛的概率最大。</font>

<h4 id="Boltzmann机的网络结构"><a href="#Boltzmann机的网络结构" class="headerlink" title="Boltzmann机的网络结构"></a>Boltzmann机的网络结构</h4><ul>
<li>Boltzmann机由输入部、输出部和中间部构成。输入神经元和输出神经元可称为显见神经元，它们是网络与外部环境进行信息交换的媒介。中间部的神经元称为隐见神经元，它们通过显见神经元与外部进行信息交换。</li>
<li>每一对神经元之间的信息传递是双向对称的，即$w_{ij}=w_{ji}$，而且自身无反馈即$w_{ii}=0$。学习期间，显见神经元将被外部环境“约束”在某一特定的状态，而中间部隐见神经元则不受外部环境约束。</li>
</ul>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/8-2.png" alt="8-2" style="zoom:67%;"></p>
<ul>
<li><p><strong>单个神经元的运行特性</strong></p>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/8-3.png" alt="8-3" style="zoom:50%;"></p>
<ul>
<li>神经元$i$的全部输入信号的总和为$u_i$为：$ u_{i}= \sum _{j}^{n}w_{ij}v_{j}+b_{i} $ </li>
<li>神经元的输出$v_i$依概率取1或0：<ul>
<li>$v_i$取1的概率：$ P( v _{i}=1)=1/(1+e^{-u_{i}/T}) $ </li>
<li>$v_i$取0的概率：$ P( v _{i}=0)=1-P( v _{i}=1)$</li>
</ul>
</li>
<li>由此可见，$v_i$取1的概率受两个因素的影响：<br>(1) $u_i$越大$v_i$则取1的概率越大，而取0的概率越小；<br>(2)参数$T$称为“温度”，在不同的温度下$v_i$取1的概率$P$随$u_i$的变化如图所示。<ul>
<li>可见，$T$越高时，曲线越平滑，因此，即使$u_i$有很大变动，也不会对$v_i$取1的概率变化造成很大的影响；反之，$T$越低时，曲线越陡峭，当$u_i$有稍许变动时就会使概率有很大差异。<u>即温度高时状态变化接近随机，随着温度的降低向确定性的动作靠近。</u></li>
<li>当$T$→0时，每个神经元不再具有随机特性，而具有确定的特性，<u>激励函数变为阶跃函数，这时Boltzmann机趋向于Hopfield网络。</u></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/8-4.png" alt="8-4" style="zoom:50%;"></p>
<h4 id="Boltzmann机的工作原理"><a href="#Boltzmann机的工作原理" class="headerlink" title="Boltzmann机的工作原理"></a>Boltzmann机的工作原理</h4><ul>
<li>Boltzmann机采用下式所示的能量函数作为描述其状态的函数: $ E=- \frac{1}{2} \sum _{i,j}w_{ij}v_{i} \nu _{j} $ </li>
<li>Boltzmann机在运行时，假设每次只改变一个神经元的状态，如第$i$个神经元，设$v_i$取0和取1时系统的能量函数分别为0和$ - \sum _{j}w_{ij}v_{j} $ ，它们的差值为$\Delta E_i$。$ \Delta E_{i}=E|_{v_{i}=0}-E|_{v_{i}=1}=0-(- \sum _{j}w_{ij} \nu _{j})= \sum _{j}w_{ij} \nu _{j} $ </li>
<li>$\Delta E_i$的取值可能有两种情况：$\Delta E_i$&gt;0或$\Delta E_i$&lt;0<ul>
<li>当$\Delta E_i&gt;0$时，即$ u_{i}= \sum _{j}w_{ij}v_{j}&gt;0 $ 时，$E|_{v_{i}=0}&gt;E|_{v_{i}=1}$<ul>
<li>神经元取1的概率：$ P( v _{i}=1)=1/(1+e^{-u_{i}/T}) $ </li>
<li>神经元取0的概率：$P( v_{i}=0)=e^{-u_{i}/T}/(1+e^{-u_{i}/T})$</li>
<li>$u_i&gt;0$时，$ e^{-u_{i}/T}&lt;1 $ ，$ p( v _{i}="1)"&gt;P( v _{i}=0) $ <!--1--></li>
<li>这时神经元$i$的状态取1的可能性比取0的可能性大，即网络状态取能量低的可能性大。</li>
</ul>
</li>
<li>当$\Delta E_i&lt;0$时，即$ u_{i}= \sum _{j}w_{ij}v_{j}&lt;0 $ 时，$E|_{v_{i}=0}&lt;E|_{v_{i}=1}$<ul>
<li>此时，$ P( v _{i}=1)&lt;P( v _{i}=0) $ </li>
<li>神经元$i$的状态取0的可能性比取1的可能性大，即网络状态取能量低的可能性大。</li>
</ul>
</li>
</ul>
</li>
<li><font color="red">网络状态取能量低的可能性大。运行过程中总的趋势是朝能量下降的方向运动，但也存在能量上升的可能性。</font>

</li>
</ul>
<h4 id="Boltzmann机的运行步骤"><a href="#Boltzmann机的运行步骤" class="headerlink" title="Boltzmann机的运行步骤"></a>Boltzmann机的运行步骤</h4><ul>
<li>第一步：对网络进行初始化。设定初始温度$T_0$、终止温度$T_{final}$和阈值𝜉，以及网络各神经元的连接权值$w_{ij}$。</li>
<li>第二步：在温度$T_m$条件下（初始温度为$T_0$）随机选取网络中的一个神经元$i$，计算神经元$i$的输入信号总和$u_i$：$ u_{i}= \sum _{j=1,i\neq j}^{n}w_{ij}v_{j}+b_{i} $ </li>
<li>第三步：若$u_i$&gt;0，即能量差$Δ𝐸_𝑖$&gt;0，取$v_i=1$为神经元$i$的下一状态值。若$u_i$&lt;0，计算概率：$ P_{i}=1/(1+e^{-u_{i}/T}) $ </li>
<li>第四步：判断网络在温度$T_m$是否达到稳定，若未达到稳定，则继续在网络中随机选取另一神经元$j$，令$j=i$，转至第二步重复计算，直至网络在$T_m$下达到稳定。若网络在$T_m$下已达到稳定则转至第五步计算。</li>
<li>第五步：以一定规律降低温度，使$T_{m+1}&lt;T_m$ ，判断$T_{m+1}$是否小于$T_{final}$，若$T_{m+1}$大于等于$T_{final}$，则$T_m=T_{m+1}$，转至第二步重复计算；若$T_{m+1}$小于$T_{final}$，则运行结束。此时在$T_m$下所求得的网络稳定状态，即为网络的输出。</li>
</ul>
<h4 id="Boltzmann机的学习规则"><a href="#Boltzmann机的学习规则" class="headerlink" title="Boltzmann机的学习规则"></a>Boltzmann机的学习规则</h4><ul>
<li>Boltzmann机是一种随机神经网络，可使用概率中的似然函数量度其模拟外界环境概率分布的性能。因此，Boltzmann机的学习规则就是根据最大似然规则，通过调整权值$w_{ij}$，最小化似然函数或其对数</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>复习笔记</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
