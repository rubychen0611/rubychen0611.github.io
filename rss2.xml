<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>与我常在</title>
    <link>https://rubychen0611.github.io/</link>
    
    <atom:link href="https://rubychen0611.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>自爱兼爱，善感而不多愁。</description>
    <pubDate>Sat, 14 Nov 2020 12:29:40 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>【论文笔记】Effective Path</title>
      <link>https://rubychen0611.github.io/2020/11/12/Effective-Path/</link>
      <guid>https://rubychen0611.github.io/2020/11/12/Effective-Path/</guid>
      <pubDate>Thu, 12 Nov 2020 07:31:20 GMT</pubDate>
      
      <description>&lt;p&gt;原文：Adversarial Defense Through Network Profiling Based Path Extraction （CVPR’19)&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>原文：Adversarial Defense Through Network Profiling Based Path Extraction （CVPR’19)  <a id="more"></a></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><ul><li><p>基本思想：找到Effective Path——一组重要连接和神经元的集合，使得模型得到最终的预测类别结果。可类比普通程序的执行路径。</p></li><li><p>前人方法（CDRP）</p><ul><li>需要重训练</li><li>维度过高</li></ul></li><li>提取路径信息的方法受到了传统程序分析中控制流和基本块的启发</li></ul><h3 id="单张图片提取"><a href="#单张图片提取" class="headerlink" title="单张图片提取"></a>单张图片提取</h3><p>欲提取有效路径$\mathcal{P=(N,S,W)}$，其中$\mathcal{N、S、W}$分别为重要神经元、连接、权重的集合。</p><ul><li><p>提取过程（由后向前）</p><ul><li><p>最后一层：只包括输出类别的神经元$n^L_p$</p></li><li><p>重要权重：最小权重集合，使得只计算该部分输出时，结果大于神经元$n^L_p$原本输出的$\theta$倍。由此得到前一层的重要神经元集合$N^{L-1}$。</p><ul><li>最小组合选取方法：对输入和权重的乘积进行排序，选最小且满足条件的组合</li></ul><p><img src="/2020/11/12/Effective-Path/1.png" alt="1" style="zoom:80%;"></p></li><li><p>对前一层的每个重要神经元：重复上述方法</p></li><li><p>卷积层特殊处理：</p><p><img src="/2020/11/12/Effective-Path/2.png" alt="2" style="zoom:80%;"></p><ul><li>根据感受野转换成全连接层</li><li>计算最小权重组合时无需对所有神经元排序，只需排感受野内的</li><li>由于卷积，多个连接可能共享相同的权重</li></ul></li></ul></li></ul><h3 id="多张图片提取"><a href="#多张图片提取" class="headerlink" title="多张图片提取"></a>多张图片提取</h3><ul><li>单张图片的路径相当于是神经元或连接的一个二元掩膜，指示了其是否在预测过程中产生了影响。多张图片的有效路径即其中所有单张图片路径组成的集合。使用每个类别预测正确的图片可得到per-class effective path，使用所有训练集预测正确的图片可得到overall effective path。</li><li>路径的稀疏性：<ul><li>设置$\theta=0.5$时，LeNet5、AlexNet、ResNet50、InceptionV4、VGG16上，Overall effective path在所有路径和权重中的占比分别为13.8%, 20.5%, 22.2%, 41.7%,17.2%，说明提取出的有效路径非常稀疏。</li></ul></li></ul><h2 id="路径的特殊化现象"><a href="#路径的特殊化现象" class="headerlink" title="路径的特殊化现象"></a>路径的特殊化现象</h2><p>不同类别的有效路径将网络拆分成不同的组件，可用于理解网络以及研究更改网络结构所带来的影响。</p><ul><li><p>路径特殊化现象：不同类别的有效路径相差很大</p><ul><li><p>计算Jacarrd系数（图2）：不同类别之间的相似度几乎都小于0.5，说明一半左右是共同激活的路径，一半是不同的</p></li><li><p>逐个合并ImageNet1000个类别的有效路径，密度一开始迅速上升然后在50个类别时开始趋于平缓（图3），与ImagNet有100个左右的基础类别这一事实相符合。</p><p><img src="/2020/11/12/Effective-Path/3.png" alt="3" style="zoom:80%;"></p></li></ul></li></ul><h2 id="对抗样本防御"><a href="#对抗样本防御" class="headerlink" title="对抗样本防御"></a>对抗样本防御</h2><ul><li><p>6种对抗攻击方法</p><p><img src="/2020/11/12/Effective-Path/4.png" alt="4" style="zoom:70%;"></p></li><li><p>对抗样本检测方法：计算<strong>路径相似度</strong></p><ul><li><p>计算待测图像的有效路径与该预测类别的有效路径之间的相似程度，因为单张图片的有效路径密度远低于类别的有效路径，所以Jaccard系数基本上取决于二者的交集占该图片有效路径的比例大小</p><ul><li>LeNet实验结果（图(a)）：正常样本相似度基本都是1.0左右，对抗样本相似度较低</li><li>AlexNet实验结果：<ul><li>Rank-1类别path：划分成不同层的相似度，对抗样本相似度同样较低（图c）。正常样本与对抗样本相似度之差（图d）显示中间层下降最多。</li><li>Rank-2类别path：正常样本相似度反而最低（图d），因为对抗样本的rank2类别往往是正常样本的rank1类别，所以对抗样本路径相似度更高。</li></ul></li></ul><p><img src="/2020/11/12/Effective-Path/5.png" alt="5"></p></li><li><p>防御模型</p><ul><li><p>使用rank1和rank2类别的有效路径来检测对抗样本</p></li><li><p>线性模型：每层rank1类别有效路径相似度$-$每层rank2类别有效路径相似度：$ \tilde{J}_{P}= \sum _{l=1}^{L} \omega ^{l}J_{P}^{l}- \sum _{l=1}^{L} \omega ^{l^{ \prime }}J_{P}^{l^\prime} $，若小于某阈值则判断为对抗样本</p></li><li><p>其他模型：random forest, AdaBoost, and gradient boosting</p></li></ul></li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E5%AF%B9%E6%8A%97%E9%98%B2%E5%BE%A1/">对抗防御</category>
      
      
      <comments>https://rubychen0611.github.io/2020/11/12/Effective-Path/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】Dynamic Slicing for Deep Neural Networks</title>
      <link>https://rubychen0611.github.io/2020/11/10/Dynamic-Slicing-for-DNN/</link>
      <guid>https://rubychen0611.github.io/2020/11/10/Dynamic-Slicing-for-DNN/</guid>
      <pubDate>Tue, 10 Nov 2020 08:07:33 GMT</pubDate>
      
      <description>&lt;p&gt;Dynamic Slicing for Deep Neural Networks （FSE’20）&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>Dynamic Slicing for Deep Neural Networks （FSE’20）<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><p>传统程序切片技术：旨在从程序中提取满足一定约束条件（切片标准）的代码片段，是一种用于分解程序的程序分析技术。</p><ul><li>例如，通过将切片标准设置为某个导致错误的特定输出，可以获得与错误相关但比整个程序小得多的程序切片，因此更容易分析。</li></ul></li><li><p><strong>DNN切片</strong>的定义：计算获得一个神经元和突触子集，使其可以显著影响某些感兴趣的神经元的值。</p><ul><li>有助于理解神经网络的决策</li><li>有助于减小模型大小</li><li>把模型划分为重要/非重要部分，有利于优先保护模型的重要部分</li></ul></li><li><p>目标：找到在决策过程中更重要的神经元和突触的子集。</p></li><li><p>DNN切片的挑战：</p><ul><li>神经元的权重、连接难以理解</li><li>神经网络输出与几乎所有神经元都有关，因此必须区分每个神经元的贡献重要程度</li><li>DNN非常庞大，对性能要求高</li></ul></li><li><p>本文方法：<strong>NNSlicer</strong></p><ul><li><p>一种基于数据流分析的神经网络动态切片技术</p></li><li><p><strong>切片标准</strong>的定义：一组具有特殊意义的神经元</p><ul><li>如输出层神经元</li></ul></li><li><p><strong>神经网络切片</strong>的定义：一组对切片标准产生重要影响的神经元集合</p></li><li><p>动态切片：针对一组特定输入切片，而不是静态、与输入独立的</p></li><li><p>包括三个阶段：</p><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/3.png" alt="3" style="zoom:67%;"></p><ul><li>Profiling phase：对每个神经元的平均行为（输出值）建模。训练集在该神经元输出的均值作为<font color="red">baseline</font>。</li><li>Forward analysis phase：对欲切片的一组感兴趣的输入，喂进DNN，记录神经元输出。输出与baseline的差异代表了<font color="red">神经元对输入的敏感程度</font></li><li>Backward analysis phase：切片过程从切片标准中定义的神经元开始，逐层向前分析具有重要影响的神经元。</li></ul></li><li><p>效率</p><ul><li>一个样本：<ul><li>ResNet10：40s</li><li>ResNet18：550s</li></ul></li><li>批样本：<ul><li>ResNet10：3s</li><li>ResNet18：40s</li></ul></li></ul></li><li><p>应用：</p><ul><li>对抗样本检测</li><li>模型剪枝</li><li>模型保护</li></ul></li></ul></li></ul><h2 id="问题形式化定义"><a href="#问题形式化定义" class="headerlink" title="问题形式化定义"></a>问题形式化定义</h2><ul><li>符号</li></ul><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/1.png" style="zoom:70%;"></p><ul><li><p>程序切片</p><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/2.png" alt="2" style="zoom:70%;"></p></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="Profiling-和-Forward-analysis阶段"><a href="#Profiling-和-Forward-analysis阶段" class="headerlink" title="Profiling 和 Forward analysis阶段"></a>Profiling 和 Forward analysis阶段</h3><ul><li>Profiling阶段：记录每个神经元在训练样本上的输出均值$\overline{y^{n}( \mathcal{D})}$（卷积层一个通道看作一个神经元）</li><li>Forward analysis阶段：对特定的输入样本，计算神经元n的输出值与均值的差$ \Delta y^{n}( \xi )=y^{n}( \xi )- \overline{y^{n}( \mathcal{D} )} $。绝对值越大表示越敏感。</li></ul><h3 id="Backward-Analysis-和切片提取"><a href="#Backward-Analysis-和切片提取" class="headerlink" title="Backward Analysis 和切片提取"></a>Backward Analysis 和切片提取</h3><ul><li><p>对每个神经元和连接都计算一个贡献值$CONTRIB$，$CONTRIB \neq 0$表示该神经元或连接是重要的，$CONTRIB&gt;0$表示贡献值是正向的，否则是负向的。</p></li><li><p>$CONTRIB$的计算方法</p><ul><li><p><strong>递归</strong>地从后向前计算</p><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/4.png" alt="4" style="zoom:67%;"></p></li><li><p>局部贡献值计算</p><ul><li>如Weighted sum操作: 对神经元$n$，前一层第$i$个神经元的局部贡献值$contrib_{i}=CONTRIB_{n} \times \Delta y^{n} \times w_{i} \Delta x_{i} $，其中$CONTRIB_n$为该神经元$n$当前的全局贡献值，$\Delta y^{n}$为该神经元$n$在forward analysis阶段计算出的相对敏感度，$\Delta x_{i}$为前一层神经元$n_i$的相对敏感度，即$\Delta y^{n_i}$。乘积$w_{i} \Delta x_{i}$表示$n_i$和$s_i$对全局贡献值$CONTRIB_n$的影响，比如，如果$\Delta y^{n}$是负的，$w_{i} \Delta x_{i}$是正的，说明$n_i$扩大了$n$的负性，对于$CONTRIB_n$有负向贡献。</li></ul><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/5.png" alt="5" style="zoom:67%;"></p></li><li><p>全局贡献值=SIGN(局部贡献值)的累加，即局部贡献值只在{-1,0,1}中取值</p></li></ul></li><li><p>参数$\theta$控制切片粒度：抛弃贡献值低于$\theta$的前层神经元</p></li><li><p>多个样本的集合：贡献值=单个样本贡献值的和</p></li><li><p>切片：去掉贡献值为0的神经元和连接。</p></li></ul><h3 id="GPU和多线程加速"><a href="#GPU和多线程加速" class="headerlink" title="GPU和多线程加速"></a>GPU和多线程加速</h3><p>对于大样本集合$I$，前向分析过程在GPU上大批次处理，反向分析过程在CPU上一个小批次用一个线程处理。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="Overhead"><a href="#Overhead" class="headerlink" title="Overhead"></a>Overhead</h3><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/6.png" alt="6" style="zoom:67%;"></p><h3 id="对抗防御"><a href="#对抗防御" class="headerlink" title="对抗防御"></a>对抗防御</h3><ul><li><p>思想：对抗样本和正常样本的切片应该不同。从正常样本集合我们可获得一个切片，若新的输入切片与正常样本切片非常不同，则很有可能是对抗样本。</p></li><li><font color="red">方法：假设DNN模型为$\mathcal{M}$，输入样本为$\xi$，其在$\mathcal{M}$上预测的输出为$\mathcal{M}(\xi)$，对于切片标准$\mathcal{C}=(\xi, \mathcal{M}(\xi))$，使用NNSlicer方法得到切片$\mathcal{M}_\xi$。构造一个切片分类器$F$，其在训练样本上学习得到切片形状与输出类别之间的关系，若$F(M_\xi) \neq \mathcal{M(\xi)}$，则判断该样本为对抗样本。</font><ul><li>$F$的输入是一个切片$M_\xi$，具体表示为一个向量$vec_\xi$，即所有神经元之间的连接及该连接的贡献的集合。</li><li>本文采用决策树算法构造分类器$F$。</li></ul></li><li><p>用NNSlicer检测对抗样本的好处</p><ul><li>不需要更改或重训练模型</li><li>支持大型模型</li><li>只需要正常样本来训练分类器，不需要对抗样本</li></ul></li><li><p>实验设置</p><ul><li>对比方法：FeatureMap和EffectivePath，都使用了分类器</li><li>数据集和模型：CIFAR10和ResNet10，10000张正常样本训练分类器。</li><li>对抗攻击方法：17种（FoolBox实现）<ul><li>FGSM_2，FGSM_4，FGSM_8</li><li>DeepFoolL2，DeepFoolLinf</li><li>JSMA</li><li>RPGD_2,RPGD_4,RPGD_8</li><li>CWL2</li><li>ADef</li><li>SinglePixel</li><li>LocalSearch</li><li>Boundary</li><li>Spatial</li><li>PointWise</li><li>GaussianBlur</li></ul></li></ul></li><li><p>实验结果</p><ul><li>平均Recall为100%，平均Precision为83%</li></ul><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/7.png" alt="7" style="zoom:60%;"></p></li></ul><h3 id="网络简化和剪枝"><a href="#网络简化和剪枝" class="headerlink" title="网络简化和剪枝"></a>网络简化和剪枝</h3><ul><li>一般的简化方法针对所有类别，NNSlicer针对特定的几个类别简化（如区分ImageNet里的各种狗）</li><li>方法：假设我们想对模型$\mathcal{M}$进行简化，目标类别为$\mathcal{O}^T$，剪枝比例为$r$。令$\mathcal{I}^T$为目标类别的输入样本，使用NNSlicer计算每个连接s的贡献$CONTRIB_s$，对每层每个连接的贡献值排序，剪掉最小的比例为$r$的这些连接，如果一个神经元的所有连接都被剪掉，则该神经元也被去掉。</li><li>实验设计：CIFAR10的10个类别中所有子集（210个）都作为目标类别，<ul><li>对比方法<ul><li>EffectivePath</li><li>Weight：去掉权重绝对值最小的边</li><li>Channel：去掉平均权值最小的神经元</li></ul></li></ul></li><li>实验结果</li></ul><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/8.png" alt="8" style="zoom:60%;"></p><ul><li><p>一轮微调：用10k个样本重训练一轮</p><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/9.png" alt="9" style="zoom:60%;"></p></li></ul><h3 id="模型保护"><a href="#模型保护" class="headerlink" title="模型保护"></a>模型保护</h3><ul><li><p>保护贡献值大的连接</p></li><li><p>对比方法</p><ul><li>EffectivePath</li><li>Weight</li><li>Random：随机选择连接</li></ul></li><li><p>实验设计：假设50%的连接参数被隐藏，攻击者试图用训练数据重训练的方式恢复连接参数，重训练准确率越低表示保护方法越好。</p></li><li><p>实验结果</p><ul><li>Target classes上准确率非常低，但All classes上准确率与Weight接近，说明非目标类上准确率可能很高，但NNSlicer不保护非目标类。</li></ul><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/10.png" alt="10" style="zoom:60%;"></p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/DNN%E6%B5%8B%E8%AF%95/">DNN测试</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E7%A8%8B%E5%BA%8F%E5%88%87%E7%89%87/">程序切片</category>
      
      
      <comments>https://rubychen0611.github.io/2020/11/10/Dynamic-Slicing-for-DNN/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Windows后台运行Python脚本，开机自动运行</title>
      <link>https://rubychen0611.github.io/2020/10/14/Windows%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8CPython/</link>
      <guid>https://rubychen0611.github.io/2020/10/14/Windows%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8CPython/</guid>
      <pubDate>Wed, 14 Oct 2020 03:15:08 GMT</pubDate>
      
      <description>&lt;p&gt;为了每天自动约洗澡也是拼了&lt;span class=&quot;github-emoji&quot; style=&quot;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8) center/contain&quot; data-src=&quot;https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8&quot;&gt;😅&lt;/span&gt;&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>为了每天自动约洗澡也是拼了<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8">😅</span> <a id="more"></a></p><h2 id="后台运行python脚本"><a href="#后台运行python脚本" class="headerlink" title="后台运行python脚本"></a>后台运行python脚本</h2><p>已知目前有一每天定时预约洗澡的Python脚本shower.py，cmd设置后台运行：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pythonw shower.py</span><br></pre></td></tr></tbody></table></figure><p>pythonw.exe是无窗口的Python可执行程序，意思是在运行程序的时候，没有窗口，代码在后台执行。</p><p>注意如果像我一样电脑同时安装了Python2 和Python3，需要区分用的是哪个phthonw.exe，最简单的是使用绝对路径：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:\python37\pythonw.exe E:\shower.py</span><br></pre></td></tr></tbody></table></figure><p>可打开任务管理器检查pythonw进程是否已经启动。</p><h2 id="设置开机自动启动"><a href="#设置开机自动启动" class="headerlink" title="设置开机自动启动"></a>设置开机自动启动</h2><p>1、新建批处理文件run_shower.bat：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@echo off </span><br><span class="line">if "%1"=="h" goto begin </span><br><span class="line">start mshta vbscript:createobject("wscript.shell").run("""%~nx0"" h",0)(window.close)&amp;&amp;exit </span><br><span class="line">:begin </span><br><span class="line">::</span><br><span class="line">start /b cmd /k "D:\python37\pythonw.exe E:\shower.py"</span><br></pre></td></tr></tbody></table></figure><p>这段代码可以隐藏批处理运行的窗口。</p><p>解释：</p><blockquote><p>如果双击一个批处理，等价于参数为空，而一些应用程序需要参数，比如在cmd窗口输入shutdowm -s -t 0,其中-s -t 0就为参数。shutdown为%0，-s为%1，-t为%2，以此类推。<br>第一行我们先跳过，看第二行，表示利用mshta创建一个vbs程序，内容为:createobject(“wscript.shell”).run(……).<br>如果运行的批处理名为a.bat，在C:\下，那%0代表C:\a.bat，%~nx0代表a.bat。h为参数%1，0表示隐藏运行。<br>由于你双击运行，故第一次批处理%1为空，if不成立，转而运行下一句。然后再次打开自己，并传递参数h，此时if成立，跳转至begin开始运行。<br>这两行很经典，可以使批处理无窗口运行。</p></blockquote><p>2、将bat文件放在开机启动项里：Win+R打开运行窗口，输入shell:startup，将bat文件复制进启动文件夹里。</p><p>3、重启测试</p><p>参考：<a href="https://www.cnblogs.com/nmap/articles/8329125.html">https://www.cnblogs.com/nmap/articles/8329125.html</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/">常用技巧</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/Python/">Python</category>
      
      
      <comments>https://rubychen0611.github.io/2020/10/14/Windows%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8CPython/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】Compiler Validation via Equivalence Modulo Inputs</title>
      <link>https://rubychen0611.github.io/2020/10/08/EMI/</link>
      <guid>https://rubychen0611.github.io/2020/10/08/EMI/</guid>
      <pubDate>Thu, 08 Oct 2020 07:35:46 GMT</pubDate>
      
      <description>&lt;p&gt;原文：Compiler Validation via Equivalence Modulo Inputs （PLDI’14)&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>原文：Compiler Validation via Equivalence Modulo Inputs （PLDI’14) <a id="more"></a></p><h2 id="Equivalence-Modulo-Inputs-EMI"><a href="#Equivalence-Modulo-Inputs-EMI" class="headerlink" title="Equivalence Modulo Inputs (EMI)"></a>Equivalence Modulo Inputs (EMI)</h2><ul><li><p>给定程序$P$和一组输入值$I$，由$I$可生成一组程序集合$\mathcal{P}$，使得$\mathcal{P}$中每个程序$Q$都等价于$P$ modulo $I$：$\forall i \in I, Q(i) = P(i) $。则集合$\mathcal{P}$可用于对任意一个编译器$Comp$进行差分测试（differential testing），若存在某个$i \in I$和$Q \in \mathcal{P}$，使得$Comp(P)(i) \neq Comp(Q)(i)$，则该编译器存在bug。</p></li><li><p>核心思想：尽管$Q$只在输入集合$I$上与程序$P$语义等价，但编译器及其使用的静态分析和优化算法应该能为$Q$生成能在$I$上完全运行正确的中间代码。$P$和$Q$在数据流和控制流上可能很不同，经编译器优化后生成的代码也很不同，但结果应该完全一致。</p><p><img src="/2020/10/08/EMI/1.png" alt="emi" style="zoom:60%;"></p></li><li><p>生成EMI 变体的策略</p><ul><li>在$P$上运行输入集合$I$，获得运行轨迹，随机在未执行代码上做剪枝、插入、修改操作（假设$P$是一个确定的程序）</li></ul></li></ul><p>编译器的两类bug：</p><ul><li>导致编译器崩溃</li><li>生成错误代码（更加严重）<ul><li>导致正确的程序运行有bug</li><li>难以发现</li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E7%BC%96%E8%AF%91%E5%99%A8%E6%B5%8B%E8%AF%95/">编译器测试</category>
      
      
      <comments>https://rubychen0611.github.io/2020/10/08/EMI/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>《计算模型导引》复习笔记</title>
      <link>https://rubychen0611.github.io/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/</link>
      <guid>https://rubychen0611.github.io/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/</guid>
      <pubDate>Sun, 06 Sep 2020 02:22:35 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;第一章-递归函数&quot;&gt;&lt;a href=&quot;#第一章-递归函数&quot; class=&quot;headerlink&quot; title=&quot;第一章 递归函数&quot;&gt;&lt;/a&gt;第一章 递归函数&lt;/h2&gt;&lt;h3 id=&quot;1-1-数论函数（P1）&quot;&gt;&lt;a href=&quot;#1-1-数论函数（P1）&quot; class=&quot;headerlink&quot; title=&quot;1.1 数论函数（P1）&quot;&gt;&lt;/a&gt;1.1 数论函数（P1）&lt;/h3&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="第一章-递归函数"><a href="#第一章-递归函数" class="headerlink" title="第一章 递归函数"></a>第一章 递归函数</h2><h3 id="1-1-数论函数（P1）"><a href="#1-1-数论函数（P1）" class="headerlink" title="1.1 数论函数（P1）"></a>1.1 数论函数（P1）<a id="more"></a></h3><ul><li><p>数论函数</p><ul><li><p>k元数论全函数</p></li><li><p>部分数论函数</p></li></ul></li><li><font color="red">本原函数（$\mathcal{IF}$）</font><ul><li><p>零函数$Z$</p></li><li><p>后继函数$S$</p></li><li><p>投影函数$P_i^n$</p></li></ul></li><li><p>常用数论函数（P2）</p><ul><li>前驱函数pred</li><li>加法函数add</li><li><strong>算术差函数sub</strong></li><li>绝对差函数diff</li><li>乘法函数mul</li><li>除法函数div</li><li>求余函数rs</li><li>指数函数pow</li><li>平方函数sq</li><li>$E(x)=x-\lfloor \sqrt{x}\rfloor$</li><li>max、min</li><li>最大公约数函数gcd、最小公倍数函数lcm</li><li><strong>素数枚举函数$P$</strong></li><li><strong>$ep(n,x)$：x的素因子分解式中第n个素数的指数</strong></li><li>$eq(x,y)$：相等时等于0，否则为1</li><li><strong>$N$：否定，x=0时为1，否则为0</strong></li><li><strong>$N^2$：x=0时为0，否则为1</strong></li></ul></li><li><p>函数的复合（P4）</p></li><li>有界迭加算子$\sum$</li><li>有界迭乘算子$\prod$</li><li>有界$\mu-$算子</li><li>有界$max-$算子</li><li><font color="red">基本函数类$\mathcal{BF}$</font>（P6）<ul><li>$\mathcal{IF} \subseteq \mathcal{BF}$</li><li>$\mathcal{BF}$对于复合封闭</li></ul></li></ul><h3 id="1-2-配对函数（P7）"><a href="#1-2-配对函数（P7）" class="headerlink" title="1.2 配对函数（P7）"></a>1.2 配对函数（P7）</h3><ul><li>配对函数、配对函数组</li><li>Godel编码</li><li>若配对函数组$\{pg,K,L\}$使pg穷尽一切自然数，则称该配对函数组是一一对应的<ul><li>康托编码（P9）</li></ul></li><li>多元配对函数（P11）</li><li>Godel $\beta-$函数（P13）<ul><li>定理1.11：有穷数列的编码和解码（中国剩余定理）</li></ul></li></ul><h3 id="1-3-初等函数（P14）"><a href="#1-3-初等函数（P14）" class="headerlink" title="1.3 初等函数（P14）"></a>1.3 初等函数（P14）</h3><ul><li><font color="red">初等函数类$\mathcal{EF}$</font><ul><li>定义<ul><li>$\mathcal{IF} \subseteq \mathcal{EF}$</li><li>$x+y,x−y(绝对差),x×y,⌊x/y⌋∈\mathcal{EF}$<ul><li>加、乘、除可省</li></ul></li><li>$\mathcal{EF}$对于复合，有界迭加算子$∑[⋅]$和有界迭乘算子$∏[⋅]$封闭</li></ul></li><li>性质<ul><li>$\mathcal{EF}$对于有界$\mu-$算子和max-算子封闭</li></ul></li></ul></li><li>数论谓词<ul><li>数论谓词的特征函数：真为0，假为1</li><li>初等数论谓词：若谓词$P$的特征函数属于$\mathcal{EF}$，则称$P$是初等的</li></ul></li><li>数论集合<ul><li>数论集合的特征函数：属于集合为0，不属于为1</li><li>初等数论集合：若数论集合$S$的特征函数属于$\mathcal{EF}$，则称$S$是初等的</li></ul></li><li>重要的初等函数（P19）<ul><li>$x^y$</li><li>$\lfloor \sqrt[y] x \rfloor$</li><li>余数$rs(x,y)$</li><li>$\tau (x)$：x因子的数目</li><li>$prime(x)$：判定x是否为素数（数论谓词）</li><li>$\pi (x)$：不超过x的素数个数</li><li>素数枚举函数$P(n)$=第n个素数</li><li>$ep(n,x)$</li></ul></li><li>控制函数（P24）<ul><li>$\mathcal{EF}$的控制函数G（P22）<ul><li>控制函数不属于$\mathcal{EF}$</li></ul></li></ul></li></ul><h3 id="1-4-原始递归函数（P25）"><a href="#1-4-原始递归函数（P25）" class="headerlink" title="1.4 原始递归函数（P25）"></a>1.4 原始递归函数（P25）</h3><ul><li>原始递归算子<ul><li>带参原始递归算子</li><li>无参原始递归算子</li></ul></li><li><font color="red">原始递归函数类$\mathcal{PRF}$</font><ul><li>定义<ul><li>$\mathcal{IF} \subseteq \mathcal{PRF}$</li><li>$\mathcal{PRF}$对于复合、带参原始递归算子、无参原始递归算子封闭</li></ul></li><li>重要的原始递归函数<ul><li>add、pred、sub、diff、mul、sq（平方）、N、$N^2$、sqrt、E</li></ul></li></ul></li><li>原始复迭算子It​、弱原始复迭算子​Itw​ (P30)</li><li><font color="red">原始复迭函数类$\mathcal{ITF}$</font><ul><li>定义<ul><li>$\mathcal{IF} \subseteq \mathcal{ITF}$</li><li>$\mathcal{ITF}$对于复合、原始复迭算子It[·]、弱原始复迭算子Itw[·]封闭</li></ul></li><li>$\mathcal{ITF=PRF}$</li></ul></li><li>若干形式不同的递归式化归到原始递归式（P34）<ul><li>串值递归</li><li>联立递归</li><li>变参递归</li><li>多重递归</li></ul></li><li>$\mathcal{PRF}$的控制函数<ul><li>Ackermann函数：$Ack(m,n)$（P38）<ul><li>不是原始递归函数</li></ul></li></ul></li><li>$\mathcal{EF} \subset \mathcal{PRF}$ (P41)<ul><li>真包含：$\mathcal{EF}$的控制函数G属于$\mathcal{PRF}$</li></ul></li></ul><h3 id="1-5-递归函数（P42）"><a href="#1-5-递归函数（P42）" class="headerlink" title="1.5 递归函数（P42）"></a>1.5 递归函数（P42）</h3><ul><li>正则函数（全函数）、正则$\mu-$算子（区别于有界$\mu-$算子）</li><li><font color="red">一般递归函数类$\mathcal{GRF}$</font><ul><li>定义<ul><li>$\mathcal{IF} \subseteq \mathcal{GRF}$</li><li>$\mathcal{GRF}$对于复合和原始递归算子（带参和无参）封闭</li><li><u>$\mathcal{GRF}$对于正则$\mu-$算子封闭</u></li></ul></li></ul></li><li>$\mu-$算子（部分函数）</li><li><font color="red">部分递归函数类$\mathcal{RF}$</font> （P43）<ul><li>定义<ul><li>$\mathcal{IF} \subseteq \mathcal{RF}$</li><li>$\mathcal{RF}$对于复合和原始递归算子（带参和无参）封闭</li><li><u>$\mathcal{RF}$对于$\mu-$算子封闭</u></li></ul></li><li>显然$\mathcal{GRF} \subset \mathcal{RF}$ ：正则$\mu-$算子是$\mu-$算子的特例</li></ul></li><li>递归数论谓词、$\mu-$谓词、递归集</li><li>$\mathcal{PRF} \subset \mathcal{GRF}$ <ul><li>法1：利用控制函数，证明Ackermann函数是一般递归函数但不是原始递归函数（P45）</li><li>法2：利用通用函数（P47）</li></ul></li><li>$\mathcal{GRF} \subset 全数论函数\mathcal{NTF}$ （P48）<ul><li>存在数论全函数f，f不是一般递归函数 </li></ul></li></ul><h3 id="1-6-结论（P48）"><a href="#1-6-结论（P48）" class="headerlink" title="1.6 结论（P48）"></a>1.6 结论（P48）</h3><script type="math/tex; mode=display">本原函数类\mathcal{IF} \subset 基本函数类\mathcal{BF} \subset 初等函数类\mathcal{EF} \subset 原始递归函数类\mathcal{PRF} = 原始复迭函数类\mathcal{ITF} \subset 一般递归函数类\mathcal{GRF} \subset 部分递归函数类\mathcal{RF}</script><h2 id="第三章-lambda-演算"><a href="#第三章-lambda-演算" class="headerlink" title="第三章 $\lambda-$演算"></a>第三章 $\lambda-$演算</h2><h3 id="3-1-lambda-演算的语法（P72）"><a href="#3-1-lambda-演算的语法（P72）" class="headerlink" title="3.1 $\lambda-$演算的语法（P72）"></a>3.1 $\lambda-$演算的语法（P72）</h3><ul><li>变量：小写字母，表示一个参数（形参）或者一个值（实参）</li><li>抽象：$\lambda x.M$，绑定变量x于该函数抽象的函数体M，简单来说就是表示一个<u>形参</u>为x的函数M。</li><li>作用：$MN$，表示将函数M应用于参数N，简单来说就是给函数M输入<u>实参</u>N。<ul><li>函数作用是左结合的，即：<code>M N P</code>意为<code>(M N) P</code>而非<code>M (N P)</code></li></ul></li><li>自由变元和约束变元（P74）<ul><li>在函数抽象中，形参绑定于函数体，即形参是约束变元，相对应地，不是约束变元的自然就是自由变元</li><li>闭$\lambda-$项：没有自由变元的项，$\Lambda ^ \circ$表示全体闭$\lambda-$项的集合</li><li>约束变元改名后仍等价</li><li>自由变元的替换：$M[x:=N]$，$x$为自由变元</li></ul></li></ul><h3 id="3-2-转换（P76）"><a href="#3-2-转换（P76）" class="headerlink" title="3.2 转换（P76）"></a>3.2 转换（P76）</h3><ul><li>形式理论$\lambda \beta$ (P76)<ul><li>标准组合子$I,K,K^*,S$</li></ul></li><li>形式理论$\lambda \beta + ext$ 和$\lambda \beta \eta$ (P79)</li></ul><h3 id="3-3-归约（P80）"><a href="#3-3-归约（P80）" class="headerlink" title="3.3 归约（P80）"></a>3.3 归约（P80）</h3><ul><li>$\to_R$（一步$R-$归约）、$\twoheadrightarrow_R$ （$R-$归约）、$=_R$（$R-$转换）</li><li>二元关系$\beta$、$\alpha$、$\eta$、$\beta \eta$<ul><li>$M =_\beta N \Leftrightarrow \lambda \beta \vdash M=N$</li></ul></li><li>$\beta-$可约式、$\beta-$范式（$\beta-$nf）、$\beta-$范式的集合（$NF_R$）、$M$有$\beta-$nf</li></ul><h3 id="3-4-Church-Rosser定理（P85）"><a href="#3-4-Church-Rosser定理（P85）" class="headerlink" title="3.4 Church-Rosser定理（P85）"></a>3.4 Church-Rosser定理（P85）</h3><ul><li>CR性质：$P \twoheadrightarrow M \bigwedge P \twoheadrightarrow N \Rightarrow \exists T.(M \twoheadrightarrow T \bigwedge N \twoheadrightarrow T)$<ul><li>对$\twoheadrightarrow_\beta$、$\twoheadrightarrow_\eta$、$\twoheadrightarrow_{\beta \eta}$成立</li></ul></li><li>con($\lambda \beta$)：存在推不出的公式</li></ul><h3 id="3-5-不动点定理（P93）"><a href="#3-5-不动点定理（P93）" class="headerlink" title="3.5 不动点定理（P93）"></a>3.5 不动点定理（P93）</h3><ul><li>不动点定理：对于任何的$F \in \Lambda$，存在$Z \in \Lambda$，使得$FZ =_\beta Z$</li><li>不动点组合子<ul><li>$Y$： 对于任何的$F \in \Lambda$，$F(YF)=_\beta YF$</li><li>$\Theta$：对于任何的$F \in \Lambda ^ \circ$，$\Theta F \twoheadrightarrow_\beta F(\Theta F)$</li></ul></li><li>$([M_1,…,M_n])^n_i \twoheadrightarrow_\beta M_i$</li></ul><h3 id="3-6-递归函数的-lambda-可定义性（P95）"><a href="#3-6-递归函数的-lambda-可定义性（P95）" class="headerlink" title="3.6 递归函数的$\lambda-$可定义性（P95）"></a>3.6 递归函数的$\lambda-$可定义性（P95）</h3><ul><li>Church数项：$\ulcorner n \urcorner \equiv \lambda fx.f^nx$</li><li>$\lambda-$可定义性：$F \ulcorner n_1 \urcorner …  \ulcorner n_k \urcorner =_\beta  \ulcorner f(n_1,…,n_k) \urcorner$</li><li>$D\ulcorner n \urcorner MN$ = if (n = 0) then M else N​ (P97)</li><li><strong>一般/部分递归函数是$\lambda-$可定义的</strong></li></ul><h3 id="3-7-与递归论对应的结果（P100）"><a href="#3-7-与递归论对应的结果（P100）" class="headerlink" title="3.7 与递归论对应的结果（P100）"></a>3.7 与递归论对应的结果（P100）</h3><ul><li>$\lambda-$项的编码：对每个$M \in \Lambda$ ，都有唯一的自然数$\sharp M$与之对应 （$\lambda-$项 $\rightarrow$ $\mathbb{N}$）</li><li>$\lambda-$项的内部编码：$M$的内部编码定义为$\ulcorner M \urcorner \equiv \ulcorner \sharp M \urcorner$ （$\lambda-$项 $\rightarrow$ $\lambda-$项)</li><li>枚举子$E$，对于任何$M \in \Lambda^ \circ$，有$E\ulcorner M \urcorner =_\beta M$</li><li>第二不动点定理：$\forall F. \exists Z. F \ulcorner Z \urcorner =_ \beta Z$</li><li><strong>不可判定性</strong>（P102）<ul><li>若自然数集S的特征函数$\mathcal{X}_s \in \mathcal{GRF}$，则称$S$是可判定的</li><li>若$\lambda-$项集合$\mathcal{A} \subseteq  \Lambda$的编码集合是可判定的，则称$\mathcal{A}$是可判定的</li><li>设$\mathcal{A} \subseteq  \Lambda$非平凡、$\mathcal{A}$对$=_\beta$封闭，则$\mathcal{A}$不可判定</li><li>$=_\beta$关系不可判定</li><li>集合$\mathcal{N}=\{M:M有\beta-nf\}$不可判定</li></ul></li></ul><h2 id="第五章-Turing机"><a href="#第五章-Turing机" class="headerlink" title="第五章 Turing机"></a>第五章 Turing机</h2><h3 id="5-1-Turing机的形式描述（P121）"><a href="#5-1-Turing机的形式描述（P121）" class="headerlink" title="5.1 Turing机的形式描述（P121）"></a>5.1 Turing机的形式描述（P121）</h3><ul><li>Turing机定义：$M=(d,p,s)$，论域$Dom(M)$</li><li>Turing可计算的定义：存在机器M计算函数f</li><li>基本机器：<ul><li>零函数：$\boxed{Z}$</li><li>后继函数：$\boxed{S}$</li><li>投影函数：$\boxed{I}$、$\boxed{K}$、$\boxed{L}$</li><li>常数函数：$\boxed{C^k_l}$</li><li>前驱函数：$\boxed{pred}$</li><li>加法函数：$\boxed{add}$</li><li>乘法函数：$\boxed{multi}$ (习题5.3)</li><li>幂函数$2^x$：习题5.4</li><li>平方根函数：习题5.7</li></ul></li></ul><h3 id="5-2-Turing机的计算能力（P127）"><a href="#5-2-Turing机的计算能力（P127）" class="headerlink" title="5.2 Turing机的计算能力（P127）"></a>5.2 Turing机的计算能力（P127）</h3><ul><li>常用机器（P128）<ul><li>$f(x)=2x$：$\boxed{double}$</li><li>$\boxed{copy_1}$（习题5.2）、$\boxed{copy_2}$、$\boxed{copy_k}$、$\boxed{copy_k}^k$</li><li>$\boxed{compress}$</li><li>$\boxed{erase}$</li><li>$\boxed{shiftr}$、$\boxed{shiftl}$</li></ul></li><li>Turing可计算函数类对于复合算子、原始递归算子、正则$\mu-$算子封闭</li><li>若f是一般/部分递归函数，则f是Turing-可计算的</li></ul><h3 id="5-3-可判定性与停机问题（P138）"><a href="#5-3-可判定性与停机问题（P138）" class="headerlink" title="5.3 可判定性与停机问题（P138）"></a>5.3 可判定性与停机问题（P138）</h3><ul><li>可判定性：设$A \subseteq \mathbb{N}$，$A$是可判定的指$A$的特征函数$\mathcal{X}_A$是Turing-可计算的（可构造出机器）</li><li>Turing机的编码<ul><li>从#M可反向求出M</li></ul></li><li>自停机问题$K=\{\sharp M:M对于输入\overline{\sharp M}停机\}$：不可判定</li><li>停机问题$\hat{K}=\{\sharp M:M对于一切输入皆停机\}$：不可判定</li></ul><h3 id="5-4-通用Turing机（P141）"><a href="#5-4-通用Turing机（P141）" class="headerlink" title="5.4 通用Turing机（P141）"></a>5.4 通用Turing机（P141）</h3><ul><li>带位置编码</li><li>标准输入编码和解码：$\boxed{code}$、$\boxed{decode}$</li><li>计算后继带位置函数STP、TS</li><li>通用图灵机$U$的定义（P146）</li></ul><h3 id="5-5-Church-Turing论题（P147）"><a href="#5-5-Church-Turing论题（P147）" class="headerlink" title="5.5 Church-Turing论题（P147）"></a>5.5 Church-Turing论题（P147）</h3><h2 id="题型"><a href="#题型" class="headerlink" title="题型"></a>题型</h2><ul><li><p>问答题</p><ul><li>什么是配对函数组？什么是配对函数？请构造一例。（P7 定义1.10，如引理1.14构造）</li><li>什么是一般递归函数？（P42 定义1.31）</li><li>什么是部分递归函数？（P43 定义1.33）</li><li><font color="red">什么是$\lambda \beta$系统的CR性质？（P85）</font></li><li>什么是Turing 机？（P122）</li><li>什么是Church-Turing Thesis？你认可它吗？/你拥护吗？（P148）</li><li>为什么算法和Turing 机概念在可以构成“思维机器”的现代观点中占有如此核心的地位？（因为图灵机的概念为现在的思维机器观点提供了抽象模型，是现代计算机的起源。）是否在原则上存在一个算法可达到绝对极限呢？（未知，自由发挥）</li><li>什么是Halting Problem？它可判定吗？（P141）</li><li>什么是Turing 机的通用性(universality)？什么是通⽤Turing 机？(P146)</li></ul></li><li><p>判断函数类（第二大题）</p><ul><li>A：$\mathcal{EF}$<ul><li>Godel的$\beta-$函数</li><li>向下取整</li><li>$\pi,e$的十进制展开中的第n个数字</li><li>$\lambda-$项呈形…</li><li>组合数个数</li><li>数列求和</li></ul></li><li>B：$\mathcal{PRF} - \mathcal{EF}$<ul><li>形如$G(x)=2^{2^{\cdots^x}}$</li><li>Ack(5,n)</li><li>变参递归(不确定？)</li></ul></li><li>C：$\mathcal{GRF} - \mathcal{PRF}$<ul><li>Ackermann函数</li><li>Ack(m,5)</li><li>$\beta_0,\beta_{x+1}$ （不确定？）</li></ul></li><li>D：$\mathcal{RF} - \mathcal{GRF}$<ul><li>处处无定义的函数</li><li>存在无定义的函数</li></ul></li><li><font color="red">E：不可计算的数论函数类 </font><ul><li>停机问题</li><li>$M有\beta-nf$、$M =_\beta N$</li></ul></li></ul></li><li><p>证明集合$S$的可判定性</p><ul><li>证$\mathcal{X}_s \in \mathcal{GRF}$</li></ul></li><li><p>证明初等函数</p><ul><li><p>根据定义用常用函数表示（Q1.2，Q1.3，1.11，1.12）</p></li><li><p>出现根号、负数等：用已知形式表示（转化为整数、分开表示等）</p></li><li><p>取整</p><ul><li>$f(n)=\lfloor e \cdot n \rfloor$ (Q1.8)</li><li>$f(n)=\lfloor (\frac{\sqrt{5}+1}{2})^n\rfloor$ (2019 四，1.19(2) 两种方法）</li><li>$f(n)=\lfloor (\sqrt{6} +\sqrt{5})^{2n} \rfloor$ (2019+ 四)</li><li>$\lfloor n! \cdot cos(1)\rfloor$</li><li>$\lfloor n! \cdot 2^n \cdot \sqrt{e} \rfloor$</li><li>$\lfloor log_{10}n \rfloor$</li><li>$\lfloor (n+1+\frac{1}{n+1})^{n+1} \rfloor$</li></ul></li><li><font color="red">十进制展开式第n位</font><blockquote><p>step1. 泰勒展开，分开整数项和小数项，证$\lfloor n!·α\rfloor \in \mathcal{EF}$</p><p>step2. 证$\lfloor n \cdot \alpha \rfloor \in \mathcal{EF}$</p><p>step3. $f(n) = \lfloor 10^n \alpha \rfloor - \lfloor 10^{n-1}\alpha \rfloor \cdot 10  \in \mathcal{EF},(n \geq 1)$</p></blockquote><ul><li>$e$ （2019+ 七，5.18，类似Q1.8）</li><li>$\pi$（1.25，一般递归函数）</li><li>$\frac{e^2+1}{2e}$ (2019 七)</li><li>$sinh(1)=(e-e^{-1})/2$ (2018 七)，$sin(1),cos(1)$</li><li>$\sqrt{e}$</li><li>证明原始递归函数</li></ul></li><li><p>串值递归（Q1.5，Q1.6，1.15）</p></li><li>变参递归（Q1.7）</li></ul></li><li><p>对给定的数论函数$f(x)$，构造$F∈Λ^∘$其$λ−$定义$f(x)$</p><ul><li>判断奇偶函数：$l(x)=N^x(0)$ （2017 三）</li><li>add（2016 三）、$f(x,y)=x+y$ （3.16）</li><li>$f(x)=2x$ ，$f(x)=3x$（Q2.3，3.17）</li><li>$g(x)=2^x$ （Q2.7，3.20）<ul><li>倒推法、Rosser引理$\ulcorner n^m \urcorner = \ulcorner m \urcorner \ulcorner n \urcorner$</li></ul></li><li>$f(x)=\lfloor \frac{x}{2} \rfloor$</li></ul></li><li><p>在已有的公理系统中加入一个额外公理</p><ul><li>$\lambda xy.xy = \lambda xy.yx$ (2017)</li><li>$\lambda x.x = \lambda x.xxx$ (2018)</li><li>$\lambda x.x = \lambda x.xx$ （2016）</li><li>$\lambda xy.x = \lambda xy.y$ （3.13）</li><li>$\lambda xyz.x(yz)=\lambda xyz.(xy)z$</li><li>$I=K$ （2019）</li><li>$I=S$（2019+）</li></ul></li><li><p>构造图灵机</p><ul><li>$f(x)=x^3$ （2019 六）、$f(x)=x^4$（2019+ 六）</li><li>$g(x)=2^x$ （2016 六、习题5.4、Q3.5）</li><li>$f(x)=\lfloor \frac{x}{2} \rfloor$</li><li>满足给定的输入输出（2018、2017 五）</li></ul></li><li><p>求图灵机运行后的输出</p><ul><li>记得写状态、箭头</li></ul></li><li><p>证明停机问题不可判定（2018、 2017 六）</p></li></ul><h2 id="Hint"><a href="#Hint" class="headerlink" title="Hint"></a>Hint</h2><ul><li>减号写点、除号写取整</li><li>$N$, $N^2$ 表示if-else</li><li><p>Godel编码</p><ul><li>P,ep</li></ul></li><li>标准组合子$I,K,K^*,S,U^3_1,…$</li><li>$y^nz=(\lambda fx.f^nx)yz = \ulcorner n \urcorner yz$</li></ul><p><img src="/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/1.png" alt="1" style="zoom:60%;"></p><p><img src="/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/2.png" alt="2" style="zoom:70%;"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/">复习笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B/">计算模型</category>
      
      
      <comments>https://rubychen0611.github.io/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>《人生的智慧》摘抄（不断更新）</title>
      <link>https://rubychen0611.github.io/2020/09/03/%E4%BA%BA%E7%94%9F%E7%9A%84%E6%99%BA%E6%85%A7/</link>
      <guid>https://rubychen0611.github.io/2020/09/03/%E4%BA%BA%E7%94%9F%E7%9A%84%E6%99%BA%E6%85%A7/</guid>
      <pubDate>Thu, 03 Sep 2020 14:43:54 GMT</pubDate>
      
      <description>&lt;p&gt;&lt;code&gt;摆在书架上很久的书，心血来潮拿下来觉得该读一读了。这本书与“幸福论”有关，教导人们如何尽量称心、愉快地度过一生这样一门艺术。在此摘抄下一些觉得有道理的句子，以备将来不时之需。&lt;/code&gt;&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p><code>摆在书架上很久的书，心血来潮拿下来觉得该读一读了。这本书与“幸福论”有关，教导人们如何尽量称心、愉快地度过一生这样一门艺术。在此摘抄下一些觉得有道理的句子，以备将来不时之需。</code><a id="more"></a></p><h2 id="第1章-基本的划分"><a href="#第1章-基本的划分" class="headerlink" title="第1章 基本的划分"></a>第1章 基本的划分</h2><p>我认为决定凡人命运的根本差别在于三项内容，它们是：<br> （1）人的自身，即在最广泛意义上属于人的个性的东西。因此，“人的自身”包括健康、力量、外貌、气质、道德品格、精神智力及其潜在发展。<br> （2）人所拥有的身外之物，亦即财产和所有意义上的占有物。<br> （3）人向其他人所展示的样子，众所周知的就是人在其他人眼中所呈现的样子，亦即人们对它的看法。他人的看法又可分为名誉、地位和名声。</p><hr><p>每一个人到底生活在何样的世界，首先取决于这个人对这个世界的理解，这因个人头脑的差异而相应不同：是贫瘠的、浅薄的和肤浅的，抑或是丰富多彩、趣味盎然和充满意义的。例如，不少人羡慕他人在生活中发现和经历饶有趣味的事情，其实他们应该羡慕后者理解事物的禀赋才对，因为正是因为由于理解事物的禀赋，他们经历过的事情，在其描绘中才是那样耐人寻味。这是因为在一个思想丰富的人看来是意味深长的事情，由一个头脑肤浅、平庸的人理解的话，那不过是平凡世界里乏味的一幕而已。</p><hr><p>每个人都囿于自己的意识，正如每个人都囿于自己的皮囊，并且只是直接活在自己的意识之中。</p><hr><p>苏格拉底在看到摆卖的奢侈品时，说道：我不需要的东西，可真不少啊！</p><h2 id="第2章-人的自身"><a href="#第2章-人的自身" class="headerlink" title="第2章 人的自身"></a>第2章 人的自身</h2><h3 id="关于愉快心情"><a href="#关于愉快心情" class="headerlink" title="关于愉快心情"></a>关于愉快心情</h3><p>当愉快心情到来之时，我们应该敞开大门欢迎，因为它的到来永远不会不合时宜。</p><hr><p>高兴的心情直接使我们获益。它才是幸福的现金，而其他别的都只是兑现幸福的支票，因为高兴的心情在当下直接给人以愉快。所以，对于我们的生存，它是一种无与伦比的恩物，因为我们生存的真实性就体现在无法割裂的此时此刻，连接着两段无尽的时间。据此，我们应把获得和促进愉快的心情放在各种追求的首位。</p><hr><p>只需泛泛浏览一下生活，就可知道：人类幸福的两个死敌就是痛苦和无聊。还有我们成功远离了上述其中一个死敌的时候，也就在同等程度上接近了另一个死敌，反之亦然。这样，我们的生活确实就是在这两者之间或强或弱地摇摆。这是因为痛苦和无聊是处于双重的对立关系。一重是外在的，或说客体的；另一重是内在的，或说主体的。也就是说，外在的一重对立关系就是生活的艰辛和匮乏造成了痛苦，而丰裕和安定就产生了无聊。</p><h3 id="关于无聊和空虚"><a href="#关于无聊和空虚" class="headerlink" title="关于无聊和空虚"></a>关于无聊和空虚</h3><p><code>以前就听陈铭老师说过这句话，没想到出处是这里。</code></p><p><img src="/2020/09/03/%E4%BA%BA%E7%94%9F%E7%9A%84%E6%99%BA%E6%85%A7/1.jpg" alt="1" style="zoom:30%;"></p><p>内在空虚就是无聊的真正根源，这种人无时无刻不在向外面寻求刺激，试图借助某事某物使他们的精神和情绪活动起来。…能让我们免于这种痛苦的可靠手段，莫过于拥有丰富的内在，即丰富的精神思想。因为人的精神思想财富越优越和显著，留给无聊的空间就越小。这些人头脑里面的思想活泼，奔涌不息，不断更新；这些人玩味和摸索着内在世界和外部世界的多种现象；还有把这些思想进行各种组合的冲动和能力——所有这些，除了精神松弛下来的个别时候，都使卓越的头脑远离了无聊。但在另一方面，突出的智力以敏锐的感觉为直接前提，以激烈的意欲，亦即强烈的冲动和激情为根基。这些素质结合在一起极大地提高了情感的强烈程度，提高了对精神痛苦，甚至肉体痛苦的敏感性。对任何不如意的事情，甚至细微的骚动，都会感到更加不耐烦。</p><hr><p>一个人的自身拥有越丰富，他对身外之物的需求也就越少，别人对他来说就越不重要。所以，卓越的精神思想会导致一个人不喜与他人交往。…在独处的时候，每个人都返回到自身，这个人的自身拥有就会暴露无遗。</p><hr><p>愚蠢的人饱受无聊之苦。——塞内加</p><hr><p>我们大致上可以发现：一个人对与人交往的热衷程度，与他贫乏的思想和总体的平庸成正比。人们在这个世界上要么选择独处，要么选择庸俗，除此之外，再没有更多别的选择了。</p><h3 id="关于闲暇"><a href="#关于闲暇" class="headerlink" title="关于闲暇"></a>关于闲暇</h3><p>人们辛苦挣来的闲暇，就是人的一生的果实和收获，因为这闲暇让人能够自由地享受自己的意识和个性所带来的乐趣。除此闲暇以外，人的整个一生就只是辛苦和劳作而已。但闲暇给大多数人带来了什么呢？如果不是声色享受和胡闹，就是无聊和浑噩。人们消磨闲暇的方式显示出闲暇对于他们是何等的没有价值。他们的闲暇也就是阿里奥斯托所说的“一无所知者的无聊“。常人考虑的只是如何去打发时间，而略具才华的人却考虑如何利用时间。头脑思想狭隘的人容易受到无聊的侵袭，其原因就是他们的智力纯粹服务于他们的意欲，是发现动因的手段。</p><hr><p>归根到底，每个人都孑然独立，最关键的就是他到底是个什么样的人。</p><hr><p>幸福属于那些自得其乐的人。——亚里士多德</p><p>这是因为幸福和快乐的外在于安全，就其本质而言，都是机器不保险、不确定、为时短暂和受制于偶然的。因此，甚至在形势大好的情况下，这些外在源泉仍会轻易终结。的确，只要这些外在源泉不在我们的控制之下，这种情形就是不可避免的。</p><hr><p>我们这个世界乏善可陈，到处充斥着匮乏和痛苦，对于那些侥幸逃过匮乏和痛苦的人们来说，无聊却正在每个角落等待着它们。</p><hr><p>生活在这样一个世界里，一个拥有丰富内在的人，就像在冬月的晚上，在漫天冰雪当中拥有一间明亮、温暖、愉快的圣诞小屋。因此，能够拥有了优越、丰富的个性，尤其是深邃的精神思想，无疑是在这地球上得到的最大幸运。</p><h3 id="关于精神生活"><a href="#关于精神生活" class="headerlink" title="关于精神生活"></a>关于精神生活</h3><p>一般来说，每个无事可做的人都会根据自己的强项能力而挑选一种能够运用此能力的消遣。</p><hr><p>每个人都会根据自己身上所突出的或这或那的能力而选择相应的异类快乐。第一类是机体新陈代谢能力所带来的快乐。第二类是发挥肌肉力量所带来的快乐。第三类为施展感觉能力方面的快乐。…</p><p>感觉能力比人的另外两种生理能力更为优越，因为人在感觉方面的明显优势就是人优胜于动物之处，而人的另外两种生理基本能力在动物身上也同样存在，甚至超过人类。感觉能力隶属于人的认知能力，因此，卓越的感觉力使我们有能力享受到属于认知的，亦即所谓精神思想上的快乐。</p><hr><p>一个具有思想天赋的人除了个人生活之外，还过着另一种精神的生活，精神的生活逐渐成为了他的唯一目标，而个人生活只是实现自己目标的一种手段而已。但对于芸芸众生来说，只有这一浅薄、空虚和充满烦恼的存在才必须是生活的目标。精神卓越的人首要关注的是精神生活。随着他们对事物的洞察和认识持续地加深和增长，他们的精神生活获得了某种连贯性和持续提升，越来越完整和完美，就像一件逐步变得完美的艺术品。与这种精神生活相比，那种纯粹以追求个人自身安逸为目标的实际生活则显得可悲——这种生活增加的只是长度而不是深度。正如我已经说过的，这种现实生活对于大众就是目的，但对于精神卓越者而言，那只是手段而已。</p><hr><p>孤身独处正是他们求之不得的，闲暇则是至高的礼物，其他的别的一切好处都是可有可无的。</p><hr><p>虽然如此，我们却要考虑到一个具有优异思想禀赋的人由于头脑超常的神经活动，对形形色色的痛苦的感受力大大加强了。另外，他拥有这些思想禀赋的前提条件，亦即那激烈的气质，以及与此密不可分的头脑中那些更加生动、更加完美的表象，都会让透过这些表象而刺激起来的激动情绪更增加了烈度。总的来说，这些激动情绪是痛苦多于愉快。最后就是巨大的精神思想禀赋使拥有这些禀赋的人疏远了其他人及其追求。因为自身的拥有越丰富，他在别人身上所能发现得到的就越少。其他人引以为乐的、花样繁多的事情，在他眼里既乏味又浅薄。</p><h3 id="菲利斯特人"><a href="#菲利斯特人" class="headerlink" title="菲利斯特人"></a>菲利斯特人</h3><p>在这里，我们不会不提及这样一类人：由于仅仅具备了常规的、有限的智力配给，所以，他们并没有精神思想上的需求，他们也就是德语里所说的“菲利斯特人”。…菲利斯特人就是一个没有精神需求的人。</p><hr><p>对这种人来说，真正的快乐只能是感官上的快乐，他们就通过这些补偿自己。</p><hr><p>在与他人的交往中，他们会寻求那些能满足自己生理上的需要，而不是精神上的需求的人。因此，在他们对别人的诸多要求中，最不重要的就是别人必须具备一定的头脑思想。</p><hr><p>菲利斯特人的巨大痛苦就是在于任何观念性的东西都无法带给他们愉快。为了逃避无聊，他们不断需要现实性的东西。但由于现实性的东西一来很快就会被穷尽，一旦这样，它们不但不再提供快乐，反而会使人厌烦；二来还会带来各种祸殃。相比较而言，观念性的东西却是不可穷尽的，它们本身既无邪也乌海。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E5%8F%94%E6%9C%AC%E5%8D%8E/">叔本华</category>
      
      
      <comments>https://rubychen0611.github.io/2020/09/03/%E4%BA%BA%E7%94%9F%E7%9A%84%E6%99%BA%E6%85%A7/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】Towards Improved Testing For Deep Learning</title>
      <link>https://rubychen0611.github.io/2020/08/26/Towards-Improved-Testing-For-Deep-Learning/</link>
      <guid>https://rubychen0611.github.io/2020/08/26/Towards-Improved-Testing-For-Deep-Learning/</guid>
      <pubDate>Wed, 26 Aug 2020 08:41:16 GMT</pubDate>
      
      <description>&lt;p&gt;原文：Towards Improved Testing For Deep Learning（ICSE-NIER’19）&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>原文：Towards Improved Testing For Deep Learning（ICSE-NIER’19）<a id="more"></a></p><h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>提出了一种2-路覆盖标准：每个神经元单独对下一层神经元值的条件影响以及上一层中神经元值的组合对下一层神经元值的影响。</p><h2 id="当前领域的提升空间"><a href="#当前领域的提升空间" class="headerlink" title="当前领域的提升空间"></a>当前领域的提升空间</h2><h3 id="目前覆盖方法的局限性"><a href="#目前覆盖方法的局限性" class="headerlink" title="目前覆盖方法的局限性"></a>目前覆盖方法的局限性</h3><ul><li>DeepXplore：神经元覆盖率粗糙、不充分</li><li>DeepCover：DeepCover的覆盖标准考虑了DNN相邻层中的条件判定依赖性。该方法除了时在相对较小的网络上测试之外，它的前提条件还要求DNN是前馈全连接的神经网络。而且它不能推广到诸如RNN、LSTM、attention网络等的架构。DeepCover不考虑神经元在其所在层的环境，即同一层中神经元输出的组合。</li><li>DeepCT：DeepCT的组合测试是启发于覆盖标准：根据每层激活的神经元比值描述测试输入所使用的逻辑比值。它没有考虑DNN中的层级之间的关系，并且也没有被证实可以扩展到具有不同类型层的、能用于真实世界的DNN</li></ul><h3 id="目前测试输入生成方法的局限性"><a href="#目前测试输入生成方法的局限性" class="headerlink" title="目前测试输入生成方法的局限性"></a>目前测试输入生成方法的局限性</h3><p>测试输入可以通过指导方式生成或选择得到，它通常有两个主要目标：最大化未覆盖故障的数量，并最大化覆盖范围。 目前测试输入生成方法存在一些主要缺点：</p><ul><li>修改现有测试输入直到找到满足标准的测试输入的迭代过程单次执行耗时长。</li><li>与总的测试和生成的输入数量相比，那些能够导致覆盖范围和/或发现的角落案例增加的测试输入数量相当低。</li></ul><h3 id="Oracle选择方面的局限性"><a href="#Oracle选择方面的局限性" class="headerlink" title="Oracle选择方面的局限性"></a>Oracle选择方面的局限性</h3><ul><li>最直接的方法是收集尽可能多的实际数据并手动标记以检查其是否正确。但是，这样的过程需要大量的手动工作。</li><li>在某些工作中，会使用同一任务的多个实现作为oracle，并将其中的的差异行为标记为角落案例行为。然而，我们观察到这种方法会错误地将某些角落情况分类为正确的行为，此外，此方法仅在具有多个高精度且相似的实现的应用程序中有效。</li></ul><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p>在本文中，作者基于对内部决策逻辑的覆盖提出了一个包含两个因子的覆盖标准:  DNN中的每个三元组$(n_{i,k-1}, n_{j,k-1}, n_{q,k})$的组合覆盖情况</p><font color="red">（覆盖率居然没有形式化定义？？）</font><p>解释：</p><ul><li>每个神经元单独对下一层神经元值的条件影响<ul><li>受MC/DC启发</li></ul></li><li>上一层中神经元值的组合对下一层神经元值的影响<ul><li>受组合测试启发</li></ul></li></ul><p>对于一个初始测试输入的结果，我们通过联合优化来实现指导测试输入的生成。任何没有达到100%覆盖率的三元组被随机选择，以确定哪些激活值的组合没有被覆盖。比如若对一个DNN三元组激活实例：$n_{i,k-1}$激活, $n_{j,k-1}$不激活，$n_{q,k}$激活，优化目标为：（直接将三者相加）</p><p>$ F_{n,t}=f_{n_{i,k-1}}(t)+f_{n_{j,k-1}}(t)+f_{n_{q,k}}(t) $ </p><p>通过这种迭代修改输入的方式最大化目标函数，以达到期望的三元组某种激活状态，从而覆盖三元组不同的激活状态，达到100%覆盖。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul><li>数据集和模型：MNIST上的三个DNN——LeNet-1，LeNet-4和LeNet-5</li><li>覆盖指标：<ul><li>10个随机测试输入获得的覆盖率（理想情况下应该很低）</li><li>边角案例数与总测试输入数的比率</li></ul></li><li><p>借鉴DeepXplore，我们使用<strong>多种实现</strong>作为oracle，只有一种图像处理——<strong>亮度</strong>。</p></li><li><p>实验结果</p><ul><li><p>结果显示，前面工作在相同的指标下获得了更高的覆盖率和对比值，而使用作者提出的标准时覆盖率直线下降，体现出作者提出的标准较于先前工作粒度更细。</p><p><img src="/2020/08/26/Towards-Improved-Testing-For-Deep-Learning/Fig1.png" alt="Fig1" style="zoom:60%;"></p></li></ul></li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/ChapterZ/article/details/96116870">https://blog.csdn.net/ChapterZ/article/details/96116870</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/DNN%E6%B5%8B%E8%AF%95/">DNN测试</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E6%B5%8B%E8%AF%95%E6%A0%87%E5%87%86/">测试标准</category>
      
      
      <comments>https://rubychen0611.github.io/2020/08/26/Towards-Improved-Testing-For-Deep-Learning/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】DeepGauge</title>
      <link>https://rubychen0611.github.io/2020/08/26/DeepGauge/</link>
      <guid>https://rubychen0611.github.io/2020/08/26/DeepGauge/</guid>
      <pubDate>Wed, 26 Aug 2020 01:44:37 GMT</pubDate>
      
      <description>&lt;p&gt;原文：DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems （ASE’18）&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>原文：DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems （ASE’18）<a id="more"></a></p><p>介绍网址：<a href="https://deepgauge.github.io/">https://deepgauge.github.io/</a></p><h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>提出基于深度神经网络的主功能区、边界区、层级三类覆盖率标准。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>$\phi (x,n)$：输入$x$在神经元$n$上的输出值</p><h3 id="神经元级别的覆盖率"><a href="#神经元级别的覆盖率" class="headerlink" title="神经元级别的覆盖率"></a>神经元级别的覆盖率</h3><ul><li><p><strong>K-multisection Neuron Coverage (KMNC)</strong></p><ul><li><p>主功能区：设一个神经元$n$在训练集的下界为$low_n$，上界为$high_n$，主功能区为$[low_n,high_n]$</p></li><li><p>将主功能区$[low_n,high_n]$均分为$k$等份，每份为$S_i^n$，则该神经元在测试集$T$上的覆盖率：</p><script type="math/tex; mode=display">\frac{ \left\{ S_{i}^{n}| \exists x \in T: \phi (x,n) \in S_{i}^{n} \right\} )}{k}</script></li><li><p>对所有神经元，KMNC定义为：（即所有神经元取平均）</p><p>$ KMNCov(T,k)= \frac{ \sum _{n \in N}| \left\{ S_{i}^{n}| \exists x \in T: \phi (x,n) \in S_{i}^{n} \right\} |}{k \times |N|}$ </p></li></ul></li><li><p><strong>Neuron Boundary Coverage（NBC）</strong></p><ul><li><p>边界区：$ (- \infty , low_n) \cup (high_n,+ \infty ) $ </p></li><li><p>NBC定义为所有神经元边缘被覆盖的比例：</p><script type="math/tex; mode=display">NBCov(T)= \frac{|UpperCornerNeuron|+|LowerCornerNeuron| }{2 \times |N|}</script></li></ul></li><li><p><strong>Strong Neuron Activation Coverage (SNAC)</strong></p><ul><li><p>这些极度活跃的神经元可能在神经网络中传递有用的学习模式</p></li><li><p>SNAC只计算上边界覆盖率：</p><script type="math/tex; mode=display">SNACov(T)= \frac{|UpperCornerNeuron|}{|N|}</script></li></ul></li></ul><h3 id="层级别的覆盖率"><a href="#层级别的覆盖率" class="headerlink" title="层级别的覆盖率"></a>层级别的覆盖率</h3><ul><li><p><strong>Top-k Neuron Coverage (TKNC)</strong></p><ul><li><p>表示所有神经元中有多少曾经做过top-k（存在某个$x$使得其激活值在该层属于top-k）</p><p>$ TKNCov(T,k)= \frac{|U_{x \in T}(U_{1 \leqslant i \leqslant 1}top_{k}(x,i))|}{|N|} $ </p></li></ul></li><li><p><strong>Top-k Neuron Patterns</strong></p><ul><li><p>给定一个测试输入$x$，每一层的top-k神经元序列也形成一个模式:</p><script type="math/tex; mode=display">TKNPat(T,k)=| \left\{ (top_{k}(x,1), \ldots ,top_{k}(x,l))|x \in T \right\} |</script></li></ul></li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul><li>数据集和模型</li></ul><p><img src="/2020/08/26/DeepGauge/1.png" alt="1" style="zoom:80%;"></p><ul><li><p>对抗样本生成算法</p><ul><li>FGSM、BIM、JSMA、CW</li></ul></li><li><p>实验步骤</p><ul><li><p>对于MNIST：</p><ul><li><p>对于每个模型，生成10000张对抗样本和原10000张测试集图片混合在一起</p></li><li><p>参数设置</p><ul><li>$\sigma$:方差</li></ul><p><img src="/2020/08/26/DeepGauge/2.png" alt="2" style="zoom:80%;"></p></li><li><p>总共：3 (models)×5 (datasets)×14 (criterion settings) = 210 evaluation configurations</p></li></ul></li><li><p>对于ImageNet：</p><ul><li>随机选择5000张测试图片</li><li>总共：2 (models)×4 (datasets)×14 (criterion settings) = 112 experimental configurations<ul><li>（JSMA因为开销问题无法运行，dataset少一个）</li></ul></li></ul></li></ul></li></ul><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul><li>覆盖率的增加表明对抗样本测试数据总体上探索了新的DNNs的内部状态，其中一些未被原始测试覆盖。</li><li>主功能区和边界区均可能出错</li><li>覆盖率提升意味着错误检测能力提升</li><li>测试数据更多覆盖主功能区域</li><li>低边界区比高边界区更难覆盖</li></ul><h3 id="与神经元覆盖率（NC）比较"><a href="#与神经元覆盖率（NC）比较" class="headerlink" title="与神经元覆盖率（NC）比较"></a>与神经元覆盖率（NC）比较</h3><ul><li>NC难以捕捉对抗样本和原测试集样本的区别</li><li>NC使用<strong>相同的阈值</strong>作为所有神经元的激活评价。但是，我们发现不同神经元的输出统计分布差异很大。给定一个用于分析的测试套件，一些神经元的输出可能表现出相当小的方差，但平均值很大，而另一些神经元可能表现出很大的方差，但平均值很低。<br>因此，对所有神经元使用相同的阈值而不考虑神经元功能分布的差异会大大降低精度。例如，给定一个具有非常小的平均值和标准偏差的神经元，即使用户指定的阈值稍微大一点，通常也会确定该神经元不能被覆盖。</li><li>NC对神经元取值进行了<strong>标准化（归约到[0,1])</strong>，所以相同的激活值在不同数据集代表了不同的意义（因为每个数据集的max和min不同）。</li></ul><p><img src="/2020/08/26/DeepGauge/3.png" alt="3" style="zoom:80%;"></p><h2 id="可控制变量及参数"><a href="#可控制变量及参数" class="headerlink" title="可控制变量及参数"></a>可控制变量及参数</h2><ul><li>KMNC和TKNC的$k$</li><li>NBC和SNAC可选增加参数$\sigma$(在分析过程中神经元输出的标准方差)</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/DNN%E6%B5%8B%E8%AF%95/">DNN测试</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E6%B5%8B%E8%AF%95%E6%A0%87%E5%87%86/">测试标准</category>
      
      
      <comments>https://rubychen0611.github.io/2020/08/26/DeepGauge/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】Surprise Adequacy</title>
      <link>https://rubychen0611.github.io/2020/08/25/Surprise-Adequacy/</link>
      <guid>https://rubychen0611.github.io/2020/08/25/Surprise-Adequacy/</guid>
      <pubDate>Tue, 25 Aug 2020 01:27:28 GMT</pubDate>
      
      <description>&lt;p&gt;原文：Guiding Deep Learning System Testing using Surprise Adequacy (ICSE‘19)&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>原文：Guiding Deep Learning System Testing using Surprise Adequacy (ICSE‘19)  <a id="more"></a></p><p>代码地址：<a href="https://github.com/coinse/sadl">https://github.com/coinse/sadl</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>提出了一个叫Surprise Adequacy的指标，对一个给定的测试用例，基于可能性或距离来衡量其激活模式相对于DNN训练集的新颖程度。实验部分较详细。</p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>已有的覆盖方法：</p><ul><li>无法针对<strong>单个</strong>输入提供信息：比如更高神经元覆盖率的输入是否比更低的好？</li><li>评估的重点是显示对抗样本和提出的标准之间的相关性，而不是评估和指导它们<strong>在实际测试DL系统时的使用</strong>。</li></ul><p>直观地说，DL系统的一个好的测试输入集应该是系统多样化的，包括从类似于训练数据的输入、到明显不同和对抗的输入。在单个样本的粒度上，SADL测量输入对于训练系统的数据对DL系统的惊讶程度:</p><ul><li>基于系统看到类似的输入的可能性在训练</li><li>或基于神经元激活向量之间的距离</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>设神经元集合为$N$，训练集为$T$，两种方式衡量测试输入$x$与训练集神经元激活向量$A_N(T)$之间的相似程度</p><h3 id="基于可能性的SA（LSA）"><a href="#基于可能性的SA（LSA）" class="headerlink" title="基于可能性的SA（LSA）"></a>基于可能性的SA（LSA）</h3><p>采用核密度估计（KDE）来获得输入数据的分布密度函数，这里使用高斯核函数</p><p>在这种方法的时候,为了减少计算量,有如下两种规则</p><ul><li><p>只选定特定的某层</p></li><li><p>方差过滤：过滤掉那些激活值的方差小于预先定义的阈值$t$的神经元</p></li></ul><p>密度函数定义：</p><script type="math/tex; mode=display">\widehat{f}(x)= \frac{1}{|A_{N_L}(T)|} \sum _{x_{i} \in T}K_{H}( \alpha _{N_{L}}(x)- \alpha _{N_{L}}(x_{i}))</script><p>公式的直观理解：对于所有的训练集中的用例，每个用例使用高斯核函数计算该用例与新输入x的激活迹的差值。概率密度低说明输入更加稀有，概率密度高说明输入更加相似。</p><p>LSA定义：</p><script type="math/tex; mode=display">LSA(x)=- \log ( \widehat{f}(x))</script><p>实际应用中只使用类别$c$的训练集数据$T_c$计算LSA。</p><h3 id="基于距离的SA（DSA）"><a href="#基于距离的SA（DSA）" class="headerlink" title="基于距离的SA（DSA）"></a>基于距离的SA（DSA）</h3><p><img src="/2020/08/25/Surprise-Adequacy/Fig1.png" alt="Fig1" style="zoom:60%;"></p><ul><li><p>对于输入样本$x$，其预测标签为$c_x$，设$x_a$为与其标签相同且激活向量距离最近的训练样本，$x_b$为激活向量离$x_a$最近且类别不同的训练样本，DSA定义为：</p><script type="math/tex; mode=display">DSA(x)= \frac{dist_{a}}{dist_{b}}</script></li><li><p>DSA越大，说明$x$对于类别$c_x$来说越surprise</p></li><li><p>DSA 只适用于分类任务</p></li></ul><h3 id="意外覆盖率的计算（SC"><a href="#意外覆盖率的计算（SC" class="headerlink" title="意外覆盖率的计算（SC)"></a>意外覆盖率的计算（SC)</h3><p>因为LSA和DSA取值都是连续的，我们用被覆盖的段数除以总段数来表示覆盖率：</p><p>给定上界$U$，将$(0,U]$分成n个SA段的bucket：$B=\{b_1,b_2,…,b_n\}$，一组输入$X$的SC定义如下：</p><script type="math/tex; mode=display">SC(X)= \frac{| \{ b_{i}| \exists x \in X:SA(x) \in \left[ U \cdot \frac{i-1}{n},U \cdot \frac{i}{n} \right] \}|}{n}</script><p>注意：一组具有较高SC的输入应该是一组多样化的输入。然而，具有特别高SA值的输入可能<strong>与问题域无关</strong>(如交通标志的图像将与动物图片分类器的测试无关)。因此，SC只能相对于<strong>预定义的上界</strong>来测量。<font color="red">（提前设置好的上界很重要）</font></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验配置"><a href="#实验配置" class="headerlink" title="实验配置"></a>实验配置</h3><ul><li>数据集和模型</li></ul><p><img src="/2020/08/25/Surprise-Adequacy/Fig2.png" alt="Fig2"></p><ul><li><p>对抗样本生成方法</p><ul><li>FGSM</li><li>BIM-A、BIM-B</li><li>JSMA</li><li>C&amp;W</li></ul></li><li><p>合成图像生成方法（Driving数据集）</p><ul><li>Dave2模型：DeepXplore的输入生成（亮度、矩形、污点）和联合优化方法（3个DNN：Dave-2、Dave-dropout、Dave-norminit）</li><li>Chauffeur模型：DeepTest输入生成方法（translation, scale, shear, rotation, contrast, brightness, and blur）</li></ul></li><li><p>参数设置</p><ul><li><p>LSA默认方差阈值：$10^{-5}$</p></li><li><p>kde的带宽使用scott规则设置</p></li><li><p>RQ1</p><ul><li>MNIST选择activation_2层，CIFAR-10选择activation_6层</li></ul></li><li><p>RQ2</p><ul><li>CIFAR10的activation_7和activation_8层的LSA默认方差阈值设置为$10^{-4}$（减少计算消耗）</li></ul></li><li><p>RQ3</p><p><img src="/2020/08/25/Surprise-Adequacy/Fig3.png" alt="Fig3" style="zoom:75%;"></p></li><li><p>RQ4</p><ul><li>MNIST选择activation_3层，CIFAR-10选择activation_5层</li><li>每次重新练运行20次</li></ul></li></ul></li></ul><h3 id="RQ1-SADL是否能够捕获DL系统输入的相对惊讶程度"><a href="#RQ1-SADL是否能够捕获DL系统输入的相对惊讶程度" class="headerlink" title="RQ1 SADL是否能够捕获DL系统输入的相对惊讶程度?"></a>RQ1 SADL是否能够捕获DL系统输入的相对惊讶程度?</h3><ul><li><p>实验方法</p><ul><li>从原始数据集检查是否SA越高的样本越难被分类正确</li><li>检查对抗样本是否SA较高</li><li>最后，我们使用逻辑回归对SA值进行对抗样本分类训练。对于每个敌对攻击策略，我们使用MNIST和CIFAR-10提供的10,000张原始测试图像生成10,000个对抗样本。使用随机选取的1,000张原始测试图像和1,000个对抗样本，我们训练<strong>logistic回归分类器</strong>。<font color="red">(标签是如何确定的？）</font>最后，我们使用剩余的9000幅原始测试图像和9000个敌对例子来评估训练过的分类器。如果SA值正确地捕获DL系统的行为，我们期望基于SA的分类器能够成功地检测出对抗样本。我们使用ROC-AUC进行评估。</li></ul></li><li><p>实验结果</p><ul><li><p>红点对应的图像集合(Ascending SA)从最小SA开始，随着SA的上升，越来越多的高SA</p></li><li><p>蓝点对应的图像组在相反的方向生长(即从SA高的图像到SA低的图像)。</p></li><li><p>作为参考，绿点表示在20次重复中随机增长的集合的平均精度。</p></li><li><p>可以看出，包含LSA值较高的图像，即更多令人惊讶的图像，会导致精度较低。</p><p><img src="/2020/08/25/Surprise-Adequacy/Fig4.png" alt="Fig4"></p></li><li><p>为了在另一个数据集上进行视觉确认，我们也选择了DeepTest为Chauffeur从三个不同级别的LSA值合成的输入集:图3显示，LSA值越高，图像视觉识别就越困难。无论是从数量上还是从视觉上，观察到的趋势都支持了我们的声明，即SADL捕获输入的意外:即使对于看不见的输入，SA也可以度量给定输入的意外程度，这与DL系统的性能直接相关。</p></li><li><p><img src="/2020/08/25/Surprise-Adequacy/Fig5.png" alt="Fig5" style="zoom:60%;"></p></li><li><p>图4显示了由五种技术中的每一种生成的10,000个对抗样本的DSA值的排序图，以及原始的测试输入：</p><p><img src="/2020/08/25/Surprise-Adequacy/Fig6.png" alt="Fig6" style="zoom:80%;"></p></li><li><p>图5是在MNIST和cifar-10的<strong>不同层</strong>中随机选取2000个对抗样本和原始测试集的LSA值的相似图。对于MNIST和cifar10，数据集提供的测试输入(用蓝色表示)往往是最不令人吃惊的，而大多数对抗样本通过较高的SA值与测试输入明显分开。这支持了我们的说法，即SADL可以捕获敌对示例中DL系统行为的差异。</p><p><img src="/2020/08/25/Surprise-Adequacy/Fig7.png" alt="Fig7"></p></li><li><p>最后，表III给出了MNIST和CIFAR-10中使用所有神经元进行基于DSA的分类的ROC-AUC结果。结果表明，图4中DSA值的差距可以用于对敌对的例子进行高精度的分类。</p></li><li><p>对于相对简单的MNIST模型，分类器的ROC-AUC范围在96.97% - 99.38%之间，可以检测出对抗样本。</p></li><li><p>对于更复杂的CIFAR-10模型，基于DSA的分类显示较低的ROC-AUC值，但RQ2的回答表明，来自特定层次的DSA可以产生明显更高的精度。</p><p><img src="/2020/08/25/Surprise-Adequacy/Fig8.png" alt="Fig8" style="zoom:80%;"></p></li><li><p><strong>基于三种不同的分析，RQ1的答案是SADL可以捕获输入的相对惊喜。高SA的输入更难正确分类;对抗性实例的SA值较高，可以根据SA进行相应的分类</strong>。</p></li></ul></li></ul><h3 id="RQ2：神经元层的选择是否对SA反映DL系统行为的准确性有任何影响"><a href="#RQ2：神经元层的选择是否对SA反映DL系统行为的准确性有任何影响" class="headerlink" title="RQ2：神经元层的选择是否对SA反映DL系统行为的准确性有任何影响?"></a>RQ2：神经元层的选择是否对SA反映DL系统行为的准确性有任何影响?</h3><ul><li><p>实验方法：我们通过计算各层的LSA和DSA，然后通过比较各层在SA上训练的对抗性例分类器来评估在SA上下文中的假设。</p></li><li><p>MNIST实验结果</p><ul><li><p>表IV给出了对敌示例分类的ROC-AUC，结果每一行分别对应MNIST中特定层的LSA和DSA上训练的分类器。行按其深度排序，即，activation_3是MNIST中最深也是最后一个隐藏层。每种攻击策略的最高ROC-AUC值以粗体显示。<font color="red">对于MNIST来说，没有明确的证据表明最深的一层是最有效的。</font></p><p><img src="/2020/08/25/Surprise-Adequacy/Fig9.png" alt="Fig9" style="zoom:80%;"></p><ul><li>图5可以解释ROC-AUC是100%的情况：MNIST activation_1对抗样本和测试集里的样本曲线清晰地分离。而MNIST activation_3的LSA曲线有很多交叉。<font color="red">（这个结果比较反直觉）</font></li></ul></li></ul></li><li><p>CIFAR-10实验结果</p><p><img src="/2020/08/25/Surprise-Adequacy/Fig10.png" alt="Fig10" style="zoom:75%;"></p><ul><li>对于LSA，没有强有力的证据表明最深的层产生最准确的分类器。</li><li>然而，对于DSA，最深层次为五种攻击策略中的三种(BIM-B、JSMA和C&amp;W)生成最准确的分类器，而第二层为BIM-A生成最准确的分类器。更重要的是，<font color="red"><strong>单层</strong>DSA比所有<strong>神经</strong>元DSA值产生的分类结果要准确得多</font>（从表III与表IV和表v的对比可以看出）。</li></ul></li><li><p>总结：DSA对其计算的层的选择很敏感，选择较深的层是有益的。然而，对于LSA，没有明确的证据支持更深层次的假设。不同的实例生成策略的层敏感性不同。</p></li></ul><h3 id="RQ3：SC是否与DL系统的现有覆盖率标准存在相关性"><a href="#RQ3：SC是否与DL系统的现有覆盖率标准存在相关性" class="headerlink" title="RQ3：SC是否与DL系统的现有覆盖率标准存在相关性?"></a>RQ3：SC是否与DL系统的现有覆盖率标准存在相关性?</h3><ul><li><p>实验方法</p><ul><li>我们通过累计添加输入控制输入的多样性，执行DL与这些输入，系统研究和比较各种覆盖标准的观察到的变化。包括SC和四个现有的标准:NC、KMNC、NBC、SNAC。</li><li>对于MNIST和cifar-10，我们从数据集提供的原始测试数据(10000幅图像)开始，每一步添加由FGSM、BIM-A、BIM-B、JSMA和C&amp;W生成的1000个对抗性示例。</li><li>对于Dave-2，我们从原始测试数据(5614张图像)开始，在每一步添加由DeepXplore生成的700张合成图像。</li><li>对于Chauffeur来说，每一步增加1000张合成图像(Set1到Set3)，每一张图像都是通过应用随机数量DeepTest的变换生成的。</li></ul></li><li><p>实验结果</p><ul><li><p>表6显示了不同的覆盖率标准如何对日益增加的多样性水平作出反应。列表示步骤，在每一个步骤中会向原始测试集添加更多的输入。如果与前一个步骤相比，<strong>步骤中覆盖率的增加小于0.1个百分点，则该值加下划线</strong>。</p><p><img src="/2020/08/25/Surprise-Adequacy/Fig11.png" alt="Fig11" style="zoom:70%;"></p></li><li><p>图6显示了来自CIFAR-10和Chauffeur的结果的可视化。(注意DSC不能为这两个DL系统计算，因为它们不是分类器。)</p><p><img src="/2020/08/25/Surprise-Adequacy/Fig12.png" alt="Fig12" style="zoom:60%;"></p></li><li><p>总的来说，大多数研究的标准会随着每一步的额外输入而增加。<strong>值得注意的例外是NC，它会在许多步骤中停滞不前。</strong>这与DeepGauge的结果一致。</p></li><li><p>添加的输入类型与不同标准的响应之间存在相互作用:SNAC、KMNC和NBC在CIFAR-10中加入BIM-B示例时显著增加，但在添加C&amp;W输入时变化不大。但是，对于Chauffeur，只有SNAC和NBC在增加输入集1时表现出类似的增长，而KMNC的增长更为稳定。</p></li><li><p>总的来说，除了NC之外，我们回答了RQ3, SC与到目前为止引入的其他覆盖率标准相关。</p></li></ul></li></ul><h3 id="RQ4：SA能否指导DL系统的再训练，以提高它们在对抗样本和由DeepXplore生成的合成测试输入时的准确性"><a href="#RQ4：SA能否指导DL系统的再训练，以提高它们在对抗样本和由DeepXplore生成的合成测试输入时的准确性" class="headerlink" title="RQ4：SA能否指导DL系统的再训练，以提高它们在对抗样本和由DeepXplore生成的合成测试输入时的准确性?"></a>RQ4：SA能否指导DL系统的再训练，以提高它们在对抗样本和由DeepXplore生成的合成测试输入时的准确性?</h3><ul><li><p>实验方法：</p><ul><li>我们检查SA是否可以指导额外训练输入的选择。从这些模型的对抗样本和合成输入中，我们从四个不同的SA范围中选择了4组100张图像。已知$U$为RQ3中用于计算SC的上界，我们将SA $[0, U]$的范围划分为四个重叠的子集: 第一个子集包括低25%的SA值($[0,\frac{U}{4}]$)，第二个子集包括低25%的SA值($[0,\frac{2U}{4}]$)，第三个子集包括低75%的SA值($[0,\frac{3U}{4}]$)，最后是整个范围$[0,U]$。这四个子集代表越来越多样化的投入。我们将范围R设为这四个中的一个，从每个R中随机抽取100张图像，并对现有的模型进行5个额外的epoch训练。最后，我们分别测量每个模型在整个敌对和合成输入下的性能(MNIST和cifar-10的精度，Dave-2的MSE)。我们期望用更多样化的子集进行再培训将会带来更高的性能。</li></ul></li><li><p>实验结果</p><p><img src="/2020/08/25/Surprise-Adequacy/Fig13.png" alt="Fig13" style="zoom:80%;"></p><ul><li>虽然我们的观察局限于DL系统和这里研究的输入生成技术，我们回答RQ4, SA可以提供指导，更有效的再训练DL系统。</li></ul></li></ul><h2 id="可控制参数-变量"><a href="#可控制参数-变量" class="headerlink" title="可控制参数/变量"></a>可控制参数/变量</h2><ul><li>选择神经元的层（单层/所有层）</li><li>过滤神经元的方差阈值</li><li>SA预定义的上界$U$</li><li>覆盖率计算式bucket个数（划分的段数）</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/qq_33935895/article/details/101155270">https://blog.csdn.net/qq_33935895/article/details/101155270</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/DNN%E6%B5%8B%E8%AF%95/">DNN测试</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E6%B5%8B%E8%AF%95%E6%A0%87%E5%87%86/">测试标准</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E8%BE%93%E5%85%A5%E9%AA%8C%E8%AF%81/">输入验证</category>
      
      
      <comments>https://rubychen0611.github.io/2020/08/25/Surprise-Adequacy/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】DeepCover</title>
      <link>https://rubychen0611.github.io/2020/08/24/DeepCover/</link>
      <guid>https://rubychen0611.github.io/2020/08/24/DeepCover/</guid>
      <pubDate>Mon, 24 Aug 2020 07:18:05 GMT</pubDate>
      
      <description>&lt;p&gt;原文：Testing Deep Neural Networks （TECS’19）&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>原文：Testing Deep Neural Networks （TECS’19）<a id="more"></a></p><p>（与DeepConcolic作者相同）</p><p>代码地址：<a href="https://github.com/TrustAI/DeepCover">https://github.com/TrustAI/DeepCover</a></p><h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>受MC/DC思想启发提出4种覆盖标准，使用线性规划模型进行约束求解（借助DeepConcolic的方法），生成满足“独立影响”条件且变化较小的测试用例。</p><h2 id="传统MC-DC覆盖"><a href="#传统MC-DC覆盖" class="headerlink" title="传统MC/DC覆盖"></a>传统MC/DC覆盖</h2><p>MC/DC是DO-178B Level A认证标准中规定的，欧美民用航空器强制要求遵守该标准。MC/DC覆盖测试<font color="red">在每个判定中的每个条件都曾独立影响判定的结果至少一次（独立影响意思是在其他条件不变的情况下，改变一个条件）</font>。</p><p>举个例子，制作咖啡需要同时满足壶、杯子和咖啡豆的条件：</p><figure class="highlight c"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>( kettle &amp;&amp; cup &amp;&amp; coffee ) {</span><br><span class="line">  <span class="keyword">return</span> cup_of_coffee;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">else</span> {</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>三个条件的取值共有8种情况：</p><p><img src="/2020/08/24/DeepCover/Fig2.png" alt="Fig2" style="zoom:80%;"></p><p>但仅4种情况（Test 4、6、7、8）就可以达到100%MC/DC覆盖率，因为：</p><ul><li>Tests 4 &amp; 8 ：Kettle可以独立影响结果</li><li>Tests 6 &amp; 8 ：Mug可以独立影响结果</li><li>Tests 7 &amp; 8：Coffe可以独立影响结果</li></ul><p><img src="/2020/08/24/DeepCover/Fig1.jpg" alt="Fig1" style="zoom:30%;"></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="DNN中的决策和条件"><a href="#DNN中的决策和条件" class="headerlink" title="DNN中的决策和条件"></a>DNN中的决策和条件</h3><ul><li>$\Psi_k$: 一个集合，其中每个元素是神经网络第$k$层节点的一个子集合，表示一个<strong>特征</strong><ul><li>核心思想：不仅要测试某个特征的存在，而且要测试简单特征对更复杂特性的影响。</li></ul></li><li>$t_k = |\Psi_k|$：特征个数</li><li>$\psi_{k,l} (1 \leq l \leq t_k)$：第$l$个特征</li><li>每个特征代表一个<strong>决策（decision）</strong>，其<strong>条件（conditions）</strong>是前一层与其相连的特征</li><li><p>特征的使用将DNN中的基本构建单位从<strong>单个节点</strong>推广到<strong>一组节点</strong>。</p></li><li><p>特征对$(\psi_{k,i},\psi_{k+1,j})$：相邻层的一对特征</p></li><li>符号变化<ul><li>$sc(\psi_{k,l},x_1,x_2)$：对$\psi_{k,l}$中的任意神经元 $n_{k,j}$，$sign(n_{k,j},x_1)\ne sign(n_{k,j},x_2)$</li><li>$nsc(\psi_{k,l},x_1,x_2)$：对$\psi_{k,l}$中的任意神经元 $n_{k,j}$，$sign(n_{k,j},x_1) = sign(n_{k,j},x_2)$</li></ul></li><li>值变化<ul><li>$vc(g,\psi_{k,l},x_1,x_2)$：$g(\psi_{k,l},x_1,x_2)=true$，$g$是一个值函数</li></ul></li></ul><h3 id="覆盖方法"><a href="#覆盖方法" class="headerlink" title="覆盖方法"></a>覆盖方法</h3><ul><li><p>Sign-Sign Coverage (SSC)</p><ul><li><p>对一个特征对$\alpha = (\psi_{k,i},\psi_{k+1,j})$，$\alpha$被两个测试用例$x_1$、$x_2$SS-覆盖，记为$SS(\alpha,x_1,x_1)$。其定义为：</p><ul><li>$sc(\psi_{k,i},x_1,x_2)$ 且$nsc(P_k  \backslash \psi_{k,i},x_1,x_2)$，其中$P_k$为第$k$层所有节点的集合</li><li>$sc(\psi_{k+1,j},x_1,x_2)$</li></ul><font color="red">（即一个条件特征量$\psi_{k,i}$的符号变化独立影响下一层决策特征$\psi_{k+1,j}的$符号变化)</font></li></ul></li><li><p>Value-Sign Coverage (VSC)</p><ul><li>对一个特征对$\alpha = (\psi_{k,i},\psi_{k+1,j})$，值函数$g$，$\alpha$被两个测试用例$x_1$、$x_2$VS-覆盖，记为$VS^g(\alpha,x_1,x_1)$。其定义为：<ul><li>$vc(g,\psi_{k,i},x_1,x_2)$ 且$nsc(P_k ,x_1,x_2)$，其中$P_k$为第$k$层所有节点的集合</li><li>$sc(\psi_{k+1,j},x_1,x_2)$</li></ul></li></ul></li><li><p>Sign-Value Coverage (SVC)</p><ul><li><p>对一个特征对$\alpha = (\psi_{k,i},\psi_{k+1,j})$，值函数$g$，$\alpha$被两个测试用例$x_1$、$x_2$SV-覆盖，记为$SV^g(\alpha,x_1,x_1)$。其定义为：</p><ul><li>$sc(\psi_{k,i},x_1,x_2)$ 且$nsc(P_k  \backslash \psi_{k,i},x_1,x_2)$，其中$P_k$为第$k$层所有节点的集合</li><li>$vc(g,\psi_{k+1,j},x_1,x_2)$且$nsc(\psi_{k+1,j},x_1,x_2)$</li></ul><font color="red">(捕获符号更改情况的决策特征的重大更改)</font></li></ul></li><li><p>Value-Value Coverage (VVC)</p><ul><li><p>对一个特征对$\alpha = (\psi_{k,i},\psi_{k+1,j})$，值函数$g_1$、$g_2$，$\alpha$被两个测试用例$x_1$、$x_2$SV-覆盖，记为$VV^{g_1,g_2}(\alpha,x_1,x_1)$。其定义为：</p><ul><li>$vc(g_1,\psi_{k,i},x_1,x_2)$ 且$nsc(P_k ,x_1,x_2)$，其中$P_k$为第$k$层所有节点的集合</li><li>$vc(g_2,\psi_{k+1,j},x_1,x_2)$且$nsc(\psi_{k+1,j},x_1,x_2)$</li></ul><font color="red">(条件特征没有符号变化，但决策特征的值发生显著变化)</font></li></ul></li></ul><h3 id="覆盖率计算"><a href="#覆盖率计算" class="headerlink" title="覆盖率计算"></a>覆盖率计算</h3><p>令$F= \left\{ SS,VS^{g},SV^{g},VV^{g_{1},g_{2}} \right\}$，给定DNN$N$和覆盖方法$f \in F$，测试特征对集合$O$，测试集$T$的覆盖率：</p><script type="math/tex; mode=display">M_{f}(N,T)= \frac{| \left\{ \alpha \in O| \exists x_{1},x_{2} \in T:f( \alpha ,x_{1},x_{2}) \right\} |}{|O|}</script><p>即被覆盖的测试特征对所占比例。</p><h3 id="与现有覆盖标准的强弱关系"><a href="#与现有覆盖标准的强弱关系" class="headerlink" title="与现有覆盖标准的强弱关系"></a>与现有覆盖标准的强弱关系</h3><p><img src="/2020/08/24/DeepCover/Fig3.png" alt="Fig3" style="zoom:60%;"></p><h3 id="自动测试用例生成"><a href="#自动测试用例生成" class="headerlink" title="自动测试用例生成"></a>自动测试用例生成</h3><ul><li>测试预言<ul><li>$X$为一组被正确标记的测试用例，$x \in X$，若$x’$与$x$足够接近且预测标签一致，则称$x’$通过测试预言</li></ul></li><li><strong>通过线性编程（LP）生成测试用例</strong><ul><li>方法同DeepConcolic</li></ul></li><li><strong>通过启发式搜索生成测试用例</strong><ul><li>基于梯度下降搜索覆盖特定特征对的输入对</li></ul></li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul><li>MNIST、CIFAR-10、ImageNet</li></ul><h3 id="可控制参数及变量"><a href="#可控制参数及变量" class="headerlink" title="可控制参数及变量"></a>可控制参数及变量</h3><ul><li>LP调用时使用的约束：$\parallel x_2 - x_1 \parallel _ \infty$</li><li>$SV^g$中：$ g= \frac{u_{k+1},j \left[ x_{2} \right] }{u_{k+1},j \left[ x_{1} \right] } \geqslant \sigma $ ，$\sigma = 2$</li><li>$VV^{g_1,g_2}$中：$\sigma = 5$</li><li>每个特征包含神经元个数<ul><li>大小用参数$w$来调节：$\psi _{k,i} \leq w \cdot s_k$</li></ul></li></ul><h3 id="MNIST上的实验结果"><a href="#MNIST上的实验结果" class="headerlink" title="MNIST上的实验结果"></a>MNIST上的实验结果</h3><ul><li>一个特征即一个神经元</li><li>训练10个DNN（准确率&gt;97%)</li></ul><p><img src="/2020/08/24/DeepCover/Fig4.png" alt="Fig4" style="zoom:67%;"></p><ul><li>DNN错误查找结果：<ul><li>测试用例生成算法有效地实现了对所有覆盖标准的高覆盖，</li><li>覆盖方法被认为是有用的，因为找出了很多对抗样本。</li></ul></li><li>DNN安全分析：<ul><li>覆盖率$M_f$和对抗性实例百分比$AE_f$一起提供了评估DNN的定量统计数据。一般来说，给定一个测试集，一个具有高覆盖率水平$M_f$和低对抗百分比$AE_f$的DNN被认为是鲁棒的。</li><li>下图展示了对抗样本的距离和累积对抗样本数的关系。一个更稳健的DNN将在小距离末端(接近0)有更低的形状，因为报告的敌对的例子相对于他们原始的正确输入是更远的。直觉上，这意味着需要付出更多的努力来愚弄一个稳健的DNN，使其从正确的分类变成错误的标签。</li></ul></li></ul><p><img src="/2020/08/24/DeepCover/Fig5.png" alt="Fig5" style="zoom:60%;"></p><ul><li>逐层的行为：<ul><li>当深入DNN时，神经元对的覆盖会变得更加困难。在这种情况下，为了提高覆盖性能，在生成测试对时需要使用较大的数据集。图5b给出了在不同层中发现的敌对示例的百分比(在所有敌对示例中)。有趣的是，大多数对抗性的例子似乎都是在测试中间层时发现的。</li></ul></li></ul><p><img src="/2020/08/24/DeepCover/Fig6.png" alt="Fig6" style="zoom:60%;"></p><ul><li><p>高权重的SSC：</p><ul><li>为了减少测试特征对的数量，仅选择带有较高权重的一些神经元作为特征，改变前后二者的区别不大，因此实际使用时可以采用这种方式减少开销。</li></ul><p><img src="/2020/08/24/DeepCover/Fig7.png" alt="Fig7" style="zoom:60%;"></p></li><li><p>调用LP的开销</p><ul><li>对于每个DNN，我们选择一组神经元对，其中每个决策神经元位于不同的层。然后，我们测量变量和约束的数量，以及在解决每个LP调用上花费的时间(以秒计算)。表3中的结果证实了部分激活模式的LP模型确实是轻量级的，并且在遍历一个DNN的更深层时，其复杂度以线性方式增加。</li></ul></li></ul><p><img src="/2020/08/24/DeepCover/Fig8.png" alt="Fig8" style="zoom:60%;"></p><h3 id="CIFAR10上的实验结果"><a href="#CIFAR10上的实验结果" class="headerlink" title="CIFAR10上的实验结果"></a>CIFAR10上的实验结果</h3><ul><li>在不失一般性的前提下，卷积层节点的激活是通过激活一个子集的先例节点来计算的，每个节点都属于其层中的一个feature map。我们将算法1中的启发式测试用例生成应用于SS覆盖率，并在每个不同的层分别度量决策特征的覆盖率结果。</li><li>总的来说，SS覆盖率高于90%是通过相当一部分的敌对示例实现的。</li><li>一个有趣的观察是,<font color="red">更深层的因果变化的特性能够检测小扰动的输入导致对抗的行为</font>,而这有可能为开发人员提供有用的反馈调试或优化神经网络参数。</li></ul><p><img src="/2020/08/24/DeepCover/Fig9.png" alt="Fig9" style="zoom:60%;"></p><h3 id="ImageNet上的实验结果"><a href="#ImageNet上的实验结果" class="headerlink" title="ImageNet上的实验结果"></a>ImageNet上的实验结果</h3><ul><li><p>VGG16+启发式测试用例生成算法</p></li><li><p>特征：一组神经元</p><ul><li>大小用参数$w$来调节：$\psi _{k,i} \leq w \cdot s_k$，$s_k$为该层神经元总数</li></ul></li><li><p>我们测试了2000个随机特征对的SS覆盖情况，$w \in \{0.1\%、0.5\%、1.0\%\}$。</p><ul><li><p>覆盖结果：10.5%，13.6%和14.6%是对抗样本。对抗样本的平均距离和标准偏差：</p><p><img src="/2020/08/24/DeepCover/Fig10.png" alt="Fig10" style="zoom:60%;"></p></li><li><p>结果表明，特征对与输入扰动之间存在一定的关系。在生成的对抗样本中，<font color="red">更细粒度的特征比粗糙的特征能够捕获更小的扰动。</font></p></li></ul></li><li><p><font color="red">我们注意到访问边界激活值很可能要求对DNNs进行更大的更改。</font>我们设置功能大小使用$w$= 10%,获得的测试集中有22.7%的对抗样本。然而，这些敌对的例子的距离，平均L∞-norm距离3.49，标准差3.88，远远大于SS覆盖的距离</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/DNN%E6%B5%8B%E8%AF%95/">DNN测试</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E6%B5%8B%E8%AF%95%E6%A0%87%E5%87%86/">测试标准</category>
      
      <category domain="https://rubychen0611.github.io/tags/MC-DC/">MC/DC</category>
      
      
      <comments>https://rubychen0611.github.io/2020/08/24/DeepCover/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
