<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>与我常在</title>
    <link>https://rubychen0611.github.io/</link>
    
    <atom:link href="https://rubychen0611.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>自爱兼爱，善感而不多愁。</description>
    <pubDate>Sun, 20 Dec 2020 08:38:39 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>【算法复习】排序和顺序统计量</title>
      <link>https://rubychen0611.github.io/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/</link>
      <guid>https://rubychen0611.github.io/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/</guid>
      <pubDate>Thu, 03 Dec 2020 12:53:15 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;排序&quot;&gt;&lt;a href=&quot;#排序&quot; class=&quot;headerlink&quot; title=&quot;排序&quot;&gt;&lt;/a&gt;排序&lt;/h2&gt;&lt;h3 id=&quot;排序算法总结&quot;&gt;&lt;a href=&quot;#排序算法总结&quot; class=&quot;headerlink&quot; title=&quot;排序算法总结&quot;&gt;&lt;/a&gt;排序算法总结&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/小结.png&quot; alt=&quot;小结&quot; style=&quot;zoom: 80%;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;任何比较排序在最坏情况下都要经过$\Omega(nlgn)$次比较，因此归并排序和堆排序都是渐近最优的&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="排序算法总结"><a href="#排序算法总结" class="headerlink" title="排序算法总结"></a>排序算法总结</h3><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/小结.png" alt="小结" style="zoom: 80%;"></p><ul><li>任何比较排序在最坏情况下都要经过$\Omega(nlgn)$次比较，因此归并排序和堆排序都是渐近最优的</li></ul><a id="more"></a><h3 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h3><ul><li>把第j个数作为key插入到A[1…j-1]的有序数列中：从后向前查找，将大于key的数向后移</li></ul><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/插入排序.png" alt="插入排序"></p><ul><li>时间开销：$\Theta(n^2)$</li><li>空间开销：原址的（仅有常数个元素 需要在排序过程中存储在数组之外）</li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InsertionSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; a.<span class="built_in">size</span>(); i++)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">int</span> key = a[i];</span><br><span class="line">        <span class="keyword">int</span> j = i - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(j &gt;= <span class="number">0</span> &amp;&amp; a[j] &gt; key)</span><br><span class="line">        {</span><br><span class="line">            a[j + <span class="number">1</span>] = a[j];</span><br><span class="line">            j--;</span><br><span class="line">        }</span><br><span class="line">        a[j + <span class="number">1</span>] = key;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="合并排序"><a href="#合并排序" class="headerlink" title="合并排序"></a>合并排序</h3><ul><li><p>思想</p><ul><li>分解（Divide）：将n个元素分成各含n/2个元素的子序列；</li><li>解决（Conquer）：用合并排序法对两个子序列递归地排序；</li><li>合并（Combine）：合并两个已排序的子序列以得到排序结果。</li></ul></li><li><p>辅助过程<strong>MERGE</strong>：（合并两个子数组A[p…q], A[q+1…r]）</p><ul><li>把两个子数组复制到两个新的数组L和R，合并两数组</li></ul></li></ul><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/合并排序1.png" alt="合并排序1"></p><ul><li><p>递归过程：MERGE-SORT</p><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/合并排序2.png" alt="合并排序2" style="zoom:60%;"></p></li><li><p>递归式分析代价：<script type="math/tex">T(n)= \begin{cases} \Theta(1)& \text{n=1}\\ 2T(n/2)+\Theta(n) & \text{n>1} \end{cases}</script></p><ul><li><p>递归树</p><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/合并排序3.png" alt="合并排序3" style="zoom:60%;"></p></li></ul></li><li><p>时间开销：$\Theta(nlgn)$</p></li><li><p>空间开销：非原址</p></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> mid, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> n1 = mid - left + <span class="number">1</span>, n2 = right - mid;</span><br><span class="line">    vector&lt;int&gt; L(n1), R(n2);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n1; i++)</span><br><span class="line">        L[i] = a[left + i];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n2; i++)</span><br><span class="line">        R[i] = a[mid + <span class="number">1</span> + i];</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> k = left; k &lt;= right; k++)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (i &lt; n1 &amp;&amp; j &lt; n2)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (L[i] &lt;= R[j])</span><br><span class="line">                a[k] = L[i++];</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                a[k] = R[j++];</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (i == n1)</span><br><span class="line">            a[k] = R[j++];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            a[k] = L[i++];</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MergeSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span> (left &lt; right)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">        MergeSort(a, left, mid);</span><br><span class="line">        MergeSort(a, mid+<span class="number">1</span>, right);</span><br><span class="line">        Merge(a, left, mid, right);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><ul><li><p>思想：反复交换相邻的未按次序排序的元素</p><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/冒泡排序1.png" alt="冒泡排序1" style="zoom:67%;"></p></li><li><p>时间开销：$\Theta(n^2)$</p></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BubbleSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.<span class="built_in">size</span>() - <span class="number">1</span>; i++)   <span class="comment">// 已到位数字数目</span></span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; a.<span class="built_in">size</span>() - i - <span class="number">1</span>; j++)   <span class="comment">//遍历剩余的相邻项</span></span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(a[j] &gt; a[j + <span class="number">1</span>])</span><br><span class="line">            {</span><br><span class="line">                <span class="keyword">int</span> temp = a[j];</span><br><span class="line">                a[j] = a[j + <span class="number">1</span>];</span><br><span class="line">                a[j + <span class="number">1</span>] = temp;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><h4 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h4><p>一个近似的完全二叉树，除了最底层外，该树是完全充满的。</p><ul><li>$A.length$：数组长度</li><li>$A.heap-size$：当前有效元素个数</li><li>根节点：$A[1]$</li><li>给定一个节点下标$i$，计算父节点、左孩子、右孩子的下标：</li></ul><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/堆1.png" alt="堆1" style="zoom: 80%;"></p><p><strong>堆的性质：</strong></p><ul><li>最大堆性质：除了根节点以外所有节点$i$满足：$A[PARENT(i)]\geq A[i]$</li><li>最小堆性质：除了根节点以外所有节点$i$满足：$A[PARENT(i)]\leq A[i]$</li><li>树的高度：一个包含$n$个元素的堆的高度为$\Theta(lgn)$</li></ul><h4 id="维护堆的性质"><a href="#维护堆的性质" class="headerlink" title="维护堆的性质"></a>维护堆的性质</h4><ul><li>输入：堆$A$，节点$i$</li><li>输出：当节点$i$违背最大堆性质（值小于其子节点时），让其值在最大堆中逐级下降，从而使得<strong>以$i$为根节点的子树</strong>重新遵循最大堆的性质。</li><li>在父节点、左孩子、右孩子中选出最大的，若最大的不是父节点，则将该子节点与父节点交换，对该子节点递归地调用函数。</li><li>代价：$O(lgn)$</li></ul><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/维护堆的性质.png" alt="维护堆的性质"></p><h4 id="建堆"><a href="#建堆" class="headerlink" title="建堆"></a>建堆</h4><p>从最后一个非叶节点到根节点，依次调用MAX-HEAPIFY方法</p><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/建堆.png" alt="建堆"></p><ul><li>代价：$O(n)$</li></ul><h4 id="堆排序-1"><a href="#堆排序-1" class="headerlink" title="堆排序"></a>堆排序</h4><p>首先调用BUILD-MAX-HEAP构造最大堆，此时最大元素在$A[1]$中，交换$A[1]$和$A[n]$，此时新的根节点可能违背了最大堆性质，调用MAX_HEAPIFY(A,1)，从而在$A[1…n-1]$上构造一个新的最大堆。以此类推，直到堆的大小降到2。</p><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/堆排序.png" alt="堆排序"></p><ul><li><p>时间开销：$O(lgn)$</p></li><li><p>空间开销：原址的</p></li></ul><h4 id="最大优先队列"><a href="#最大优先队列" class="headerlink" title="最大优先队列"></a>最大优先队列</h4><ul><li>$MAXIMUM(S)$：返回$S$中最大键字的元素。（return A[1]）<ul><li>$\Theta(1)$</li></ul></li><li>$EXTRACT-MAX(S)$：去掉并返回S中具有最大键字的元素。<ul><li>交换A[1]和A[n]，调用MAX_HEAPIFY(A,1)</li><li>$O(lgn)$</li></ul></li><li>$INCREASE(S,x,k)$：将元素x的关键字值增加到k（假设k不小于x节点原关键字值）<ul><li>将新关键字值不断与父节点进行比较，大于则交换</li><li>$O(lgn)$</li></ul></li><li>$INSERT(S,x)$：把元素x插入到S中。<ul><li>在最后增加一个大小为$-\infin$节点，调用$INCREASE(A,A.heapsize,key)$</li><li>$O(lgn)$</li></ul></li></ul><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><p>PARTITION：将数组$A[p…r]$划分成两个子数组$A[p…q-1]$和$A[q+1…r]$，使得$A[p…q-1]$中每个元素都小于等于$A[q]$，$A[q]$也小于等于$A[q+1…r]$中每个元素。</p><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/快速排序.png" alt="快速排序" style="zoom:80%;"></p><ul><li><p>PARTITION过程</p><ul><li>选择$x=A[r]$作为主元</li><li>$A[p…i]$：小于等于主元的部分</li><li>$A[i+1…j-1]$：大于主元的部分</li><li>$A[j…r-1]$：尚未考虑的部分</li><li>复杂度：$\Theta(n)$，其中$n=r-p+1$</li></ul></li><li><p>时间开销：最坏$\Theta(n^2)$，最好$O(nlgn)$</p></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> x = a[right];   <span class="comment">// 主元</span></span><br><span class="line">    <span class="keyword">int</span> i = left - <span class="number">1</span>;       <span class="comment">// i标记小于x、大于x的分界点（a[i]是最后一个小于主元的元素，a[i+1]是第一个大于主元的元素）</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = left; j &lt;= right - <span class="number">1</span>; j++)     <span class="comment">// j遍历每个除主元外的元素</span></span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span>(a[j] &lt;= x)</span><br><span class="line">        {</span><br><span class="line">            i ++;        </span><br><span class="line">            swap(a[i], a[j]);        <span class="comment">//将新的a[j]与第一个大于主元的元素交换</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    swap(a[i + <span class="number">1</span>], a[right]);    <span class="comment">// 放置主元</span></span><br><span class="line">    <span class="keyword">return</span> i + <span class="number">1</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">QuickSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span> (left &lt; right)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">int</span> pivot = Partition(a, left, right);</span><br><span class="line">        QuickSort(a, left, pivot - <span class="number">1</span>);</span><br><span class="line">        QuickSort(a, pivot + <span class="number">1</span>, right);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h3><p>假设$n$个输入元素都是在0到$k$区间内的一个整数。对于每一个输入元素$x$，确定小于x的元素个数，利用这一信息，就可以直接把x放到它在输出数组中的位置上了。例如有17个元素小于x，则x就应该在第18个输出位置上。</p><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/计数排序.png" style="zoom:80%;"></p><ul><li>代价：当$k=O(n)$时，排序的运行时间为$\Theta(n)$</li></ul><h3 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h3><p>从最低有效位到最高有效位进行排序。</p><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/基数排序2.png" alt="基数排序2" style="zoom:80%;"></p><ul><li>代价：给定$n$个$d$位数，其中每一个数位可能有$k$个可能的取值，若使用的稳定排序方法耗时$n+k$，那么它就可以在$\Theta(d(n+k))$时间内将这些数排好序。</li></ul><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/基数排序.png" alt="基数排序" style="zoom:80%;"></p><h3 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h3><p>假设数据服从$[0,1)$上的$均匀分布</p><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/桶排序.png" alt="桶排序" style="zoom:80%;"></p><ul><li>时间代价：$O(n)$</li></ul><h2 id="中位数和顺序统计量"><a href="#中位数和顺序统计量" class="headerlink" title="中位数和顺序统计量"></a>中位数和顺序统计量</h2><h3 id="同时找到最大值和最小值"><a href="#同时找到最大值和最小值" class="headerlink" title="同时找到最大值和最小值"></a>同时找到最大值和最小值</h3><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/最大值最小值.png" alt="最大值最小值" style="zoom: 67%;"></p><h3 id="期望时间为线性的选择算法"><a href="#期望时间为线性的选择算法" class="headerlink" title="期望时间为线性的选择算法"></a>期望时间为线性的选择算法</h3><p>一种分治算法，用到了快速排序中的RANDOMIZED-SELECT算法，但快排会递归处理划分的两边，这里只处理一边。</p><ul><li>算法返回数组$A[p…r]$中第$i$小的元素</li><li>期望运行时间：$\Theta(n)$</li><li>最坏运行时间：$\Theta(n^2)$</li></ul><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/随机选择.png" alt="随机选择" style="zoom:80%;"></p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//LeetCode 215: 在未排序的数组中找到第 k 个最大的元素。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">int</span> x = a[right];</span><br><span class="line">        <span class="keyword">int</span> i = left - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = left; j &lt;= right - <span class="number">1</span>; j++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (a[j] &gt;= x)    <span class="comment">// 注意这里改成了&gt;=</span></span><br><span class="line">            {</span><br><span class="line">                i++;</span><br><span class="line">                swap(a[i], a[j]);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        swap(a[i+<span class="number">1</span>], a[right]);</span><br><span class="line">        <span class="keyword">return</span> i + <span class="number">1</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Select</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> left, <span class="keyword">int</span> right, <span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span> (left == right)</span><br><span class="line">            <span class="keyword">return</span> a[left];</span><br><span class="line">        <span class="keyword">int</span> x = Partition(a, left, right);</span><br><span class="line">        <span class="keyword">int</span> k = x - left + <span class="number">1</span>;   <span class="comment">// 当前的x是第几个数字</span></span><br><span class="line">        <span class="keyword">if</span> (i == k)</span><br><span class="line">            <span class="keyword">return</span> a[x];</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (i &lt; k)</span><br><span class="line">            <span class="keyword">return</span> Select(a, left, x - <span class="number">1</span>, i);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> Select(a, x + <span class="number">1</span>, right, i - k);</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findKthLargest</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> Select(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>()<span class="number">-1</span>, k);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="最坏情况为线性的选择算法"><a href="#最坏情况为线性的选择算法" class="headerlink" title="最坏情况为线性的选择算法"></a>最坏情况为线性的选择算法</h3><p>与随机选择相比，保证对数组能做出一个较好的划分：</p><p><img src="/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/select.png" alt="select" style="zoom: 67%;"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E7%AE%97%E6%B3%95/">算法</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/">算法导论</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E6%8E%92%E5%BA%8F/">排序</category>
      
      
      <comments>https://rubychen0611.github.io/2020/12/03/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>用python从零实现一个神经网络</title>
      <link>https://rubychen0611.github.io/2020/11/30/My-Neural-Network/</link>
      <guid>https://rubychen0611.github.io/2020/11/30/My-Neural-Network/</guid>
      <pubDate>Mon, 30 Nov 2020 06:03:50 GMT</pubDate>
      
      <description>&lt;p&gt;不用tensorflow、pytorch等任何现有深度学习框架以及各种封装好的机器学习库，仅使用python语言及矩阵运算的库，从零开始实现一个全联接的神经网络。&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>不用tensorflow、pytorch等任何现有深度学习框架以及各种封装好的机器学习库，仅使用python语言及矩阵运算的库，从零开始实现一个全联接的神经网络。<a id="more"></a></p><h2 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h2><p>我们以一个回归任务为例，实现一个多层神经网络来拟合一个非线性函数$ y= \sin (x_{1})- \cos (x_{2}),x_{1} \in \left[ -5,5 \right] ,x_{2} \in \left[ -5,5 \right] $，函数图像如下图所示，输入层包含2个神经元，输出层包含1个神经元，隐藏层的数目和神经元数自定义，隐藏层采用ReLU作为激活函数，输出层用恒等函数做为激活函数，损失函数为均方误差（MSE）。</p><p>实际上我们实现的神经网络可以自定义输入输出大小、隐藏层结构、激活函数、损失函数等，因此用在其他任务上也是完全可以的。</p><p><img src="/2020/11/30/My-Neural-Network/1.png" alt="1" style="zoom:50%;"></p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>训练神经网络之前，我们需要先完成一些准备工作，包括生成训练和验证的数据集、定义用到的激活函数和损失函数。</p><h3 id="数据集生成"><a href="#数据集生成" class="headerlink" title="数据集生成"></a>数据集生成</h3><p>在轴$x_1$和$x_2$的$[-5,5]$区间上每隔0.1均匀地对样本点进行采样，然后计算目标函数的值，生成100*100=10000个训练数据，转换成为numpy​矩阵，作为训练数据集：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成训练数据集</span></span><br><span class="line">x_train = []</span><br><span class="line">y_train = []</span><br><span class="line"><span class="keyword">for</span> x1 <span class="keyword">in</span> np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="keyword">for</span> x2 <span class="keyword">in</span> np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>):</span><br><span class="line">        x_train.append([x1, x2])</span><br><span class="line">        y_train.append(np.sin(x1)-np.cos(x2))</span><br><span class="line">x_train = np.array(x_train)</span><br><span class="line">y_train = np.array(y_train)</span><br></pre></td></tr></tbody></table></figure><p>另外随机采样并生成大小为1000的验证数据集：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成验证数据集</span></span><br><span class="line">x_val = []</span><br><span class="line">y_val = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    x1 = random.uniform(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">    x2 = random.uniform(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">    x_val.append([x1, x2])</span><br><span class="line">    y_val.append(np.sin(x1)-np.cos(x2))</span><br><span class="line">x_val = np.array(x_val)</span><br><span class="line">y_val = np.array(y_val)</span><br><span class="line"> </span><br></pre></td></tr></tbody></table></figure><h3 id="激活函数和损失函数"><a href="#激活函数和损失函数" class="headerlink" title="激活函数和损失函数"></a>激活函数和损失函数</h3><p>定义本次任务用到的数学函数，包括激活函数和损失函数：</p><ul><li><p>激活函数</p><p>定义ActivationFunction类作为激活函数的基类，包括calculate（计算）和derivative（求导）两个抽象方法。本次任务中用到了ReLU和Pureline两种激活函数，若想使用其他激活函数可以仿照下面的形式定义（注意接受的参数x都是一个numpy矩阵）。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABCMeta, abstractmethod</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActivationFunction</span>:</span></span><br><span class="line">    <span class="string">'''激活函数基类'''</span></span><br><span class="line">    __metaclass__ = ABCMeta</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derivative</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReLU</span>(<span class="params">ActivationFunction</span>):</span></span><br><span class="line">    <span class="string">'''relu激活函数'''</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.maximum(x, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derivative</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.where(x &gt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pureline</span>(<span class="params">ActivationFunction</span>):</span></span><br><span class="line">    <span class="string">'''恒等激活函数'''</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derivative</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.ones(shape=x.shape)</span><br></pre></td></tr></tbody></table></figure></li></ul><ul><li><p>MSE损失函数</p><p>假设样本个数为$n$，计算公式：$MSE=\frac{1}{n} \sum_{i=1}^{n}(y_{true}[i]-y_{pred}[i])^2$</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse_loss</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="string">'''MSE损失函数'''</span></span><br><span class="line">    <span class="keyword">return</span> ((y_true - y_pred) ** <span class="number">2</span>).mean()</span><br></pre></td></tr></tbody></table></figure></li></ul><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="数据结构定义和初始化"><a href="#数据结构定义和初始化" class="headerlink" title="数据结构定义和初始化"></a>数据结构定义和初始化</h3><p>首先定义类NeuralNetwork表示神经网络类，初始化时用户向类构造函数传入参数input_size（输入层维度）、output_size（输出层维度）、hidden_size（隐藏层维度，用一个数组表示，如[5,3]表示两个隐藏层，分别包含5个和3个神经元），从而可以达到任意调整神经网络结构的目的。</p><p>接着定义两个数组self.w和self.b表示神经网络的权重和偏移量，每层的权重是一个numpy矩阵，用np.random.normal函数进行高斯分布初始化（设置均值为0，方差为0.1）；同时定义一个数组self.activations表示每层的激活函数，这里隐藏层激活函数为ReLU，输出层激活函数为Pureline：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机初始化权重和偏移量</span></span><br><span class="line">self.len = len(hidden_size) + <span class="number">1</span>    <span class="comment"># 层数(除输入层)</span></span><br><span class="line">loc = <span class="number">0.0</span></span><br><span class="line">scale = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 隐藏层</span></span><br><span class="line">self.w = [np.random.normal(loc, scale, size=(input_size, hidden_size[<span class="number">0</span>]))]</span><br><span class="line">self.b = [np.random.normal(loc, scale, size=(hidden_size[<span class="number">0</span>],))]</span><br><span class="line">self.activations = [ReLU]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(hidden_size)<span class="number">-1</span>):</span><br><span class="line">    self.w.append(np.random.normal(loc, scale, size=(hidden_size[i], hidden_size[i+<span class="number">1</span>])))</span><br><span class="line">    self.b.append(np.random.normal(loc, scale, size=(hidden_size[i+<span class="number">1</span>],)))</span><br><span class="line">    self.activations.append(ReLU)</span><br><span class="line"><span class="comment"># 输出层</span></span><br><span class="line">self.w.append(np.random.normal(loc, scale, size=(hidden_size[<span class="number">-1</span>], output_size)))</span><br><span class="line">self.b.append(np.random.normal(loc, scale, size=(output_size,)))</span><br><span class="line">self.activations.append(Pureline)</span><br></pre></td></tr></tbody></table></figure><p>然后定义五个数组用于保存训练时的中间结果，均初始化为全0：</p><ul><li>self.z：每个神经元激活前的输出。</li><li>self.a：每个神经元激活后的输出。</li><li>self.delta：神经单元误差$\delta$，即loss对每个神经元激活前输出$z$的梯度（用于反向传播更新权重）。</li><li>self.delta_w：权重w的累积更新值，用于处理完每个batch后更新权重。</li><li>self.delta_b：权重b的累积更新值。</li></ul><h3 id="前向计算中间结果"><a href="#前向计算中间结果" class="headerlink" title="前向计算中间结果"></a>前向计算中间结果</h3><p>每轮训练首先调用一个自定义的shuffle方法打乱训练样本集合。然后开始训练，设置训练轮数为30，batch_size为10，初始学习率为0.01。对每个输入样本，通过前向传播计算得到每层的输出self.z和self.a，如假设第$i$层（$i&gt;1$)的输入为$a[i-1]$，权重和偏移为$w[i]$和$b[i]$，则有:</p><p>$z[i]=a[i-1]\times w[i] + b[i]$，</p><p>$a[i]=activations[i].calculate(z[i])$</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="string">'''随机打乱数据'''</span></span><br><span class="line">    xy= np.c_[x,y]</span><br><span class="line">    np.random.shuffle(xy)</span><br><span class="line">    x = xy[:, :<span class="number">-1</span>]</span><br><span class="line">    y = xy[:, <span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, x_train, y_train, x_val, y_val, epochs=<span class="number">30</span>, batch_size=<span class="number">10</span>, lr=<span class="number">0.01</span></span>):</span></span><br><span class="line">    <span class="string">'''训练模型'''</span></span><br><span class="line"></span><br><span class="line">    self.validate(<span class="number">0</span>, x_train, y_train, x_val, y_val)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        x_train_shuffle, y_train_shuffle = shuffle(x_train, y_train)</span><br><span class="line">        batch_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x, y_true <span class="keyword">in</span> zip(x_train_shuffle, y_train_shuffle):</span><br><span class="line">            <span class="comment"># 前向传播得到每层输出</span></span><br><span class="line">            layer_output = x</span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">0</span>, self.len):</span><br><span class="line">                self.z[layer] = np.matmul(layer_output, self.w[layer]) + self.b[layer]</span><br><span class="line">                layer_output = self.activations[layer].calculate(self.z[layer])</span><br><span class="line">                self.a[layer] = layer_output</span><br></pre></td></tr></tbody></table></figure><h3 id="反向传播计算梯度"><a href="#反向传播计算梯度" class="headerlink" title="反向传播计算梯度"></a>反向传播计算梯度</h3><p>接下来计算输出层（第-1层）的神经单元误差$\delta$，对于每个样本，假如输出层维度为$m$，损失函数$C=\frac{1}{m}\sum_{i=1}^{m}(y_{true}[i]-a[-1][i])^2$（本次任务中$m=1$），则$\delta[-1] = \frac{\partial C}{\partial z[-1]} = \frac{\partial C}{\partial a[-1]} \cdot \frac{\partial a[-1]}{\partial z[-1]}=  \frac{1}{m} \cdot 2 \cdot (a[-1] - y_{true}) \cdot \theta’(z[-1])$，其中$\theta$为输出层的激活函数。</p><p>然后可以通过递推式反向计算出前面每层的神经单元误差，第$i$层的神经单元误差$\delta[i]=w[i+1] \times \delta[i+1] \times \theta’(z[i])$ ，其中$\theta$表示第$i$层的激活函数。</p><p>通过神经单元误差$\delta$我们可以计算出损失函数对w和b的导数，对第$i$层：$\frac{\partial C}{\partial w[i]} = a[i-1]^T \cdot \delta[i]$，$\frac{\partial C}{\partial b[i]}= \delta[i]$。并将导数累加结果记录在self.delta_w、self.delta_b数组中。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算最后一层的神经单元误差δ</span></span><br><span class="line">self.delta[<span class="number">-1</span>] = <span class="number">2</span> * (self.a[<span class="number">-1</span>] - y_true) * self.activations[<span class="number">-1</span>].derivative(self.z[<span class="number">-1</span>]) / len(self.delta[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向递推计算每层神经单元误差δ</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(self.len<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">    self.delta[layer] = np.matmul(self.w[layer + <span class="number">1</span>], self.delta[layer + <span class="number">1</span>]) * self.activations[layer].derivative(self.z[layer])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 累加导数</span></span><br><span class="line">last_layer_output = x</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(self.len):</span><br><span class="line">    self.delta_w[layer] += np.matmul(np.transpose([last_layer_output]), [self.delta[layer]])</span><br><span class="line">    self.delta_b[layer] += self.delta[layer]</span><br><span class="line">    last_layer_output = self.a[layer]</span><br></pre></td></tr></tbody></table></figure><h3 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h3><p>每个batch结束后，更新w和b。对第i层：</p><p>$w[i] \leftarrow w[i]- lr \cdot \Delta w[i]$</p><p>$b[i] \leftarrow b[i]- lr \cdot \Delta b[i]$</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个batch结束后更新w、b</span></span><br><span class="line">batch_count += <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> batch_count == batch_size:</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> range(self.len):</span><br><span class="line">        self.w[layer] -= lr * self.delta_w[layer]</span><br><span class="line">        self.b[layer] -= lr * self.delta_b[layer]</span><br><span class="line">        self.delta_w[layer].fill(<span class="number">0</span>)</span><br><span class="line">        self.delta_b[layer].fill(<span class="number">0</span>)</span><br><span class="line">batch_count = <span class="number">0</span></span><br></pre></td></tr></tbody></table></figure><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>这里采用了简单的学习率衰减方案，每过三轮学习率衰减为0.9倍。大家可以尝试其他衰减方案。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">3</span> == <span class="number">0</span>:</span><br><span class="line">    lr *= <span class="number">0.9</span></span><br></pre></td></tr></tbody></table></figure><h2 id="验证和可视化"><a href="#验证和可视化" class="headerlink" title="验证和可视化"></a>验证和可视化</h2><p>每轮训练完成后调用validate方法验证模型在训练数据集和验证数据集上的损失，并绘制图像。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="string">'''前向传播计算预测结果'''</span></span><br><span class="line">    layer_output = x</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">0</span>, self.len):</span><br><span class="line">        layer_output = self.activations[layer].calculate((np.matmul(layer_output, self.w[layer]) + self.b[layer]))</span><br><span class="line">    <span class="keyword">return</span> layer_output.squeeze()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">self, epoch, x_train, y_train, x_val, y_val</span>):</span></span><br><span class="line">    <span class="string">'''计算训练集和验证集上的损失，生成可视化图像'''</span></span><br><span class="line">    y_pred_train = self.feedforward(x_train)</span><br><span class="line">    train_loss = mse_loss(y_train, y_pred_train)</span><br><span class="line">    y_pred_val = self.feedforward(x_val)</span><br><span class="line">    val_loss = mse_loss(y_val, y_pred_val)</span><br><span class="line">    self.visualize(epoch, y_pred_train, train_loss, val_loss)</span><br><span class="line">    print(<span class="string">"Epoch %d, train loss: %.4f, validation loss: %.4f"</span> % (epoch, train_loss, val_loss))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span>(<span class="params">self, epoch, y_pred, train_loss, val_loss</span>):</span></span><br><span class="line">   <span class="string">'''可视化训练结果'''</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.gca(projection=<span class="string">'3d'</span>)</span><br><span class="line">    X = np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">    Y = np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">    X, Y = np.meshgrid(X, Y)</span><br><span class="line">    Z = y_pred.reshape((<span class="number">100</span>,<span class="number">100</span>))</span><br><span class="line">    surf = ax.plot_surface(X, Y, Z, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>, cmap=cm.jet, linewidth=<span class="number">0</span>, antialiased=<span class="literal">False</span>)</span><br><span class="line">    ax.set_zlim(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">    ax.zaxis.set_major_locator(LinearLocator(<span class="number">10</span>))</span><br><span class="line">    ax.zaxis.set_major_formatter(FormatStrFormatter(<span class="string">'%.02f'</span>))</span><br><span class="line">    ax.set_title(<span class="string">'epoch=%d, train loss=%.4f, validation loss=%.4f'</span>% (epoch, train_loss, val_loss))</span><br><span class="line">    fig.colorbar(surf, shrink=<span class="number">0.5</span>, aspect=<span class="number">5</span>)</span><br><span class="line">    plt.savefig(<span class="string">'figure/%04d.png'</span> % epoch)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure><h2 id="拟合效果"><a href="#拟合效果" class="headerlink" title="拟合效果"></a>拟合效果</h2><h3 id="增加隐藏层宽度对拟合效果的影响"><a href="#增加隐藏层宽度对拟合效果的影响" class="headerlink" title="增加隐藏层宽度对拟合效果的影响"></a>增加隐藏层宽度对拟合效果的影响</h3><p>设置神经网络只有一个隐藏层，考察隐藏层分别包含10、20、50、100个神经元时，训练集loss随训练轮数增加的变化情况（如下图）。（设置学习率为恒定值0.001，训练轮数为30轮）</p><p><img src="/2020/11/30/My-Neural-Network/3.png" alt="image-20201129230937618" style="zoom:70%;"></p><p>可见随着训练轮数增长，各个模型的训练集loss首先快速下降，接着缓慢下降。单隐藏层神经元个数越多，训练完成后的loss越低，说明增加神经元宽度能提升模型对目标函数的拟合能力。</p><h3 id="增加隐藏层深度对拟合效果的影响"><a href="#增加隐藏层深度对拟合效果的影响" class="headerlink" title="增加隐藏层深度对拟合效果的影响"></a>增加隐藏层深度对拟合效果的影响</h3><p>考察神经网络分别包含1、2、3、4个隐藏层，且每个隐藏层包含20个神经元时，训练集loss随训练轮数增加的变化情况（如下图）。（设置学习率为恒定值0.001，训练轮数为30轮）。</p><p><img src="/2020/11/30/My-Neural-Network/4.png" alt="image-20201129232317776" style="zoom:70%;"></p><p>由图可见，神经网络层数越多，30轮训练结束后训练集的loss越低，说明层数越多，模型对目标函数的拟合能力越强。</p><h3 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h3><p>经过以上实验，我们验证了增加隐藏层的宽度和深度能减小拟合的误差。最后为了让拟合误差尽量小，经过多次尝试后，我们最终选择设置隐藏层个数为5，分别包含100、80、50、30、10个神经元，学习率衰减方案为每经过3轮训练，lr衰减为0.9倍。最终训练集loss为0.00015，验证集loss为0.00016。第0轮（训练前）、第1轮、第5轮、第30轮训练后的拟合结果可视化后如下图所示：</p><p><img src="/2020/11/30/My-Neural-Network/5.jpg" alt="图片1"></p><p>动图效果：</p><p><img src="/2020/11/30/My-Neural-Network/2.gif" alt="2" style="zoom:80%;"></p><p>完整代码见：<a href="https://github.com/rubychen0611/MyNeuralNetwork">https://github.com/rubychen0611/MyNeuralNetwork</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/">技术笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      <comments>https://rubychen0611.github.io/2020/11/30/My-Neural-Network/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】On Decomposing a Deep Neural Network into Modules</title>
      <link>https://rubychen0611.github.io/2020/11/16/DNN-Decomposition/</link>
      <guid>https://rubychen0611.github.io/2020/11/16/DNN-Decomposition/</guid>
      <pubDate>Mon, 16 Nov 2020 03:21:36 GMT</pubDate>
      
      <description>&lt;p&gt;On Decomposing a Deep Neural Network into Modules （FSE’20）&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>On Decomposing a Deep Neural Network into Modules （FSE’20）<a id="more"></a></p><p>会议视频：<a href="https://www.youtube.com/watch?v=EH1aUbFj0HQ">https://www.youtube.com/watch?v=EH1aUbFj0HQ</a></p><p>代码：<a href="https://github.com/rangeetpan/decomposeDNNintoModules">https://github.com/rangeetpan/decomposeDNNintoModules</a></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul><li><p>传统软件的模块化分解与重组：可用于软件重用、替换、独立测试、独立开发等</p><p><img src="/2020/11/16/DNN-Decomposition/1.png" alt="1" style="zoom:67%;"></p></li><li><p>为什么DNN也需要模块分解？应用场景举例</p><ul><li><p>场景1：数据集内部分解</p><ul><li>手写数字识别→0，1识别，传统方法需要重新训练，本方法可直接将DNN解构成10个模块，组合成0、1识别模型。</li></ul><p><img src="/2020/11/16/DNN-Decomposition/2.png" alt="2" style="zoom:50%;"></p></li><li><p>场景2：数据集之间</p><ul><li><p>数字识别+字母识别-&gt;16进制数字识别</p><p><img src="/2020/11/16/DNN-Decomposition/3.png" alt="3" style="zoom:50%;"></p></li></ul></li><li><p>场景3：模块替换</p><ul><li><p>数字识别模型中某个数字5识别效果较差，从另一个模型中分解出单独识别5的模块，与A的其他模块组合在一起</p><p><img src="/2020/11/16/DNN-Decomposition/4.png" alt="4" style="zoom:50%;"></p></li></ul></li></ul></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="步骤1：关注点识别-Concern-Identification-CI"><a href="#步骤1：关注点识别-Concern-Identification-CI" class="headerlink" title="步骤1：关注点识别 Concern Identification (CI)"></a>步骤1：关注点识别 Concern Identification (CI)</h3><ul><li>关注点识别本质上是识别出整体模型中对特定功能或关注点有贡献的那些部分，解构DNN。</li><li>具体地，依次对模型喂入关注类别的训练样本：<ul><li>对未激活节点：将输入边和输出边的权重都置为零（删除边）</li><li>对激活节点：将输出边权重置为正（<code>算法1没看懂为什么是min</code>）</li><li>删除与输出层其他类别相连的边。</li></ul></li></ul><p><img src="/2020/11/16/DNN-Decomposition/5.png" alt="5" style="zoom:60%;"></p><h3 id="步骤2：纠缠识别-Tangling-Identification（TI）"><a href="#步骤2：纠缠识别-Tangling-Identification（TI）" class="headerlink" title="步骤2：纠缠识别 Tangling Identification（TI）"></a>步骤2：纠缠识别 Tangling Identification（TI）</h3><ul><li><p>第一步之后，解构出的模块只对目标类别的识别起作用，即目前只是一个<strong>单分类器</strong>，无法对非本类的样本进行判断。这就好比从一个程序中删除一个条件及其分支，从而产生直接执行剩余分支功能的子程序，但无条件地这样做。（<code>即所有样本都会被分类成这一个类别</code>）</p></li><li><p>解决方法：<font color="red">加回一些节点和边，帮助区分非目标类别。</font>四种TI方法：</p><ul><li><p>Imbalance (TI-I)</p><p><img src="/2020/11/16/DNN-Decomposition/6.png" alt="6" style="zoom:60%;"></p></li><li><p>Punish Negative Examples (TI-PN)</p><p><img src="/2020/11/16/DNN-Decomposition/7.png" alt="7" style="zoom:60%;"></p></li><li><p>Higher Priority to Negative Examples (TI-HP)</p><p><img src="/2020/11/16/DNN-Decomposition/8.png" alt="8" style="zoom:60%;"></p></li><li><p>Strong Negative Edges (TI-SNE)</p><p><img src="/2020/11/16/DNN-Decomposition/9.png" alt="9" style="zoom:60%;"></p></li></ul></li></ul><h3 id="步骤3：关注点模块化-Concern-Modularization-CM"><a href="#步骤3：关注点模块化-Concern-Modularization-CM" class="headerlink" title="步骤3：关注点模块化 Concern Modularization (CM)"></a>步骤3：关注点模块化 Concern Modularization (CM)</h3><p>这一步作用是将多个非关注点及其相应的神经元和边，抽象成一个输出层的节点（如下图的非0节点）：</p><p><img src="/2020/11/16/DNN-Decomposition/10.png" alt="10" style="zoom:50%;"></p><ul><li><p>Channeling (CM-C)：将最后一层输出到非关注点的边，通过取权重的平均值，都改向到一个”非“节点上</p><p><img src="/2020/11/16/DNN-Decomposition/11.png" alt="11" style="zoom:60%;"></p></li><li><p>Remove Irrelevant Edges (CM-RIE)：在Channeling之前，去掉倒数第二层仅对非关注点有贡献的边及其相关神经元</p><p><img src="/2020/11/16/DNN-Decomposition/12.png" alt="12" style="zoom:60%;"></p><p><img src="/2020/11/16/DNN-Decomposition/13.png" alt="13" style="zoom:70%;"></p></li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h3><ul><li>数据集：<ul><li>MNIST</li><li>EMNSIT（手写英文字母）：只用A-J（10个字母）训练</li><li>FMNIST：衣服</li><li>KMNIST：日语字母</li></ul></li><li>模型：<ul><li>4个数据集分别训练带1、2、3、4个隐藏层的全连接神经网络，每个隐藏层49个神经元</li></ul></li><li>测试准则<ul><li>准确率：衡量DNN模型模块化后的准确率，对输入的样本用分解后的每个模块进行预测，将给出positive预测结果且置信度最高并的子模型预测结果作为预测类别（投票）。</li><li>Jaccard系数（JI）：衡量模块之间的相似度，将所有权重和偏置放在一个向量中，比较Jaccard系数。</li></ul></li></ul><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul><li><p>实验1：分解后的模块有效性如何？</p><p><img src="/2020/11/16/DNN-Decomposition/14.png" alt="14" style="zoom:80%;"></p><ul><li>TI-HP方法：JI最低，但准确率同样很低<ul><li>因为先更新负类样本权重，再更新正类样本权重，负类样本权重被覆盖</li></ul></li><li>TI-PN方法：多个模型上低准确率<ul><li>先更新正类样本权重，再更新负类样本权重，JI较高，因为负类样本权重重合较多</li></ul></li><li>TI-SNE方法：准确率最高</li><li>CM-RIE相比CM-C降低了JI系数，且在准确率上表现较好。</li><li>准确率不变或有提升的模型中：33.79%的边是失效的，说明这些模型与原模型并不是完全一样。</li><li>调整了每层神经元的个数至70个之后，CM-RIE方法效果几乎一致。</li></ul></li><li><p>实验2：模块化后的DNN支持<strong>重用</strong>吗？</p><ul><li><p>数据集内部重用</p><ul><li><p>从原模型模型构造两类别模型（共C(10,2)=45种情况)：用这两类数据重训练的相同结构的模型作为对比。结果显示与重训练的模型准确率差不多。</p><p><img src="/2020/11/16/DNN-Decomposition/15.png" alt="15" style="zoom:67%;"></p><p><img src="/2020/11/16/DNN-Decomposition/17.png" alt="17" style="zoom:50%;"></p></li></ul></li><li><p>数据集之间重用</p><ul><li>绝大多数模型有一定的精度下降</li></ul><p><img src="/2020/11/16/DNN-Decomposition/18.png" alt="18" style="zoom:50%;"></p></li></ul></li></ul><ul><li><p>实验3：模块化后的DNN支持<strong>替换</strong>吗？</p><ul><li><p>同数据集模型替换</p></li><li><p>不同数据集模型替换</p><p><img src="/2020/11/16/DNN-Decomposition/19.png" alt="19" style="zoom:60%;"></p></li></ul></li></ul><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li><p>应用场景设想很新颖。</p></li><li><p>文章笔误太多，算法解释得不清楚。</p></li><li><p>直接对模型权重进行操纵的做法很大胆，实验用的模型都是全联接的小型模型，怀疑在其他大型模型上的可行性。</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/DNN%E5%88%86%E8%A7%A3/">DNN分解</category>
      
      
      <comments>https://rubychen0611.github.io/2020/11/16/DNN-Decomposition/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】Effective Path</title>
      <link>https://rubychen0611.github.io/2020/11/12/Effective-Path/</link>
      <guid>https://rubychen0611.github.io/2020/11/12/Effective-Path/</guid>
      <pubDate>Thu, 12 Nov 2020 07:31:20 GMT</pubDate>
      
      <description>&lt;p&gt;原文：Adversarial Defense Through Network Profiling Based Path Extraction （CVPR’19)&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>原文：Adversarial Defense Through Network Profiling Based Path Extraction （CVPR’19)  <a id="more"></a></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><ul><li><p>基本思想：找到Effective Path——一组重要连接和神经元的集合，使得模型得到最终的预测类别结果。可类比普通程序的执行路径。</p></li><li><p>前人方法（CDRP）</p><ul><li>需要重训练</li><li>维度过高</li></ul></li><li>提取路径信息的方法受到了传统程序分析中控制流和基本块的启发</li></ul><h3 id="单张图片提取"><a href="#单张图片提取" class="headerlink" title="单张图片提取"></a>单张图片提取</h3><p>欲提取有效路径$\mathcal{P=(N,S,W)}$，其中$\mathcal{N、S、W}$分别为重要神经元、连接、权重的集合。</p><ul><li><p>提取过程（由后向前）</p><ul><li><p>最后一层：只包括输出类别的神经元$n^L_p$</p></li><li><p>重要权重：最小权重集合，使得只计算该部分输出时，结果大于神经元$n^L_p$原本输出的$\theta$倍。由此得到前一层的重要神经元集合$N^{L-1}$。</p><ul><li>最小组合选取方法：对输入和权重的乘积进行排序，选最小且满足条件的组合</li></ul><p><img src="/2020/11/12/Effective-Path/1.png" alt="1" style="zoom:80%;"></p></li><li><p>对前一层的每个重要神经元：重复上述方法</p></li><li><p>卷积层特殊处理：</p><p><img src="/2020/11/12/Effective-Path/2.png" alt="2" style="zoom:80%;"></p><ul><li>根据感受野转换成全连接层</li><li>计算最小权重组合时无需对所有神经元排序，只需排感受野内的</li><li>由于卷积，多个连接可能共享相同的权重</li></ul></li></ul></li></ul><h3 id="多张图片提取"><a href="#多张图片提取" class="headerlink" title="多张图片提取"></a>多张图片提取</h3><ul><li>单张图片的路径相当于是神经元或连接的一个二元掩膜，指示了其是否在预测过程中产生了影响。多张图片的有效路径即其中所有单张图片路径组成的集合。使用每个类别预测正确的图片可得到per-class effective path，使用所有训练集预测正确的图片可得到overall effective path。</li><li>路径的稀疏性：<ul><li>设置$\theta=0.5$时，LeNet5、AlexNet、ResNet50、InceptionV4、VGG16上，Overall effective path在所有路径和权重中的占比分别为13.8%, 20.5%, 22.2%, 41.7%,17.2%，说明提取出的有效路径非常稀疏。</li></ul></li></ul><h2 id="路径的特殊化现象"><a href="#路径的特殊化现象" class="headerlink" title="路径的特殊化现象"></a>路径的特殊化现象</h2><p>不同类别的有效路径将网络拆分成不同的组件，可用于理解网络以及研究更改网络结构所带来的影响。</p><ul><li><p>路径特殊化现象：不同类别的有效路径相差很大</p><ul><li><p>计算Jacarrd系数（图2）：不同类别之间的相似度几乎都小于0.5，说明一半左右是共同激活的路径，一半是不同的</p></li><li><p>逐个合并ImageNet1000个类别的有效路径，密度一开始迅速上升然后在50个类别时开始趋于平缓（图3），与ImagNet有100个左右的基础类别这一事实相符合。</p><p><img src="/2020/11/12/Effective-Path/3.png" alt="3" style="zoom:80%;"></p></li></ul></li></ul><h2 id="对抗样本防御"><a href="#对抗样本防御" class="headerlink" title="对抗样本防御"></a>对抗样本防御</h2><ul><li><p>6种对抗攻击方法</p><p><img src="/2020/11/12/Effective-Path/4.png" alt="4" style="zoom:70%;"></p></li><li><p>对抗样本检测方法：计算<strong>路径相似度</strong></p><ul><li><p>计算待测图像的有效路径与该预测类别的有效路径之间的相似程度，因为单张图片的有效路径密度远低于类别的有效路径，所以Jaccard系数基本上取决于二者的交集占该图片有效路径的比例大小</p><ul><li>LeNet实验结果（图(a)）：正常样本相似度基本都是1.0左右，对抗样本相似度较低</li><li>AlexNet实验结果：<ul><li>Rank-1类别path：划分成不同层的相似度，对抗样本相似度同样较低（图c）。正常样本与对抗样本相似度之差（图d）显示中间层下降最多。</li><li>Rank-2类别path：正常样本相似度反而最低（图d），因为对抗样本的rank2类别往往是正常样本的rank1类别，所以对抗样本路径相似度更高。</li></ul></li></ul><p><img src="/2020/11/12/Effective-Path/5.png" alt="5"></p></li><li><p>防御模型</p><ul><li><p>使用rank1和rank2类别的有效路径来检测对抗样本</p></li><li><p>线性模型：每层rank1类别有效路径相似度$-$每层rank2类别有效路径相似度：$ \tilde{J}_{P}= \sum _{l=1}^{L} \omega ^{l}J_{P}^{l}- \sum _{l=1}^{L} \omega ^{l^{ \prime }}J_{P}^{l^\prime} $，若小于某阈值则判断为对抗样本</p></li><li><p>其他模型：random forest, AdaBoost, and gradient boosting</p></li></ul></li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E5%AF%B9%E6%8A%97%E9%98%B2%E5%BE%A1/">对抗防御</category>
      
      
      <comments>https://rubychen0611.github.io/2020/11/12/Effective-Path/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】Dynamic Slicing for Deep Neural Networks</title>
      <link>https://rubychen0611.github.io/2020/11/10/Dynamic-Slicing-for-DNN/</link>
      <guid>https://rubychen0611.github.io/2020/11/10/Dynamic-Slicing-for-DNN/</guid>
      <pubDate>Tue, 10 Nov 2020 08:07:33 GMT</pubDate>
      
      <description>&lt;p&gt;Dynamic Slicing for Deep Neural Networks （FSE’20）&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>Dynamic Slicing for Deep Neural Networks （FSE’20）<a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><p>传统程序切片技术：旨在从程序中提取满足一定约束条件（切片标准）的代码片段，是一种用于分解程序的程序分析技术。</p><ul><li>例如，通过将切片标准设置为某个导致错误的特定输出，可以获得与错误相关但比整个程序小得多的程序切片，因此更容易分析。</li></ul></li><li><p><strong>DNN切片</strong>的定义：计算获得一个神经元和突触子集，使其可以显著影响某些感兴趣的神经元的值。</p><ul><li>有助于理解神经网络的决策</li><li>有助于减小模型大小</li><li>把模型划分为重要/非重要部分，有利于优先保护模型的重要部分</li></ul></li><li><p>目标：找到在决策过程中更重要的神经元和突触的子集。</p></li><li><p>DNN切片的挑战：</p><ul><li>神经元的权重、连接难以理解</li><li>神经网络输出与几乎所有神经元都有关，因此必须区分每个神经元的贡献重要程度</li><li>DNN非常庞大，对性能要求高</li></ul></li><li><p>本文方法：<strong>NNSlicer</strong></p><ul><li><p>一种基于数据流分析的神经网络动态切片技术</p></li><li><p><strong>切片标准</strong>的定义：一组具有特殊意义的神经元</p><ul><li>如输出层神经元</li></ul></li><li><p><strong>神经网络切片</strong>的定义：一组对切片标准产生重要影响的神经元集合</p></li><li><p>动态切片：针对一组特定输入切片，而不是静态、与输入独立的</p></li><li><p>包括三个阶段：</p><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/3.png" alt="3" style="zoom:67%;"></p><ul><li>Profiling phase：对每个神经元的平均行为（输出值）建模。训练集在该神经元输出的均值作为<font color="red">baseline</font>。</li><li>Forward analysis phase：对欲切片的一组感兴趣的输入，喂进DNN，记录神经元输出。输出与baseline的差异代表了<font color="red">神经元对输入的敏感程度</font></li><li>Backward analysis phase：切片过程从切片标准中定义的神经元开始，逐层向前分析具有重要影响的神经元。</li></ul></li><li><p>效率</p><ul><li>一个样本：<ul><li>ResNet10：40s</li><li>ResNet18：550s</li></ul></li><li>批样本：<ul><li>ResNet10：3s</li><li>ResNet18：40s</li></ul></li></ul></li><li><p>应用：</p><ul><li>对抗样本检测</li><li>模型剪枝</li><li>模型保护</li></ul></li></ul></li></ul><h2 id="问题形式化定义"><a href="#问题形式化定义" class="headerlink" title="问题形式化定义"></a>问题形式化定义</h2><ul><li>符号</li></ul><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/1.png" style="zoom:70%;"></p><ul><li><p>程序切片</p><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/2.png" alt="2" style="zoom:70%;"></p></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="Profiling-和-Forward-analysis阶段"><a href="#Profiling-和-Forward-analysis阶段" class="headerlink" title="Profiling 和 Forward analysis阶段"></a>Profiling 和 Forward analysis阶段</h3><ul><li>Profiling阶段：记录每个神经元在训练样本上的输出均值$\overline{y^{n}( \mathcal{D})}$（卷积层一个通道看作一个神经元）</li><li>Forward analysis阶段：对特定的输入样本，计算神经元n的输出值与均值的差$ \Delta y^{n}( \xi )=y^{n}( \xi )- \overline{y^{n}( \mathcal{D} )} $。绝对值越大表示越敏感。</li></ul><h3 id="Backward-Analysis-和切片提取"><a href="#Backward-Analysis-和切片提取" class="headerlink" title="Backward Analysis 和切片提取"></a>Backward Analysis 和切片提取</h3><ul><li><p>对每个神经元和连接都计算一个贡献值$CONTRIB$，$CONTRIB \neq 0$表示该神经元或连接是重要的，$CONTRIB&gt;0$表示贡献值是正向的，否则是负向的。</p></li><li><p>$CONTRIB$的计算方法</p><ul><li><p><strong>递归</strong>地从后向前计算</p><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/4.png" alt="4" style="zoom:67%;"></p></li><li><p>局部贡献值计算</p><ul><li>如Weighted sum操作: 对神经元$n$，前一层第$i$个神经元的局部贡献值$contrib_{i}=CONTRIB_{n} \times \Delta y^{n} \times w_{i} \Delta x_{i} $，其中$CONTRIB_n$为该神经元$n$当前的全局贡献值，$\Delta y^{n}$为该神经元$n$在forward analysis阶段计算出的相对敏感度，$\Delta x_{i}$为前一层神经元$n_i$的相对敏感度，即$\Delta y^{n_i}$。乘积$w_{i} \Delta x_{i}$表示$n_i$和$s_i$对全局贡献值$CONTRIB_n$的影响，比如，如果$\Delta y^{n}$是负的，$w_{i} \Delta x_{i}$是正的，说明$n_i$扩大了$n$的负性，对于$CONTRIB_n$有负向贡献。</li></ul><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/5.png" alt="5" style="zoom:67%;"></p></li><li><p>全局贡献值=SIGN(局部贡献值)的累加，即局部贡献值只在{-1,0,1}中取值</p></li></ul></li><li><p>参数$\theta$控制切片粒度：抛弃贡献值低于$\theta$的前层神经元</p></li><li><p>多个样本的集合：贡献值=单个样本贡献值的和</p></li><li><p>切片：去掉贡献值为0的神经元和连接。</p></li></ul><h3 id="GPU和多线程加速"><a href="#GPU和多线程加速" class="headerlink" title="GPU和多线程加速"></a>GPU和多线程加速</h3><p>对于大样本集合$I$，前向分析过程在GPU上大批次处理，反向分析过程在CPU上一个小批次用一个线程处理。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="Overhead"><a href="#Overhead" class="headerlink" title="Overhead"></a>Overhead</h3><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/6.png" alt="6" style="zoom:67%;"></p><h3 id="对抗防御"><a href="#对抗防御" class="headerlink" title="对抗防御"></a>对抗防御</h3><ul><li><p>思想：对抗样本和正常样本的切片应该不同。从正常样本集合我们可获得一个切片，若新的输入切片与正常样本切片非常不同，则很有可能是对抗样本。</p></li><li><font color="red">方法：假设DNN模型为$\mathcal{M}$，输入样本为$\xi$，其在$\mathcal{M}$上预测的输出为$\mathcal{M}(\xi)$，对于切片标准$\mathcal{C}=(\xi, \mathcal{M}(\xi))$，使用NNSlicer方法得到切片$\mathcal{M}_\xi$。构造一个切片分类器$F$，其在训练样本上学习得到切片形状与输出类别之间的关系，若$F(M_\xi) \neq \mathcal{M(\xi)}$，则判断该样本为对抗样本。</font><ul><li>$F$的输入是一个切片$M_\xi$，具体表示为一个向量$vec_\xi$，即所有神经元之间的连接及该连接的贡献的集合。</li><li>本文采用决策树算法构造分类器$F$。</li></ul></li><li><p>用NNSlicer检测对抗样本的好处</p><ul><li>不需要更改或重训练模型</li><li>支持大型模型</li><li>只需要正常样本来训练分类器，不需要对抗样本</li></ul></li><li><p>实验设置</p><ul><li>对比方法：FeatureMap和EffectivePath，都使用了分类器</li><li>数据集和模型：CIFAR10和ResNet10，10000张正常样本训练分类器。</li><li>对抗攻击方法：17种（FoolBox实现）<ul><li>FGSM_2，FGSM_4，FGSM_8</li><li>DeepFoolL2，DeepFoolLinf</li><li>JSMA</li><li>RPGD_2,RPGD_4,RPGD_8</li><li>CWL2</li><li>ADef</li><li>SinglePixel</li><li>LocalSearch</li><li>Boundary</li><li>Spatial</li><li>PointWise</li><li>GaussianBlur</li></ul></li></ul></li><li><p>实验结果</p><ul><li>平均Recall为100%，平均Precision为83%</li></ul><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/7.png" alt="7" style="zoom:60%;"></p></li></ul><h3 id="网络简化和剪枝"><a href="#网络简化和剪枝" class="headerlink" title="网络简化和剪枝"></a>网络简化和剪枝</h3><ul><li>一般的简化方法针对所有类别，NNSlicer针对特定的几个类别简化（如区分ImageNet里的各种狗）</li><li>方法：假设我们想对模型$\mathcal{M}$进行简化，目标类别为$\mathcal{O}^T$，剪枝比例为$r$。令$\mathcal{I}^T$为目标类别的输入样本，使用NNSlicer计算每个连接s的贡献$CONTRIB_s$，对每层每个连接的贡献值排序，剪掉最小的比例为$r$的这些连接，如果一个神经元的所有连接都被剪掉，则该神经元也被去掉。</li><li>实验设计：CIFAR10的10个类别中所有子集（210个）都作为目标类别，<ul><li>对比方法<ul><li>EffectivePath</li><li>Weight：去掉权重绝对值最小的边</li><li>Channel：去掉平均权值最小的神经元</li></ul></li></ul></li><li>实验结果：NNSlicer能保持准确率降低较少，这是因为我们牺牲其他类别的连接来保障目标类别的准确率能缓慢降低</li></ul><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/8.png" alt="8" style="zoom:60%;"></p><ul><li><p>一轮微调：用10k个样本重训练一轮，即使剪枝比例很大，但NNSlicer已能达到较高的准确率</p><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/9.png" alt="9" style="zoom:60%;"></p></li></ul><h3 id="模型保护"><a href="#模型保护" class="headerlink" title="模型保护"></a>模型保护</h3><ul><li><p>保护贡献值大的连接</p></li><li><p>对比方法</p><ul><li>EffectivePath</li><li>Weight</li><li>Random：随机选择连接</li></ul></li><li><p>实验设计：假设50%的连接参数被隐藏，攻击者试图用训练数据重训练的方式恢复连接参数，重训练准确率越低表示保护方法越好。</p></li><li><p>实验结果</p><ul><li>Target classes上准确率非常低，但All classes上准确率与Weight接近，说明非目标类上准确率可能很高，但NNSlicer不保护非目标类。</li></ul><p><img src="/2020/11/10/Dynamic-Slicing-for-DNN/10.png" alt="10" style="zoom:60%;"></p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/DNN%E6%B5%8B%E8%AF%95/">DNN测试</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E7%A8%8B%E5%BA%8F%E5%88%87%E7%89%87/">程序切片</category>
      
      
      <comments>https://rubychen0611.github.io/2020/11/10/Dynamic-Slicing-for-DNN/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【算法复习】基础知识——算法分析、分治策略</title>
      <link>https://rubychen0611.github.io/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link>
      <guid>https://rubychen0611.github.io/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid>
      <pubDate>Wed, 04 Nov 2020 12:51:12 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;算法&quot;&gt;&lt;a href=&quot;#算法&quot; class=&quot;headerlink&quot; title=&quot;算法&quot;&gt;&lt;/a&gt;算法&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;算法&lt;/strong&gt;的定义：定义良好的计算过程。即一系列的计算步骤，用来将输入数据转换成输出结果。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;正确的算法：如果一个算法对其每一个输入实例，都能输出正确的结果并停止，则称它是正确的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><ul><li><p><strong>算法</strong>的定义：定义良好的计算过程。即一系列的计算步骤，用来将输入数据转换成输出结果。</p></li><li><p>正确的算法：如果一个算法对其每一个输入实例，都能输出正确的结果并停止，则称它是正确的。</p></li></ul><a id="more"></a><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2><ul><li>算法的运行时间：特定输入时，算法执行的基本操作数（步数）。</li><li><p>一般考察算法的最坏运行时间</p></li><li><p>$\Theta$记号：</p><ul><li><p>$\Theta(g(n))=\{f(n):存在正常量c_1、c_2和n_0，使得对于所有n\geq n_0，有0 \leq c_1g(n) \leq f(n)\leq c_2g(n)\}$</p><p><img src="/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/算法分析1.png" alt="算法分析1" style="zoom:70%;"></p></li></ul></li><li><p>$O$记号和$\Omega$记号</p><ul><li>$O(g(n))=\{f(n):存在正常量c和n_0，使得对于所有n\geq n_0，有0 \leq f(n)\leq cg(n)\}$</li><li>$\Omega(g(n))=\{f(n):存在正常量c和n_0，使得对于所有n\geq n_0，有0 \leq cg(n) \leq f(n)\}$</li></ul></li><li><p>$f(n)=\Theta(g(n))$当且仅当$f(n)=O(g(n))$且$f(n)=\Omega(g(n))$</p></li></ul><h2 id="分治策略（divide-and-conquer"><a href="#分治策略（divide-and-conquer" class="headerlink" title="分治策略（divide-and-conquer)"></a>分治策略（divide-and-conquer)</h2><p>把原问题划分成n个规模较小而结构与原问题相似的子问题；递归地解决这些问题，然后合并其结果，就得到原问题的解。</p><ul><li>分解（Divide）：划分子问题</li><li>解决（Conquer）：递归地求解子问题</li><li>合并（Combine）：合并子问题的解</li></ul><h3 id="最大子数组问题"><a href="#最大子数组问题" class="headerlink" title="最大子数组问题"></a>最大子数组问题</h3><p>分治法求解数组A的和最大的非空连续子数组。</p><ul><li><p>三种情况：</p><p><img src="/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/最大子数组1.png" alt="最大子数组1" style="zoom:75%;"></p></li><li><p>求解跨越中点的子数组（$\Theta(n)$）：必须包含中点，向左右依次寻找最大数组然后合并即可</p><p><img src="/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/最大子数组2.png" alt="最大子数组2" style="zoom:80%;"></p></li><li><p>递归求解：</p><p><img src="/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/最大子数组3.png" alt="最大子数组3" style="zoom:80%;"></p></li><li><p>时间开销：$\Theta(nlgn)$</p></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LeetCode 53: 最大子序和</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> MAX_INT = <span class="number">0xFFFFFFF</span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">max</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b, <span class="keyword">int</span> c)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span> (a &gt;= b &amp;&amp; a &gt;= c)</span><br><span class="line">            <span class="keyword">return</span> a;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (b &gt;= a &amp;&amp; b &gt;= c)</span><br><span class="line">            <span class="keyword">return</span> b;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> c;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findCrossingMaxSumArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> mid, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">int</span> left_max = -MAX_INT, right_max = -MAX_INT, left_sum = <span class="number">0</span>, right_sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = mid; i &gt;= left; i--)</span><br><span class="line">        {</span><br><span class="line">            left_sum += a[i];</span><br><span class="line">            <span class="keyword">if</span> (left_sum &gt; left_max)</span><br><span class="line">                left_max = left_sum;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = mid+<span class="number">1</span>; i &lt;= right; i++)</span><br><span class="line">        {</span><br><span class="line">            right_sum += a[i];</span><br><span class="line">            <span class="keyword">if</span> (right_sum &gt; right_max)</span><br><span class="line">                right_max = right_sum;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> left_max + right_max;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findMaxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span>(left == right)</span><br><span class="line">            <span class="keyword">return</span> a[left];</span><br><span class="line">        <span class="keyword">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">int</span> left_max = findMaxSubArray(a, left, mid);</span><br><span class="line">        <span class="keyword">int</span> right_max = findMaxSubArray(a, mid+<span class="number">1</span>, right);</span><br><span class="line">        <span class="keyword">int</span> crossing_max = findCrossingMaxSumArray(a, left, mid, right);</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(left_max, right_max, crossing_max);</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>{</span><br><span class="line">        <span class="keyword">return</span> findMaxSubArray(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>()<span class="number">-1</span>);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="矩阵乘法的Strassen算法"><a href="#矩阵乘法的Strassen算法" class="headerlink" title="矩阵乘法的Strassen算法"></a>矩阵乘法的Strassen算法</h3><ul><li>减少一次递归，时间开销：$\Theta(n^{lg7})$，即$O(n^{2.81})$</li></ul><h3 id="求解递归式的方法"><a href="#求解递归式的方法" class="headerlink" title="求解递归式的方法"></a>求解递归式的方法</h3><ul><li>代入法：猜测解的形式，数学归纳法证明</li><li>递归树</li><li>主方法<ul><li>$T(n) = aT(n/b) + f(n)$</li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E7%AE%97%E6%B3%95/">算法</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/">算法导论</category>
      
      
      <comments>https://rubychen0611.github.io/2020/11/04/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】mMutant</title>
      <link>https://rubychen0611.github.io/2020/11/02/mMutant/</link>
      <guid>https://rubychen0611.github.io/2020/11/02/mMutant/</guid>
      <pubDate>Mon, 02 Nov 2020 02:52:48 GMT</pubDate>
      
      <description>&lt;p&gt;原文：Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing  (ICSE’19)&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>原文：Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing  (ICSE’19)  <a id="more"></a></p><p>代码地址：<a href="https://github.com/dgl-prc/m_testing_adversatial_sample">https://github.com/dgl-prc/m_testing_adversatial_sample</a></p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>目前对抗样本检测方法：</p><ul><li>训练数据扩充+重训练：不能防止未知的对抗样本</li><li>鲁棒优化、对抗训练：增加了训练成本</li><li>测试标准、黑盒测试、白盒测试、concolic测试等：不能提升DNN鲁棒性，也不能为DNN在被对抗攻击时提供鲁棒性保证</li><li>形式化验证DNN的鲁棒性：高成本、只针对少量特定类型DNN和特定性质</li></ul><p>本文：</p><ul><li>通过DNN模型的变异测试和提出了一种运行时检测对抗样本的方法。</li><li>基于的观察：相比正常样本，对抗样本对于模型的变异更加敏感（预测标签更易改变）</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="模型变异方法"><a href="#模型变异方法" class="headerlink" title="模型变异方法"></a>模型变异方法</h3><p>采用DeepMutation中的4个模型级变异方法（无须训练）：</p><ul><li><p>Gaussian Fuzzing（GF）：根据高斯分布扰动权重</p><p><img src="/2020/11/02/mMutant/4.png" alt="4" style="zoom:70%;"></p></li><li><p>Weight Shuffing（WS）：随机排列选中的权重</p><p><img src="/2020/11/02/mMutant/5.png" alt="5" style="zoom:67%;"></p></li><li><p>Neuron Switch（NS）：交换同一层的两个神经元</p><p><img src="/2020/11/02/mMutant/6.png" alt="6" style="zoom:67%;"></p></li><li><p>Neuron Activation Inverse（NAI）：变更神经元的激活状态</p><p><img src="/2020/11/02/mMutant/3.png" alt="3" style="zoom:70%;"></p></li></ul><h3 id="验证假设（Empirical-study）"><a href="#验证假设（Empirical-study）" class="headerlink" title="验证假设（Empirical study）"></a>验证假设（Empirical study）</h3><p>通过实验验证对抗样本和正常样本的标签改变率（Label change rate， LCR）的差异。由于生成的变异模型可能存在质量很低的情况，因此我们扔掉了在验证集上低准确率的模型（如准确率低于90%的）。</p><p>令变异后的模型集合为$F$，变异模型$f_i$对于输入$x$的预测标签$f_i(x)$，LCR的计算方法：</p><p>$\zeta ( x ) = \frac { | \{ f _ { i } | f _ { i } \in F \text { and } f _ { i } ( x ) \neq f ( x ) \} | } { | F | }$</p><p>直觉上，LCR衡量了一个输入$x$对于DNN变异模型的敏感程度。</p><p><strong>在MNIST和CIFAR-10上的实证研究</strong></p><ul><li>使用NAI变异方法生成了500个变异模型（随机选择一些神经元，更改激活状态）</li><li>实验结果：对抗样本的预测标签更易受到扰动<img src="/2020/11/02/mMutant/1.png" alt="1"></li><li>解释：对抗样本常处于决策边界附近，更易受到模型扰动的干扰。</li></ul><h3 id="检测算法"><a href="#检测算法" class="headerlink" title="检测算法"></a>检测算法</h3><ul><li><p>输入：一个DNN模型$f$，一个样本$x$，阈值$\zeta_ h$（可自动计算获得）</p></li><li><p>使用假设检验：</p><p>​    $ H_{0}: \zeta (x) \geqslant \zeta_ h $</p><p>​    $ H_{1}: \zeta (x) \leqslant \zeta_ h $</p><ul><li>三个标准参数$\alpha$、$\beta$、$\delta$，控制错误可能性<ul><li>Type I 错误（H0为真但拒绝H0）的可能性小于$\alpha$</li><li>Type II 错误（H1为真但拒绝H1）的可能性小于$\beta$</li><li>indifferent region：$(r-\delta,r+\delta)$</li></ul></li></ul></li><li><p>不断生成mutate后的模型（准确率大于阈值），直到触发停止条件。（模型可保存使用，无需运行时生成）</p></li><li><p>两种方法决定算法是否停止（我们有足够信心拒绝假设）</p><ul><li><p>1、Fixed-size Sampling Test (FSST)：运行固定数量的测试</p></li><li><p>2、Sequential Probability Ratio Test (SPRT)：模型数量不固定，每次更新阈值$\zeta_ h$后自动判断是否停止，一般运行速度更快（拒绝假设后就停止运行）。</p><ul><li>SPRT概率计算方法：$ pr= \frac{p_{1}^{z}(1-p_{1})^{n-z}}{p_{0}^{z}(1-p_{0})^{n-z}} $，其中$p_1=\zeta_ h - \delta$，$p_0=\zeta_ h + \delta$</li></ul><p><img src="/2020/11/02/mMutant/2.png" alt="2" style="zoom:67%;"></p></li></ul></li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul><li><p>模型和数据集</p><ul><li>MNIST-LeNet：236个神经元</li><li>CIFAR10-GoogleNet：7914个神经元</li></ul></li><li><p>变异率</p><ul><li>MNIST模型：{0.01,0.03,0.05}</li><li>CIFAR10模型：{0.003,0.005,0.007}</li></ul></li><li><p>保留测试集上准确率高于90%的变异模型。每种变异率生成500个变异模型。</p></li><li><p>对抗样本生成方法</p><ul><li>FGSM</li><li>JSMA</li><li>C&amp;W</li><li>DeepFool</li><li>Black-Box</li></ul></li><li><p>每种攻击方法生成1000张图片（不一定都攻击成功）</p><p><img src="/2020/11/02/mMutant/7.png" alt="7" style="zoom:75%;"></p></li><li><p>评价指标</p><ul><li>标签改变率的距离$d_{lcr}=\zeta_{adv}/\zeta_{nor}$，越大表明对抗样本与正常样本的标签改变率差异越大</li><li>ROC、AUC</li><li>检测准确率：二分类（正常/异常样本）准确率</li></ul></li></ul><h3 id="RQ1-正常样本和对抗样本的LCR是否有显著差异？"><a href="#RQ1-正常样本和对抗样本的LCR是否有显著差异？" class="headerlink" title="RQ1 正常样本和对抗样本的LCR是否有显著差异？"></a>RQ1 正常样本和对抗样本的LCR是否有显著差异？</h3><p>结合Table II和IV可见，对抗样本LCR显著高于正常样本。</p><p><img src="/2020/11/02/mMutant/8.png" alt="8" style="zoom:80%;"></p><p><img src="/2020/11/02/mMutant/9.png" alt="9"></p><h3 id="RQ2：LCR是否适合作为检测指标？"><a href="#RQ2：LCR是否适合作为检测指标？" class="headerlink" title="RQ2：LCR是否适合作为检测指标？"></a>RQ2：LCR是否适合作为检测指标？</h3><p>多数情况下LCR比baseline的AUROC更高。</p><p><img src="/2020/11/02/mMutant/10.png" alt="10" style="zoom:80%;"></p><h3 id="RQ3：检测对抗样本的效果如何？"><a href="#RQ3：检测对抗样本的效果如何？" class="headerlink" title="RQ3：检测对抗样本的效果如何？"></a>RQ3：检测对抗样本的效果如何？</h3><p>检测精度和最少mutation次数：</p><ul><li>设置$\zeta_h=\rho \cdot \zeta_{nor}$，其中$\zeta_{nor}$为正常样本lcr的上界</li></ul><p><img src="/2020/11/02/mMutant/11.png" alt="11"></p><h3 id="RQ4：检测算法的开销如何？"><a href="#RQ4：检测算法的开销如何？" class="headerlink" title="RQ4：检测算法的开销如何？"></a>RQ4：检测算法的开销如何？</h3><p>$c_g$：生成变异模型的时间</p><p>$c_f$：前向传播预测的时间</p><p>检测一张图片的时间开销：$C=n(c_g+c_f)$，$n$为生成模型的个数</p><p>检测$m$张图片的时间开销：$C(m)=m\cdot n \cdot c_f + n \cdot c_g$</p><p><img src="/2020/11/02/mMutant/12.png" alt="12" style="zoom:80%;"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/DNN%E6%B5%8B%E8%AF%95/">DNN测试</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E5%8F%98%E5%BC%82%E6%B5%8B%E8%AF%95/">变异测试</category>
      
      
      <comments>https://rubychen0611.github.io/2020/11/02/mMutant/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Windows后台运行Python脚本，开机自动运行</title>
      <link>https://rubychen0611.github.io/2020/10/14/Windows%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8CPython/</link>
      <guid>https://rubychen0611.github.io/2020/10/14/Windows%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8CPython/</guid>
      <pubDate>Wed, 14 Oct 2020 03:15:08 GMT</pubDate>
      
      <description>&lt;p&gt;为了每天自动约洗澡也是拼了&lt;span class=&quot;github-emoji&quot; style=&quot;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8) center/contain&quot; data-src=&quot;https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8&quot;&gt;😅&lt;/span&gt;&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>为了每天自动约洗澡也是拼了<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8">😅</span> <a id="more"></a></p><h2 id="后台运行python脚本"><a href="#后台运行python脚本" class="headerlink" title="后台运行python脚本"></a>后台运行python脚本</h2><p>已知目前有一每天定时预约洗澡的Python脚本shower.py，cmd设置后台运行：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pythonw shower.py</span><br></pre></td></tr></tbody></table></figure><p>pythonw.exe是无窗口的Python可执行程序，意思是在运行程序的时候，没有窗口，代码在后台执行。</p><p>注意如果像我一样电脑同时安装了Python2 和Python3，需要区分用的是哪个phthonw.exe，最简单的是使用绝对路径：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:\python37\pythonw.exe E:\shower.py</span><br></pre></td></tr></tbody></table></figure><p>可打开任务管理器检查pythonw进程是否已经启动。</p><h2 id="设置开机自动启动"><a href="#设置开机自动启动" class="headerlink" title="设置开机自动启动"></a>设置开机自动启动</h2><p>1、新建批处理文件run_shower.bat：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@echo off </span><br><span class="line">if "%1"=="h" goto begin </span><br><span class="line">start mshta vbscript:createobject("wscript.shell").run("""%~nx0"" h",0)(window.close)&amp;&amp;exit </span><br><span class="line">:begin </span><br><span class="line">::</span><br><span class="line">start /b cmd /k "D:\python37\pythonw.exe E:\shower.py"</span><br></pre></td></tr></tbody></table></figure><p>这段代码可以隐藏批处理运行的窗口。</p><p>解释：</p><blockquote><p>如果双击一个批处理，等价于参数为空，而一些应用程序需要参数，比如在cmd窗口输入shutdowm -s -t 0,其中-s -t 0就为参数。shutdown为%0，-s为%1，-t为%2，以此类推。<br>第一行我们先跳过，看第二行，表示利用mshta创建一个vbs程序，内容为:createobject(“wscript.shell”).run(……).<br>如果运行的批处理名为a.bat，在C:\下，那%0代表C:\a.bat，%~nx0代表a.bat。h为参数%1，0表示隐藏运行。<br>由于你双击运行，故第一次批处理%1为空，if不成立，转而运行下一句。然后再次打开自己，并传递参数h，此时if成立，跳转至begin开始运行。<br>这两行很经典，可以使批处理无窗口运行。</p></blockquote><p>2、将bat文件放在开机启动项里：Win+R打开运行窗口，输入shell:startup，将bat文件复制进启动文件夹里。</p><p>3、重启测试</p><p>参考：<a href="https://www.cnblogs.com/nmap/articles/8329125.html">https://www.cnblogs.com/nmap/articles/8329125.html</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/">常用技巧</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/Python/">Python</category>
      
      
      <comments>https://rubychen0611.github.io/2020/10/14/Windows%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8CPython/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】Compiler Validation via Equivalence Modulo Inputs</title>
      <link>https://rubychen0611.github.io/2020/10/08/EMI/</link>
      <guid>https://rubychen0611.github.io/2020/10/08/EMI/</guid>
      <pubDate>Thu, 08 Oct 2020 07:35:46 GMT</pubDate>
      
      <description>&lt;p&gt;原文：Compiler Validation via Equivalence Modulo Inputs （PLDI’14)&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>原文：Compiler Validation via Equivalence Modulo Inputs （PLDI’14) <a id="more"></a></p><h2 id="Equivalence-Modulo-Inputs-EMI"><a href="#Equivalence-Modulo-Inputs-EMI" class="headerlink" title="Equivalence Modulo Inputs (EMI)"></a>Equivalence Modulo Inputs (EMI)</h2><ul><li><p>给定程序$P$和一组输入值$I$，由$I$可生成一组程序集合$\mathcal{P}$，使得$\mathcal{P}$中每个程序$Q$都等价于$P$ modulo $I$：$\forall i \in I, Q(i) = P(i) $。则集合$\mathcal{P}$可用于对任意一个编译器$Comp$进行差分测试（differential testing），若存在某个$i \in I$和$Q \in \mathcal{P}$，使得$Comp(P)(i) \neq Comp(Q)(i)$，则该编译器存在bug。</p></li><li><p>核心思想：尽管$Q$只在输入集合$I$上与程序$P$语义等价，但编译器及其使用的静态分析和优化算法应该能为$Q$生成能在$I$上完全运行正确的中间代码。$P$和$Q$在数据流和控制流上可能很不同，经编译器优化后生成的代码也很不同，但结果应该完全一致。</p><p><img src="/2020/10/08/EMI/1.png" alt="emi" style="zoom:60%;"></p></li><li><p>生成EMI 变体的策略</p><ul><li>在$P$上运行输入集合$I$，获得运行轨迹，随机在未执行代码上做剪枝、插入、修改操作（假设$P$是一个确定的程序）</li></ul></li></ul><p>编译器的两类bug：</p><ul><li>导致编译器崩溃</li><li>生成错误代码（更加严重）<ul><li>导致正确的程序运行有bug</li><li>难以发现</li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E7%BC%96%E8%AF%91%E5%99%A8%E6%B5%8B%E8%AF%95/">编译器测试</category>
      
      
      <comments>https://rubychen0611.github.io/2020/10/08/EMI/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>《计算模型导引》复习笔记</title>
      <link>https://rubychen0611.github.io/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/</link>
      <guid>https://rubychen0611.github.io/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/</guid>
      <pubDate>Sun, 06 Sep 2020 02:22:35 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;第一章-递归函数&quot;&gt;&lt;a href=&quot;#第一章-递归函数&quot; class=&quot;headerlink&quot; title=&quot;第一章 递归函数&quot;&gt;&lt;/a&gt;第一章 递归函数&lt;/h2&gt;&lt;h3 id=&quot;1-1-数论函数（P1）&quot;&gt;&lt;a href=&quot;#1-1-数论函数（P1）&quot; class=&quot;headerlink&quot; title=&quot;1.1 数论函数（P1）&quot;&gt;&lt;/a&gt;1.1 数论函数（P1）&lt;/h3&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="第一章-递归函数"><a href="#第一章-递归函数" class="headerlink" title="第一章 递归函数"></a>第一章 递归函数</h2><h3 id="1-1-数论函数（P1）"><a href="#1-1-数论函数（P1）" class="headerlink" title="1.1 数论函数（P1）"></a>1.1 数论函数（P1）<a id="more"></a></h3><ul><li><p>数论函数</p><ul><li><p>k元数论全函数</p></li><li><p>部分数论函数</p></li></ul></li><li><font color="red">本原函数（$\mathcal{IF}$）</font><ul><li><p>零函数$Z$</p></li><li><p>后继函数$S$</p></li><li><p>投影函数$P_i^n$</p></li></ul></li><li><p>常用数论函数（P2）</p><ul><li>前驱函数pred</li><li>加法函数add</li><li><strong>算术差函数sub</strong></li><li>绝对差函数diff</li><li>乘法函数mul</li><li>除法函数div</li><li>求余函数rs</li><li>指数函数pow</li><li>平方函数sq</li><li>$E(x)=x-\lfloor \sqrt{x}\rfloor$</li><li>max、min</li><li>最大公约数函数gcd、最小公倍数函数lcm</li><li><strong>素数枚举函数$P$</strong></li><li><strong>$ep(n,x)$：x的素因子分解式中第n个素数的指数</strong></li><li>$eq(x,y)$：相等时等于0，否则为1</li><li><strong>$N$：否定，x=0时为1，否则为0</strong></li><li><strong>$N^2$：x=0时为0，否则为1</strong></li></ul></li><li><p>函数的复合（P4）</p></li><li>有界迭加算子$\sum$</li><li>有界迭乘算子$\prod$</li><li>有界$\mu-$算子</li><li>有界$max-$算子</li><li><font color="red">基本函数类$\mathcal{BF}$</font>（P6）<ul><li>$\mathcal{IF} \subseteq \mathcal{BF}$</li><li>$\mathcal{BF}$对于复合封闭</li></ul></li></ul><h3 id="1-2-配对函数（P7）"><a href="#1-2-配对函数（P7）" class="headerlink" title="1.2 配对函数（P7）"></a>1.2 配对函数（P7）</h3><ul><li>配对函数、配对函数组</li><li>Godel编码</li><li>若配对函数组$\{pg,K,L\}$使pg穷尽一切自然数，则称该配对函数组是一一对应的<ul><li>康托编码（P9）</li></ul></li><li>多元配对函数（P11）</li><li>Godel $\beta-$函数（P13）<ul><li>定理1.11：有穷数列的编码和解码（中国剩余定理）</li></ul></li></ul><h3 id="1-3-初等函数（P14）"><a href="#1-3-初等函数（P14）" class="headerlink" title="1.3 初等函数（P14）"></a>1.3 初等函数（P14）</h3><ul><li><font color="red">初等函数类$\mathcal{EF}$</font><ul><li>定义<ul><li>$\mathcal{IF} \subseteq \mathcal{EF}$</li><li>$x+y,x−y(绝对差),x×y,⌊x/y⌋∈\mathcal{EF}$<ul><li>加、乘、除可省</li></ul></li><li>$\mathcal{EF}$对于复合，有界迭加算子$∑[⋅]$和有界迭乘算子$∏[⋅]$封闭</li></ul></li><li>性质<ul><li>$\mathcal{EF}$对于有界$\mu-$算子和max-算子封闭</li></ul></li></ul></li><li>数论谓词<ul><li>数论谓词的特征函数：真为0，假为1</li><li>初等数论谓词：若谓词$P$的特征函数属于$\mathcal{EF}$，则称$P$是初等的</li></ul></li><li>数论集合<ul><li>数论集合的特征函数：属于集合为0，不属于为1</li><li>初等数论集合：若数论集合$S$的特征函数属于$\mathcal{EF}$，则称$S$是初等的</li></ul></li><li>重要的初等函数（P19）<ul><li>$x^y$</li><li>$\lfloor \sqrt[y] x \rfloor$</li><li>余数$rs(x,y)$</li><li>$\tau (x)$：x因子的数目</li><li>$prime(x)$：判定x是否为素数（数论谓词）</li><li>$\pi (x)$：不超过x的素数个数</li><li>素数枚举函数$P(n)$=第n个素数</li><li>$ep(n,x)$</li></ul></li><li>控制函数（P24）<ul><li>$\mathcal{EF}$的控制函数G（P22）<ul><li>控制函数不属于$\mathcal{EF}$</li></ul></li></ul></li></ul><h3 id="1-4-原始递归函数（P25）"><a href="#1-4-原始递归函数（P25）" class="headerlink" title="1.4 原始递归函数（P25）"></a>1.4 原始递归函数（P25）</h3><ul><li>原始递归算子<ul><li>带参原始递归算子</li><li>无参原始递归算子</li></ul></li><li><font color="red">原始递归函数类$\mathcal{PRF}$</font><ul><li>定义<ul><li>$\mathcal{IF} \subseteq \mathcal{PRF}$</li><li>$\mathcal{PRF}$对于复合、带参原始递归算子、无参原始递归算子封闭</li></ul></li><li>重要的原始递归函数<ul><li>add、pred、sub、diff、mul、sq（平方）、N、$N^2$、sqrt、E</li></ul></li></ul></li><li>原始复迭算子It​、弱原始复迭算子​Itw​ (P30)</li><li><font color="red">原始复迭函数类$\mathcal{ITF}$</font><ul><li>定义<ul><li>$\mathcal{IF} \subseteq \mathcal{ITF}$</li><li>$\mathcal{ITF}$对于复合、原始复迭算子It[·]、弱原始复迭算子Itw[·]封闭</li></ul></li><li>$\mathcal{ITF=PRF}$</li></ul></li><li>若干形式不同的递归式化归到原始递归式（P34）<ul><li>串值递归</li><li>联立递归</li><li>变参递归</li><li>多重递归</li></ul></li><li>$\mathcal{PRF}$的控制函数<ul><li>Ackermann函数：$Ack(m,n)$（P38）<ul><li>不是原始递归函数</li></ul></li></ul></li><li>$\mathcal{EF} \subset \mathcal{PRF}$ (P41)<ul><li>真包含：$\mathcal{EF}$的控制函数G属于$\mathcal{PRF}$</li></ul></li></ul><h3 id="1-5-递归函数（P42）"><a href="#1-5-递归函数（P42）" class="headerlink" title="1.5 递归函数（P42）"></a>1.5 递归函数（P42）</h3><ul><li>正则函数（全函数）、正则$\mu-$算子（区别于有界$\mu-$算子）</li><li><font color="red">一般递归函数类$\mathcal{GRF}$</font><ul><li>定义<ul><li>$\mathcal{IF} \subseteq \mathcal{GRF}$</li><li>$\mathcal{GRF}$对于复合和原始递归算子（带参和无参）封闭</li><li><u>$\mathcal{GRF}$对于正则$\mu-$算子封闭</u></li></ul></li></ul></li><li>$\mu-$算子（部分函数）</li><li><font color="red">部分递归函数类$\mathcal{RF}$</font> （P43）<ul><li>定义<ul><li>$\mathcal{IF} \subseteq \mathcal{RF}$</li><li>$\mathcal{RF}$对于复合和原始递归算子（带参和无参）封闭</li><li><u>$\mathcal{RF}$对于$\mu-$算子封闭</u></li></ul></li><li>显然$\mathcal{GRF} \subset \mathcal{RF}$ ：正则$\mu-$算子是$\mu-$算子的特例</li></ul></li><li>递归数论谓词、$\mu-$谓词、递归集</li><li>$\mathcal{PRF} \subset \mathcal{GRF}$ <ul><li>法1：利用控制函数，证明Ackermann函数是一般递归函数但不是原始递归函数（P45）</li><li>法2：利用通用函数（P47）</li></ul></li><li>$\mathcal{GRF} \subset 全数论函数\mathcal{NTF}$ （P48）<ul><li>存在数论全函数f，f不是一般递归函数 </li></ul></li></ul><h3 id="1-6-结论（P48）"><a href="#1-6-结论（P48）" class="headerlink" title="1.6 结论（P48）"></a>1.6 结论（P48）</h3><script type="math/tex; mode=display">本原函数类\mathcal{IF} \subset 基本函数类\mathcal{BF} \subset 初等函数类\mathcal{EF} \subset 原始递归函数类\mathcal{PRF} = 原始复迭函数类\mathcal{ITF} \subset 一般递归函数类\mathcal{GRF} \subset 部分递归函数类\mathcal{RF}</script><h2 id="第三章-lambda-演算"><a href="#第三章-lambda-演算" class="headerlink" title="第三章 $\lambda-$演算"></a>第三章 $\lambda-$演算</h2><h3 id="3-1-lambda-演算的语法（P72）"><a href="#3-1-lambda-演算的语法（P72）" class="headerlink" title="3.1 $\lambda-$演算的语法（P72）"></a>3.1 $\lambda-$演算的语法（P72）</h3><ul><li>变量：小写字母，表示一个参数（形参）或者一个值（实参）</li><li>抽象：$\lambda x.M$，绑定变量x于该函数抽象的函数体M，简单来说就是表示一个<u>形参</u>为x的函数M。</li><li>作用：$MN$，表示将函数M应用于参数N，简单来说就是给函数M输入<u>实参</u>N。<ul><li>函数作用是左结合的，即：<code>M N P</code>意为<code>(M N) P</code>而非<code>M (N P)</code></li></ul></li><li>自由变元和约束变元（P74）<ul><li>在函数抽象中，形参绑定于函数体，即形参是约束变元，相对应地，不是约束变元的自然就是自由变元</li><li>闭$\lambda-$项：没有自由变元的项，$\Lambda ^ \circ$表示全体闭$\lambda-$项的集合</li><li>约束变元改名后仍等价</li><li>自由变元的替换：$M[x:=N]$，$x$为自由变元</li></ul></li></ul><h3 id="3-2-转换（P76）"><a href="#3-2-转换（P76）" class="headerlink" title="3.2 转换（P76）"></a>3.2 转换（P76）</h3><ul><li>形式理论$\lambda \beta$ (P76)<ul><li>标准组合子$I,K,K^*,S$</li></ul></li><li>形式理论$\lambda \beta + ext$ 和$\lambda \beta \eta$ (P79)</li></ul><h3 id="3-3-归约（P80）"><a href="#3-3-归约（P80）" class="headerlink" title="3.3 归约（P80）"></a>3.3 归约（P80）</h3><ul><li>$\to_R$（一步$R-$归约）、$\twoheadrightarrow_R$ （$R-$归约）、$=_R$（$R-$转换）</li><li>二元关系$\beta$、$\alpha$、$\eta$、$\beta \eta$<ul><li>$M =_\beta N \Leftrightarrow \lambda \beta \vdash M=N$</li></ul></li><li>$\beta-$可约式、$\beta-$范式（$\beta-$nf）、$\beta-$范式的集合（$NF_R$）、$M$有$\beta-$nf</li></ul><h3 id="3-4-Church-Rosser定理（P85）"><a href="#3-4-Church-Rosser定理（P85）" class="headerlink" title="3.4 Church-Rosser定理（P85）"></a>3.4 Church-Rosser定理（P85）</h3><ul><li>CR性质：$P \twoheadrightarrow M \bigwedge P \twoheadrightarrow N \Rightarrow \exists T.(M \twoheadrightarrow T \bigwedge N \twoheadrightarrow T)$<ul><li>对$\twoheadrightarrow_\beta$、$\twoheadrightarrow_\eta$、$\twoheadrightarrow_{\beta \eta}$成立</li></ul></li><li>con($\lambda \beta$)：存在推不出的公式</li></ul><h3 id="3-5-不动点定理（P93）"><a href="#3-5-不动点定理（P93）" class="headerlink" title="3.5 不动点定理（P93）"></a>3.5 不动点定理（P93）</h3><ul><li>不动点定理：对于任何的$F \in \Lambda$，存在$Z \in \Lambda$，使得$FZ =_\beta Z$</li><li>不动点组合子<ul><li>$Y$： 对于任何的$F \in \Lambda$，$F(YF)=_\beta YF$</li><li>$\Theta$：对于任何的$F \in \Lambda ^ \circ$，$\Theta F \twoheadrightarrow_\beta F(\Theta F)$</li></ul></li><li>$([M_1,…,M_n])^n_i \twoheadrightarrow_\beta M_i$</li></ul><h3 id="3-6-递归函数的-lambda-可定义性（P95）"><a href="#3-6-递归函数的-lambda-可定义性（P95）" class="headerlink" title="3.6 递归函数的$\lambda-$可定义性（P95）"></a>3.6 递归函数的$\lambda-$可定义性（P95）</h3><ul><li>Church数项：$\ulcorner n \urcorner \equiv \lambda fx.f^nx$</li><li>$\lambda-$可定义性：$F \ulcorner n_1 \urcorner …  \ulcorner n_k \urcorner =_\beta  \ulcorner f(n_1,…,n_k) \urcorner$</li><li>$D\ulcorner n \urcorner MN$ = if (n = 0) then M else N​ (P97)</li><li><strong>一般/部分递归函数是$\lambda-$可定义的</strong></li></ul><h3 id="3-7-与递归论对应的结果（P100）"><a href="#3-7-与递归论对应的结果（P100）" class="headerlink" title="3.7 与递归论对应的结果（P100）"></a>3.7 与递归论对应的结果（P100）</h3><ul><li>$\lambda-$项的编码：对每个$M \in \Lambda$ ，都有唯一的自然数$\sharp M$与之对应 （$\lambda-$项 $\rightarrow$ $\mathbb{N}$）</li><li>$\lambda-$项的内部编码：$M$的内部编码定义为$\ulcorner M \urcorner \equiv \ulcorner \sharp M \urcorner$ （$\lambda-$项 $\rightarrow$ $\lambda-$项)</li><li>枚举子$E$，对于任何$M \in \Lambda^ \circ$，有$E\ulcorner M \urcorner =_\beta M$</li><li>第二不动点定理：$\forall F. \exists Z. F \ulcorner Z \urcorner =_ \beta Z$</li><li><strong>不可判定性</strong>（P102）<ul><li>若自然数集S的特征函数$\mathcal{X}_s \in \mathcal{GRF}$，则称$S$是可判定的</li><li>若$\lambda-$项集合$\mathcal{A} \subseteq  \Lambda$的编码集合是可判定的，则称$\mathcal{A}$是可判定的</li><li>设$\mathcal{A} \subseteq  \Lambda$非平凡、$\mathcal{A}$对$=_\beta$封闭，则$\mathcal{A}$不可判定</li><li>$=_\beta$关系不可判定</li><li>集合$\mathcal{N}=\{M:M有\beta-nf\}$不可判定</li></ul></li></ul><h2 id="第五章-Turing机"><a href="#第五章-Turing机" class="headerlink" title="第五章 Turing机"></a>第五章 Turing机</h2><h3 id="5-1-Turing机的形式描述（P121）"><a href="#5-1-Turing机的形式描述（P121）" class="headerlink" title="5.1 Turing机的形式描述（P121）"></a>5.1 Turing机的形式描述（P121）</h3><ul><li>Turing机定义：$M=(d,p,s)$，论域$Dom(M)$</li><li>Turing可计算的定义：存在机器M计算函数f</li><li>基本机器：<ul><li>零函数：$\boxed{Z}$</li><li>后继函数：$\boxed{S}$</li><li>投影函数：$\boxed{I}$、$\boxed{K}$、$\boxed{L}$</li><li>常数函数：$\boxed{C^k_l}$</li><li>前驱函数：$\boxed{pred}$</li><li>加法函数：$\boxed{add}$</li><li>乘法函数：$\boxed{multi}$ (习题5.3)</li><li>幂函数$2^x$：习题5.4</li><li>平方根函数：习题5.7</li></ul></li></ul><h3 id="5-2-Turing机的计算能力（P127）"><a href="#5-2-Turing机的计算能力（P127）" class="headerlink" title="5.2 Turing机的计算能力（P127）"></a>5.2 Turing机的计算能力（P127）</h3><ul><li>常用机器（P128）<ul><li>$f(x)=2x$：$\boxed{double}$</li><li>$\boxed{copy_1}$（习题5.2）、$\boxed{copy_2}$、$\boxed{copy_k}$、$\boxed{copy_k}^k$</li><li>$\boxed{compress}$</li><li>$\boxed{erase}$</li><li>$\boxed{shiftr}$、$\boxed{shiftl}$</li></ul></li><li>Turing可计算函数类对于复合算子、原始递归算子、正则$\mu-$算子封闭</li><li>若f是一般/部分递归函数，则f是Turing-可计算的</li></ul><h3 id="5-3-可判定性与停机问题（P138）"><a href="#5-3-可判定性与停机问题（P138）" class="headerlink" title="5.3 可判定性与停机问题（P138）"></a>5.3 可判定性与停机问题（P138）</h3><ul><li>可判定性：设$A \subseteq \mathbb{N}$，$A$是可判定的指$A$的特征函数$\mathcal{X}_A$是Turing-可计算的（可构造出机器）</li><li>Turing机的编码<ul><li>从#M可反向求出M</li></ul></li><li>自停机问题$K=\{\sharp M:M对于输入\overline{\sharp M}停机\}$：不可判定</li><li>停机问题$\hat{K}=\{\sharp M:M对于一切输入皆停机\}$：不可判定</li></ul><h3 id="5-4-通用Turing机（P141）"><a href="#5-4-通用Turing机（P141）" class="headerlink" title="5.4 通用Turing机（P141）"></a>5.4 通用Turing机（P141）</h3><ul><li>带位置编码</li><li>标准输入编码和解码：$\boxed{code}$、$\boxed{decode}$</li><li>计算后继带位置函数STP、TS</li><li>通用图灵机$U$的定义（P146）</li></ul><h3 id="5-5-Church-Turing论题（P147）"><a href="#5-5-Church-Turing论题（P147）" class="headerlink" title="5.5 Church-Turing论题（P147）"></a>5.5 Church-Turing论题（P147）</h3><h2 id="题型"><a href="#题型" class="headerlink" title="题型"></a>题型</h2><ul><li><p>问答题</p><ul><li>什么是配对函数组？什么是配对函数？请构造一例。（P7 定义1.10，如引理1.14构造）</li><li>什么是一般递归函数？（P42 定义1.31）</li><li>什么是部分递归函数？（P43 定义1.33）</li><li><font color="red">什么是$\lambda \beta$系统的CR性质？（P85）</font></li><li>什么是Turing 机？（P122）</li><li>什么是Church-Turing Thesis？你认可它吗？/你拥护吗？（P148）</li><li>为什么算法和Turing 机概念在可以构成“思维机器”的现代观点中占有如此核心的地位？（因为图灵机的概念为现在的思维机器观点提供了抽象模型，是现代计算机的起源。）是否在原则上存在一个算法可达到绝对极限呢？（未知，自由发挥）</li><li>什么是Halting Problem？它可判定吗？（P141）</li><li>什么是Turing 机的通用性(universality)？什么是通⽤Turing 机？(P146)</li></ul></li><li><p>判断函数类（第二大题）</p><ul><li>A：$\mathcal{EF}$<ul><li>Godel的$\beta-$函数</li><li>向下取整</li><li>$\pi,e$的十进制展开中的第n个数字</li><li>$\lambda-$项呈形…</li><li>组合数个数</li><li>数列求和</li></ul></li><li>B：$\mathcal{PRF} - \mathcal{EF}$<ul><li>形如$G(x)=2^{2^{\cdots^x}}$</li><li>Ack(5,n)</li><li>变参递归(不确定？)</li></ul></li><li>C：$\mathcal{GRF} - \mathcal{PRF}$<ul><li>Ackermann函数</li><li>Ack(m,5)</li><li>$\beta_0,\beta_{x+1}$ （不确定？）</li></ul></li><li>D：$\mathcal{RF} - \mathcal{GRF}$<ul><li>处处无定义的函数</li><li>存在无定义的函数</li></ul></li><li><font color="red">E：不可计算的数论函数类 </font><ul><li>停机问题</li><li>$M有\beta-nf$、$M =_\beta N$</li></ul></li></ul></li><li><p>证明集合$S$的可判定性</p><ul><li>证$\mathcal{X}_s \in \mathcal{GRF}$</li></ul></li><li><p>证明初等函数</p><ul><li><p>根据定义用常用函数表示（Q1.2，Q1.3，1.11，1.12）</p></li><li><p>出现根号、负数等：用已知形式表示（转化为整数、分开表示等）</p></li><li><p>取整</p><ul><li>$f(n)=\lfloor e \cdot n \rfloor$ (Q1.8)</li><li>$f(n)=\lfloor (\frac{\sqrt{5}+1}{2})^n\rfloor$ (2019 四，1.19(2) 两种方法）</li><li>$f(n)=\lfloor (\sqrt{6} +\sqrt{5})^{2n} \rfloor$ (2019+ 四)</li><li>$\lfloor n! \cdot cos(1)\rfloor$</li><li>$\lfloor n! \cdot 2^n \cdot \sqrt{e} \rfloor$</li><li>$\lfloor log_{10}n \rfloor$</li><li>$\lfloor (n+1+\frac{1}{n+1})^{n+1} \rfloor$</li></ul></li><li><font color="red">十进制展开式第n位</font><blockquote><p>step1. 泰勒展开，分开整数项和小数项，证$\lfloor n!·α\rfloor \in \mathcal{EF}$</p><p>step2. 证$\lfloor n \cdot \alpha \rfloor \in \mathcal{EF}$</p><p>step3. $f(n) = \lfloor 10^n \alpha \rfloor - \lfloor 10^{n-1}\alpha \rfloor \cdot 10  \in \mathcal{EF},(n \geq 1)$</p></blockquote><ul><li>$e$ （2019+ 七，5.18，类似Q1.8）</li><li>$\pi$（1.25，一般递归函数）</li><li>$\frac{e^2+1}{2e}$ (2019 七)</li><li>$sinh(1)=(e-e^{-1})/2$ (2018 七)，$sin(1),cos(1)$</li><li>$\sqrt{e}$</li><li>证明原始递归函数</li></ul></li><li><p>串值递归（Q1.5，Q1.6，1.15）</p></li><li>变参递归（Q1.7）</li></ul></li><li><p>对给定的数论函数$f(x)$，构造$F∈Λ^∘$其$λ−$定义$f(x)$</p><ul><li>判断奇偶函数：$l(x)=N^x(0)$ （2017 三）</li><li>add（2016 三）、$f(x,y)=x+y$ （3.16）</li><li>$f(x)=2x$ ，$f(x)=3x$（Q2.3，3.17）</li><li>$g(x)=2^x$ （Q2.7，3.20）<ul><li>倒推法、Rosser引理$\ulcorner n^m \urcorner = \ulcorner m \urcorner \ulcorner n \urcorner$</li></ul></li><li>$f(x)=\lfloor \frac{x}{2} \rfloor$</li></ul></li><li><p>在已有的公理系统中加入一个额外公理</p><ul><li>$\lambda xy.xy = \lambda xy.yx$ (2017)</li><li>$\lambda x.x = \lambda x.xxx$ (2018)</li><li>$\lambda x.x = \lambda x.xx$ （2016）</li><li>$\lambda xy.x = \lambda xy.y$ （3.13）</li><li>$\lambda xyz.x(yz)=\lambda xyz.(xy)z$</li><li>$I=K$ （2019）</li><li>$I=S$（2019+）</li></ul></li><li><p>构造图灵机</p><ul><li>$f(x)=x^3$ （2019 六）、$f(x)=x^4$（2019+ 六）</li><li>$g(x)=2^x$ （2016 六、习题5.4、Q3.5）</li><li>$f(x)=\lfloor \frac{x}{2} \rfloor$</li><li>满足给定的输入输出（2018、2017 五）</li></ul></li><li><p>求图灵机运行后的输出</p><ul><li>记得写状态、箭头</li></ul></li><li><p>证明停机问题不可判定（2018、 2017 六）</p></li></ul><h2 id="Hint"><a href="#Hint" class="headerlink" title="Hint"></a>Hint</h2><ul><li>减号写点、除号写取整</li><li>$N$, $N^2$ 表示if-else</li><li><p>Godel编码</p><ul><li>P,ep</li></ul></li><li>标准组合子$I,K,K^*,S,U^3_1,…$</li><li>$y^nz=(\lambda fx.f^nx)yz = \ulcorner n \urcorner yz$</li></ul><p><img src="/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/1.png" alt="1" style="zoom:60%;"></p><p><img src="/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/2.png" alt="2" style="zoom:70%;"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/">复习笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B/">计算模型</category>
      
      
      <comments>https://rubychen0611.github.io/2020/09/06/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
