<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>与我常在</title>
    <link>https://rubychen0611.github.io/</link>
    
    <atom:link href="https://rubychen0611.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>自爱兼爱，善感而不多愁。</description>
    <pubDate>Thu, 11 Mar 2021 01:35:36 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Keras CNN模型转换为PyTorch模型</title>
      <link>https://rubychen0611.github.io/2021/03/10/Keras-CNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BApytorch%E6%A8%A1%E5%9E%8B/</link>
      <guid>https://rubychen0611.github.io/2021/03/10/Keras-CNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BApytorch%E6%A8%A1%E5%9E%8B/</guid>
      <pubDate>Wed, 10 Mar 2021 02:59:44 GMT</pubDate>
      
      <description>&lt;p&gt;最近想用两种方法跑同一个CNN模型，但一个用Keras写的，一个用PyTorch写的，需要把原先的Keras模型转换为等价的PyTorch模型。目前没有找到合适的自动转换工具，因此只能手动在PyTorch中创建一个一模一样的模型，然后再将权重复制进去：&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>最近想用两种方法跑同一个CNN模型，但一个用Keras写的，一个用PyTorch写的，需要把原先的Keras模型转换为等价的PyTorch模型。目前没有找到合适的自动转换工具，因此只能手动在PyTorch中创建一个一模一样的模型，然后再将权重复制进去：</p><a id="more"></a><h2 id="模型结构复制"><a href="#模型结构复制" class="headerlink" title="模型结构复制"></a>模型结构复制</h2><p>以Keras里的LeNet4模型为例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># block1</span></span><br><span class="line">x = Convolution2D(<span class="number">6</span>, kernel_size, activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block1_conv1'</span>)(input_tensor)</span><br><span class="line">x = MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block1_pool1'</span>)(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># block2</span></span><br><span class="line">x = Convolution2D(<span class="number">16</span>, kernel_size, activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block2_conv1'</span>)(x)</span><br><span class="line">x = MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block2_pool1'</span>)(x)</span><br><span class="line"></span><br><span class="line">x = Flatten(name=<span class="string">'flatten'</span>)(x)</span><br><span class="line">x = Dense(<span class="number">84</span>, activation=<span class="string">'relu'</span>, name=<span class="string">'fc1'</span>)(x)</span><br><span class="line">x = Dense(nb_classes, name=<span class="string">'before_softmax'</span>)(x)</span><br><span class="line">x = Activation(<span class="string">'softmax'</span>, name=<span class="string">'predictions'</span>)(x)</span><br></pre></td></tr></tbody></table></figure><p>model.summary()查看参数情况：</p><p><img src="/2021/03/10/Keras-CNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BApytorch%E6%A8%A1%E5%9E%8B/keras.png" alt="keras" style="zoom:67%;"></p><p>在PyTorch中复现相同的结构（忽略最后的softmax层）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">self.conv2 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.fc1 = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">84</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">)</span><br><span class="line">self.before_softmax = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br></pre></td></tr></tbody></table></figure><p><img src="/2021/03/10/Keras-CNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BApytorch%E6%A8%A1%E5%9E%8B/pytorch.png" alt="pytorch" style="zoom:67%;"></p><h2 id="权重参数转换"><a href="#权重参数转换" class="headerlink" title="权重参数转换"></a>权重参数转换</h2><p>用<code>model.get_weights</code>可以获得Keras模型的权重<code>weights</code>，它是一个list，list中每一项是一个numpy矩阵，依次表示每层的weight、bias。由于各层的bias只有一维，因此两个模型对应位置的bias可以直接互相赋值,如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.conv1[<span class="number">0</span>].bias.data = from_numpy(weights[<span class="number">1</span>])</span><br></pre></td></tr></tbody></table></figure><p>但weight是多维矩阵，由于keras和PyTorch的通道维度存储方式不同，需要进行转换，方法如下：</p><h3 id="卷积层权重"><a href="#卷积层权重" class="headerlink" title="卷积层权重"></a>卷积层权重</h3><p>在Keras的卷积层中kernel weights存储格式是[kernel_height, kernel_width, kernel_channel, kernel_number]，在Pytorch的卷积层中，kernel weights存储格式是[kernel_number, kernel_channel, kernel_height, kernel_width]，因此将Keras模型权重复制到Pytorch模型之前需要进行转换，使用<code>numpy.transpose</code>方法可以方便地对矩阵维度进行位置调换：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.conv1[<span class="number">0</span>].weight.data = from_numpy(np.transpose(weights[<span class="number">0</span>], (<span class="number">3</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br></pre></td></tr></tbody></table></figure><h3 id="flatten层-第一个全连接层"><a href="#flatten层-第一个全连接层" class="headerlink" title="flatten层/第一个全连接层"></a>flatten层/第一个全连接层</h3><p>在将卷积层的输出拉平输入到全连接层时，注意由于Keras和PyTorch通道顺序不同，需要对第一个全连接层的权重进行特殊处理，比如上面的例子中Keras模型第二个池化层的输出形状为（7,7,16），而PyTorch模型的输出形状为（16,7,7），如果不经处理flatten后神经元的顺序将不一样，因此需要先将第一个全连接层的第一个维度复原为Keras中的形状，然后进行转置操作，最后再重新flatten并赋值给PyTorch模型的权重：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w4 = weights[<span class="number">4</span>].reshape((<span class="number">7</span>,<span class="number">7</span>,<span class="number">16</span>,<span class="number">84</span>))    <span class="comment"># 复原形状(7,7,16)</span></span><br><span class="line">w4 = np.transpose(w4, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>)).reshape(<span class="number">784</span>, <span class="number">84</span>)    <span class="comment"># 转置为(16,7,7)</span></span><br><span class="line">self.fc1[<span class="number">0</span>].weight.data = from_numpy(np.transpose(w4))</span><br></pre></td></tr></tbody></table></figure><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>全连接层权重矩阵的维度为2，直接将矩阵转置即可：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.before_softmax.weight.data = from_numpy(np.transpose(weights[<span class="number">6</span>]))</span><br></pre></td></tr></tbody></table></figure><p>完整权重转换代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">self.conv1[<span class="number">0</span>].weight.data = from_numpy(np.transpose(weights[<span class="number">0</span>], (<span class="number">3</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">self.conv1[<span class="number">0</span>].bias.data = from_numpy(weights[<span class="number">1</span>])</span><br><span class="line">self.conv2[<span class="number">0</span>].weight.data = from_numpy(np.transpose(weights[<span class="number">2</span>], (<span class="number">3</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">self.conv2[<span class="number">0</span>].bias.data = from_numpy(weights[<span class="number">3</span>])</span><br><span class="line">w4 = weights[<span class="number">4</span>].reshape((<span class="number">7</span>,<span class="number">7</span>,<span class="number">16</span>,<span class="number">84</span>))</span><br><span class="line">w4 = np.transpose(w4, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>)).reshape(<span class="number">784</span>, <span class="number">84</span>)</span><br><span class="line">self.fc1[<span class="number">0</span>].weight.data = from_numpy(np.transpose(w4))</span><br><span class="line">self.fc1[<span class="number">0</span>].bias.data = from_numpy(weights[<span class="number">5</span>])</span><br><span class="line">self.before_softmax.weight.data = from_numpy(np.transpose(weights[<span class="number">6</span>]))</span><br><span class="line">self.before_softmax.bias.data = from_numpy(weights[<span class="number">7</span>])</span><br></pre></td></tr></tbody></table></figure><p>参考：</p><p>[1] HOW TO TRANSFER A SIMPLE KERAS MODEL TO PYTORCH – THE HARD WAY  <a href="https://gereshes.com/2019/06/24/how-to-transfer-a-simple-keras-model-to-pytorch-the-hard-way/">https://gereshes.com/2019/06/24/how-to-transfer-a-simple-keras-model-to-pytorch-the-hard-way/</a></p><p>[2] Pytorch与Tensorflow权重互转 <a href="https://blog.csdn.net/qq_37541097/article/details/113862998">https://blog.csdn.net/qq_37541097/article/details/113862998</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/">技术笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      <category domain="https://rubychen0611.github.io/tags/Keras/">Keras</category>
      
      <category domain="https://rubychen0611.github.io/tags/PyTorch/">PyTorch</category>
      
      
      <comments>https://rubychen0611.github.io/2021/03/10/Keras-CNN%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%BApytorch%E6%A8%A1%E5%9E%8B/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>2021 Google SWE实习面经</title>
      <link>https://rubychen0611.github.io/2021/03/04/2021-Google-SWE%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F/</link>
      <guid>https://rubychen0611.github.io/2021/03/04/2021-Google-SWE%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F/</guid>
      <pubDate>Thu, 04 Mar 2021 07:16:46 GMT</pubDate>
      
      <description>&lt;p&gt;撒花！在本学期刚开始的时候提前结束找实习之旅😆😆&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/03/04/2021-Google-SWE%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F/1.jpg&quot; alt=&quot;1&quot; style=&quot;zoom:40%;&quot;&gt;&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>撒花！在本学期刚开始的时候提前结束找实习之旅😆😆</p><p><img src="/2021/03/04/2021-Google-SWE%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F/1.jpg" alt="1" style="zoom:40%;"></p><a id="more"></a><p>记录下时间线：</p><p>2020.12.26 投递简历</p><p>2021.1.12 简历筛选状态</p><p>2021.2.8 约面试时间</p><p>2021.2.19 上午连着2轮技术面</p><p>2021.2.22 进入招聘委员会审核（Hiring Committee Review）阶段</p><p>2021.3.3 进入项目匹配（Host Match）并成功匹配到仅剩两个的名额之一</p><p>首先关于投递时间上，其实我投简历和面试都太晚了（先到先得，一定要尽早投！！），过年前后其实名额已经剩得很少了，到我面试的时候已经是最后一批。但也是迫不得已，因为毫无竞赛刷题经验，算法数据结构自从大二上课之后就没复习过了，所以只好晚投一些给自己争取一些准备和复习时间（过年期间都没怎么好好玩T T）。</p><p>接下来讲下面试，因为是人生中第一次找工作面试，面的还是Google，所以非常非常紧张，以至于面试前几天都没怎么睡好。。面试是用Bluejeans进行视频的，一上来就开始做题（甚至不需要自我介绍= =），面试题目不知道可不可以讲就不讲了，难度其实不大，但是在第一时间看到题目的时候还是有点慌乱的，没有立马想出最优解，但在和面试官讨论思路以及面试官一点点引导下最后想出了最优解，然后开始在 <a href="https://codebunk.com/">CodeBunk</a>上进行白版编程，注意可以慢点写，一定要边写边和面试官交流，这样自己的思路也清晰点。</p><p>最后是一串感谢：</p><p>感谢一直鼓励我和帮我模拟面试的npy（一定要夸一下，特地看了俩小时视频，一秒进入状态，非常有Google面试官的范儿！）</p><p>感谢爸爸妈妈在我紧张的时候安慰我，还给我讲他们第一次求职时的故事。</p><p>感谢一直和我对接的HR Queenie小姐姐~</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E6%B1%82%E8%81%8C/">求职</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/Google/">Google</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E9%9D%A2%E7%BB%8F/">面经</category>
      
      
      <comments>https://rubychen0611.github.io/2021/03/04/2021-Google-SWE%E5%AE%9E%E4%B9%A0%E9%9D%A2%E7%BB%8F/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【算法复习】图算法</title>
      <link>https://rubychen0611.github.io/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/</link>
      <guid>https://rubychen0611.github.io/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/</guid>
      <pubDate>Mon, 01 Feb 2021 09:10:49 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;基本的图算法&quot;&gt;&lt;a href=&quot;#基本的图算法&quot; class=&quot;headerlink&quot; title=&quot;基本的图算法&quot;&gt;&lt;/a&gt;基本的图算法&lt;/h2&gt;&lt;h3 id=&quot;图的表示&quot;&gt;&lt;a href=&quot;#图的表示&quot; class=&quot;headerlink&quot; title=&quot;图的表示&quot;&gt;&lt;/a&gt;图的表示&lt;/h3&gt;&lt;p&gt;图$G=(V,E)$&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/图表示1.png&quot; alt=&quot;图表示1&quot; style=&quot;zoom:80%;&quot;&gt;&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="基本的图算法"><a href="#基本的图算法" class="headerlink" title="基本的图算法"></a>基本的图算法</h2><h3 id="图的表示"><a href="#图的表示" class="headerlink" title="图的表示"></a>图的表示</h3><p>图$G=(V,E)$</p><p><img src="/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/图表示1.png" alt="图表示1" style="zoom:80%;"> <a id="more"></a></p><h4 id="1、邻接链表"><a href="#1、邻接链表" class="headerlink" title="1、邻接链表"></a>1、邻接链表</h4><ul><li>适合稀疏图</li><li>存储需求：$\Theta(V+E)$</li></ul><h4 id="2、邻接矩阵"><a href="#2、邻接矩阵" class="headerlink" title="2、邻接矩阵"></a>2、邻接矩阵</h4><ul><li>适合稠密图，或需要快速判断两个节点中是否有边时，或图规模较小时</li><li><p>存储需求：$\Theta(V^2)$</p></li><li><p>无向图的邻接矩阵是一个对称矩阵</p></li></ul><h3 id="广度优先搜索"><a href="#广度优先搜索" class="headerlink" title="广度优先搜索"></a>广度优先搜索</h3><ul><li>结点颜色<ul><li>白色：未遍历</li><li>灰色：队列中</li><li>黑色：已遍历完</li></ul></li><li>运行时间：$O(V+E)$</li><li><strong>性质</strong>：<ul><li><strong>BFS计算出的v.d为s到v的最短路径距离</strong></li><li>广度优先树：BFS搜索过程中生成的树<ul><li>根据v.π（前驱）可以打印s到v的最短路径</li></ul></li></ul></li></ul><p><img src="/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/BFS1.png" alt="BFS1" style="zoom:80%;"></p><h3 id="深度优先搜索"><a href="#深度优先搜索" class="headerlink" title="深度优先搜索"></a>深度优先搜索</h3><ul><li>记录时间戳<ul><li>u.d：发现时间</li><li>u.f：结束时间</li></ul></li><li>运行时间：$\Theta(V+E)$</li></ul><p><img src="/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/DFS1.png" alt="DFS1" style="zoom:80%;"></p><ul><li><p><strong>性质</strong></p><ul><li><p>括号化结构</p><ul><li>对于两个结点u、v：<ul><li>区间包含关系：在深度优先森林中具有祖先与后代关系</li><li>区间不相交：没有后代关系</li></ul></li></ul></li><li><p>有向图边的分类（图c）</p><ul><li>树边：深度优先森林中的边</li><li>后向边（B）：子孙指向祖先的边</li><li>前向边（F）：祖先指向子孙的边</li><li>横向边（C）：其他边</li></ul></li><li><p><strong>无向图中只有树边和后向边</strong>，没有前向边和横向边</p><p><img src="/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/DFS2.png" alt="DFS2" style="zoom:80%;"></p></li></ul></li></ul><h3 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h3><ul><li>拓扑排序：G中所有节点的一种线性次序，满足如果图G包含边(u,v)，则节点u在拓扑排序中处于节点v的前面（如果图G包含环路，则不可能排出一种线性次序）</li><li>拓扑排序算法：与DFS完成时间顺序相反</li></ul><p><img src="/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/拓扑排序1.png" alt="拓扑排序1" style="zoom:80%;"></p><p>在图论中，<strong>拓扑排序（Topological Sorting）</strong>是一个<strong>有向无环图（DAG, Directed Acyclic Graph）</strong>的所有顶点的线性序列。且该序列必须满足下面两个条件：</p><ol><li>每个顶点出现且只出现一次。</li><li>若存在一条从顶点 A 到顶点 B 的路径，那么在序列中顶点 A 出现在顶点 B 的前面。</li></ol><p>它是一个 DAG 图，那么如何写出它的拓扑排序呢？这里说一种比较常用的方法：</p><ol><li>从 DAG 图中选择一个 没有前驱（即入度为0）的顶点并输出。</li><li>从图中删除该顶点和所有以它为起点的有向边。</li><li>重复 1 和 2 直到当前的 DAG 图为空或<strong>当前图中不存在无前驱的顶点为止</strong>。后一种情况说明有向图中必然存在环。</li></ol><p><img src="/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/拓扑排序2.png" alt="拓扑排序2"></p><p><strong>实现：</strong>（LeetCode 210 课程表II)</p><p>先建立一个邻接矩阵表示图，方便进行直接查找。拓扑排序也可以被看成是广度优先搜索的一种情况：我们先遍历一遍所有节点，把入度为0的节点（即没有前置课程要求）放在<strong>队列</strong>中。在每次从队列中获得节点时，我们将该节点放在目前排序的末尾，并且把它指向的课程的入度各减1；如果在这个过程中有课程的所有前置必修课都已修完（即入度为0），我们把这个节点加入队列中。当队列的节点都被处理完时，说明所有的节点都已排好序，或因图中存在循环而无法上完所有课程。</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LeetCode 210. 课程表II</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">findOrder</span><span class="params">(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; prerequisites)</span> </span>{</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">graph</span><span class="params">(numCourses, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;())</span></span>;   <span class="comment">// 图</span></span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">indegree</span><span class="params">(numCourses, <span class="number">0</span>)</span></span>; <span class="comment">// 入度</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="built_in">pair</span>: prerequisites)</span><br><span class="line">        {</span><br><span class="line">            graph[<span class="built_in">pair</span>[<span class="number">1</span>]].push_back(<span class="built_in">pair</span>[<span class="number">0</span>]);</span><br><span class="line">            indegree[<span class="built_in">pair</span>[<span class="number">0</span>]] ++;</span><br><span class="line">        }</span><br><span class="line">        <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; Q; <span class="comment">// 队列保存入度为0的结点</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numCourses; i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(indegree[i] == <span class="number">0</span>)</span><br><span class="line">                Q.push(i);</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">while</span>(!Q.empty())</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> p = Q.front();</span><br><span class="line">            Q.pop();</span><br><span class="line">            ans.push_back(p);</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> neighbor: graph[p])</span><br><span class="line">            {</span><br><span class="line">                indegree[neighbor] --;</span><br><span class="line">                <span class="keyword">if</span>(indegree[neighbor] == <span class="number">0</span>)</span><br><span class="line">                    Q.push(neighbor);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span>(ans.<span class="built_in">size</span>() != numCourses)    <span class="comment">// 有环，没有拓扑序</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="强连通分量"><a href="#强连通分量" class="headerlink" title="强连通分量"></a>强连通分量</h3><ul><li><p>强连通分量：有向图中的一个最大节点集合，该集合中的任意一对结点u和v可以互相到达</p></li><li><p>算法</p><ol><li><p>深度优先遍历G，算出每个结点u的结束时间f[u]，起点如何选择无所谓。</p></li><li><p>深度优先遍历G的转置图G T ,选择遍历的起点时,按照结点的结束时间从大到小进行。遍历的过程中,一边遍历,一边给结点做分类标记,每找到一个新的起点,分类标记值就加1。</p></li><li><p>第2步中产生的标记值相同的结点构成深度优先森林中的一棵树,也即一个强连通分量</p></li></ol><p><img src="/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/强连通分量.png" alt="强连通分量" style="zoom:80%;"></p></li></ul><h2 id="最小生成树"><a href="#最小生成树" class="headerlink" title="最小生成树"></a>最小生成树</h2><p>最小生成树：无向图中的一个边的子集，既能够将所有的结点连接起来，又具有最小的权重</p><h3 id="Kruskal算法"><a href="#Kruskal算法" class="headerlink" title="Kruskal算法"></a>Kruskal算法</h3><p>在所有连接森林中两棵不同树的边里，找到权重最小的边（u，v）。使用并查集数据结构。</p><ul><li>运行时间：$O(ElgV)$</li></ul><p><img src="/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/最小生成树1.png" alt="最小生成树1" style="zoom: 67%;"></p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//leetcode 1135 最低成本连通所有城市</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Edge</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">int</span> u, v;</span><br><span class="line">    <span class="keyword">int</span> weight;</span><br><span class="line">    Edge(<span class="keyword">int</span> s, <span class="keyword">int</span> e, <span class="keyword">int</span> w):u(s), v(e), weight(w){}</span><br><span class="line">    <span class="keyword">bool</span> <span class="keyword">operator</span> &gt; (<span class="keyword">const</span> Edge&amp; e1) <span class="keyword">const</span></span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">return</span> weight &gt; e1.weight;</span><br><span class="line">    }</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Kruskal</span><span class="params">(<span class="keyword">int</span> N, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; connections)</span>    <span class="comment">// Kruskal算法</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">priority_queue</span>&lt;Edge,<span class="built_in">vector</span>&lt;Edge&gt;, greater&lt;Edge&gt;&gt; min_heap;</span><br><span class="line">    <span class="keyword">for</span>(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; e: connections)</span><br><span class="line">    {</span><br><span class="line">        min_heap.push(Edge(e[<span class="number">0</span>] - <span class="number">1</span>, e[<span class="number">1</span>] - <span class="number">1</span>, e[<span class="number">2</span>]));</span><br><span class="line">    }</span><br><span class="line">    <span class="function">UFSet <span class="title">ufset</span><span class="params">(N)</span></span>;    <span class="comment">// 并查集</span></span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(count &lt; N &amp;&amp; !min_heap.empty())   <span class="comment">// 取 N-1条边</span></span><br><span class="line">    {</span><br><span class="line">        Edge e = min_heap.top();</span><br><span class="line">        min_heap.pop();</span><br><span class="line">        <span class="keyword">int</span> r1 = ufset.<span class="built_in">find</span>(e.u), r2 = ufset.<span class="built_in">find</span>(e.v);</span><br><span class="line">        <span class="keyword">if</span>(r1 != r2)</span><br><span class="line">        {</span><br><span class="line">            ufset.union_root(r1, r2);</span><br><span class="line">            ans += e.weight;</span><br><span class="line">            count ++;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span>(count != N<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="Prim算法"><a href="#Prim算法" class="headerlink" title="Prim算法"></a>Prim算法</h3><p>每一步在连接集合A和A之外的结点的所有边中，选择一条轻量级边加入到A中。使用最小堆数据结构。</p><ul><li>运行时间：$O(ElgV)$</li></ul><p><img src="/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/最小生成树2.png" alt="最小生成树2" style="zoom: 67%;"></p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//leetcode 1135 最低成本连通所有城市</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">EdgeNode</span> // 用于<span class="title">Prim</span>算法的图</span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">int</span> v;    <span class="comment">//另一端点</span></span><br><span class="line">    <span class="keyword">int</span> weight;</span><br><span class="line">    EdgeNode(<span class="keyword">int</span> n, <span class="keyword">int</span> w):v(n), weight(w){}</span><br><span class="line">    <span class="keyword">bool</span> <span class="keyword">operator</span> &gt; (<span class="keyword">const</span> EdgeNode&amp; e1) <span class="keyword">const</span></span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">return</span> weight &gt; e1.weight;</span><br><span class="line">    }</span><br><span class="line">};</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Prim</span><span class="params">(<span class="keyword">int</span> N, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; connections)</span>    <span class="comment">// prim算法</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;EdgeNode&gt;&gt; <span class="title">graph</span><span class="params">(N, <span class="built_in">vector</span>&lt;EdgeNode&gt;())</span></span>;</span><br><span class="line">    <span class="comment">// 建图</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; e: connections)</span><br><span class="line">    {</span><br><span class="line">        graph[e[<span class="number">0</span>] - <span class="number">1</span>].push_back(EdgeNode(e[<span class="number">1</span>] - <span class="number">1</span>, e[<span class="number">2</span>]));</span><br><span class="line">        graph[e[<span class="number">1</span>] - <span class="number">1</span>].push_back(EdgeNode(e[<span class="number">0</span>] - <span class="number">1</span>, e[<span class="number">2</span>]));</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; <span class="title">used</span><span class="params">(N, <span class="literal">false</span>)</span></span>; <span class="comment">// 顶点集合</span></span><br><span class="line">    <span class="built_in">priority_queue</span>&lt;EdgeNode,<span class="built_in">vector</span>&lt;EdgeNode&gt;, greater&lt;EdgeNode&gt;&gt; min_heap;</span><br><span class="line">    <span class="keyword">int</span> p = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// 新顶点：邻边（另一顶点不在集合中的）加入最小堆</span></span><br><span class="line">        used[p] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span>(EdgeNode neighbor: graph[p])</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(used[neighbor.v] == <span class="literal">false</span>)</span><br><span class="line">            {</span><br><span class="line">                min_heap.push(neighbor);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 选最小堆中最小的边</span></span><br><span class="line">        <span class="keyword">while</span>(!min_heap.empty() &amp;&amp; count &lt; N)</span><br><span class="line">        {</span><br><span class="line">            EdgeNode e = min_heap.top();</span><br><span class="line">            min_heap.pop();</span><br><span class="line">            <span class="keyword">if</span>(!used[e.v])    <span class="comment">// 需要再次判断</span></span><br><span class="line">            {</span><br><span class="line">                ans += e.weight;</span><br><span class="line">                used[e.v] = <span class="literal">true</span>;</span><br><span class="line">                p = e.v;</span><br><span class="line">                count ++;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }<span class="keyword">while</span>(count &lt; N);</span><br><span class="line">    <span class="keyword">if</span>(count &lt; N)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h2 id="单源最短路径"><a href="#单源最短路径" class="headerlink" title="单源最短路径"></a>单源最短路径</h2><h3 id="Bellman-Ford算法"><a href="#Bellman-Ford算法" class="headerlink" title="Bellman-Ford算法"></a>Bellman-Ford算法</h3><p>数据结构 P379</p><ul><li><p>允许输入图中包含负权重的边</p><ul><li>不能包含带父权值的环路</li><li>算法可以检测出环路</li></ul></li><li><p>时间复杂度：$O(n^2)$</p></li></ul><h3 id="Dijkstra算法"><a href="#Dijkstra算法" class="headerlink" title="Dijkstra算法"></a>Dijkstra算法</h3><ul><li>假设输入图的所有边权重为<strong>非负值</strong></li><li>时间复杂度：$O(n^2)$</li></ul><p><img src="/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/Dijkstra.png" alt="Dijkstra" style="zoom:50%;"></p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 建图</span></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;GraphNode&gt;&gt; <span class="title">graph</span><span class="params">(n, <span class="built_in">vector</span>&lt;GraphNode&gt;())</span></span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; edge: edges)</span><br><span class="line">{</span><br><span class="line">    graph[edge[<span class="number">0</span>]].push_back(GraphNode(edge[<span class="number">1</span>], edge[<span class="number">2</span>]));</span><br><span class="line">    graph[edge[<span class="number">1</span>]].push_back(GraphNode(edge[<span class="number">0</span>], edge[<span class="number">2</span>]));</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dist</span><span class="params">(n, INF)</span></span>;    <span class="comment">// 初始化最短距离</span></span><br><span class="line">dist[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(GraphNode neighbor: graph[<span class="number">0</span>])</span><br><span class="line">{</span><br><span class="line">    dist[neighbor.v] = neighbor.weight;     <span class="comment">// 初始化</span></span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; <span class="title">used</span><span class="params">(n, <span class="literal">false</span>)</span></span>;</span><br><span class="line">used[<span class="number">0</span>] = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">1</span>;  <span class="comment">// 已找到最短路径的顶点个数</span></span><br><span class="line"><span class="keyword">while</span>(count &lt; n)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">int</span> p = findMin(dist, used);</span><br><span class="line">    used[p] = <span class="literal">true</span>;</span><br><span class="line">    count ++;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++)  <span class="comment">// 更新</span></span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span>(!used[i])</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> w = getWeight(graph, i, p);</span><br><span class="line">            <span class="keyword">if</span>(w != <span class="number">-1</span>) <span class="comment">// i 和p 之间有边</span></span><br><span class="line">            {</span><br><span class="line">                dist[i] = <span class="built_in">min</span>(dist[i], dist[p] + w);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h2 id="所有结点间的最短路径"><a href="#所有结点间的最短路径" class="headerlink" title="所有结点间的最短路径"></a>所有结点间的最短路径</h2><h3 id="Floyd算法"><a href="#Floyd算法" class="headerlink" title="Floyd算法"></a>Floyd算法</h3><p>数据结构P381</p><ul><li>时间复杂度：$O(n^3)$</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E7%AE%97%E6%B3%95/">算法</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/">算法导论</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E5%9B%BE/">图</category>
      
      
      <comments>https://rubychen0611.github.io/2021/02/01/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E5%9B%BE%E7%AE%97%E6%B3%95/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【算法复习】贪心和动态规划</title>
      <link>https://rubychen0611.github.io/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</link>
      <guid>https://rubychen0611.github.io/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</guid>
      <pubDate>Sat, 23 Jan 2021 02:37:54 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;动态规划&quot;&gt;&lt;a href=&quot;#动态规划&quot; class=&quot;headerlink&quot; title=&quot;动态规划&quot;&gt;&lt;/a&gt;动态规划&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;动态规划与分治法的区别&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分治法将问题划分为&lt;u&gt;互不相交&lt;/u&gt;的子问题，递归地求解子问题，再将它们的解组合起来，求出原问题的解。&lt;/li&gt;
&lt;li&gt;动态规划适用于&lt;u&gt;子问题重叠&lt;/u&gt;的情况，即不同的子问题具有公共的子子问题，在这种情况下，分治法会做许多不必要的工作，会反复地求解那些公共子子问题。而动态规划对每个子子问题只求解一次，将其解保存在一个表格中，避免了不必要的计算工作。&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p><strong>动态规划与分治法的区别</strong></p><ul><li>分治法将问题划分为<u>互不相交</u>的子问题，递归地求解子问题，再将它们的解组合起来，求出原问题的解。</li><li>动态规划适用于<u>子问题重叠</u>的情况，即不同的子问题具有公共的子子问题，在这种情况下，分治法会做许多不必要的工作，会反复地求解那些公共子子问题。而动态规划对每个子子问题只求解一次，将其解保存在一个表格中，避免了不必要的计算工作。</li></ul><a id="more"></a><p><strong>动态规划算法</strong></p><ul><li>通常用来求解<strong>最优化问题</strong></li><li>步骤<ol><li>刻画一个最优解的结构特征</li><li>递归地定义最优解的值</li><li>计算最优解的值，通常采用自底向上的方法</li><li>利用计算出的信息构造一个最优解</li></ol></li></ul><h3 id="钢条切割"><a href="#钢条切割" class="headerlink" title="钢条切割"></a>钢条切割</h3><h4 id="钢条切割问题"><a href="#钢条切割问题" class="headerlink" title="钢条切割问题"></a>钢条切割问题</h4><ul><li><p>给定一段长度为$n$英寸和一个价格表$p_i$，求钢条切割方案，使得销售收益$r_n$最大。</p></li><li><p>满足<strong>最优子结构性质</strong></p><ul><li>问题的最优解由相关子问题的最优解组合而成，而这些子问题可以独立求解</li></ul></li><li>思路1：完成首次切割后，将两段钢条看成两个独立的钢条切割问题实例。通过组合两个相关子问题的最优解，并在所有可能的两段切割方案中选择组合收益最大者，构成原问题的最优解。</li><li>思路2：进一步，将钢条从左边切下长度为$i$的一段，只对右边剩下的长度为$n-i$的一段继续进行切割，左边的一段不再切割。则原问题的最优解只包含一个相关子问题，而不是两个。</li></ul><h4 id="自顶向下递归实现"><a href="#自顶向下递归实现" class="headerlink" title="自顶向下递归实现"></a>自顶向下递归实现</h4><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/钢条切割1.png" alt="钢条切割1" style="zoom:75%;"></p><ul><li>考虑了所有$2^{n-1}$种切割方案</li><li>运行时间为$O(2^n)$</li></ul><h4 id="动态规划算法实现"><a href="#动态规划算法实现" class="headerlink" title="动态规划算法实现"></a>动态规划算法实现</h4><h5 id="方法一：带备忘的自顶向下法"><a href="#方法一：带备忘的自顶向下法" class="headerlink" title="方法一：带备忘的自顶向下法"></a>方法一：带备忘的自顶向下法</h5><p>仍按自然递归形式编写过程，但过程会保存每个子问题的解（数组/散列表），当需要一个子问题的解时，先检查过去是否已保存过此解。(是一种深度优先搜索)</p><ul><li>运行时间$\Theta(n^2)$</li></ul><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/钢条切割2.png" alt="钢条切割2" style="zoom:70%;"></p><h5 id="方法二：自底向上法"><a href="#方法二：自底向上法" class="headerlink" title="方法二：自底向上法"></a>方法二：自底向上法</h5><p>将子问题由小到大的顺序进行求解，当求解某个子问题时，它所依赖的那些更小的子问题都已经求解完毕。（逆拓扑序）</p><ul><li>运行时间$\Theta(n^2)$</li></ul><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/钢条切割3.png" alt="钢条切割3" style="zoom:80%;"></p><h5 id="重构解"><a href="#重构解" class="headerlink" title="重构解"></a>重构解</h5><p>增加数组s保存钢条长度为j时第一段钢条的切割长度s[j]：</p><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/钢条切割4.png" alt="钢条切割4" style="zoom:70%;"></p><p>输出解：</p><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/钢条切割5.png" alt="钢条切割5" style="zoom:80%;"></p><h3 id="矩阵链乘法"><a href="#矩阵链乘法" class="headerlink" title="矩阵链乘法"></a>矩阵链乘法</h3><p>给定$n$个矩阵的链$<a_1,a_2,...,a_n>$，矩阵$A_i$的规模为$p_{i-1}\times p_i(1\leq n \leq n)$，求完全括号化方案，使得计算乘积$A_1A_2\cdots A_n$所需的标量乘法次数最少。</a_1,a_2,...,a_n></p><ul><li>令$m[i,j]$表示计算矩阵$A_{i,j}$所需标量乘法次数的最小值，原问题的最优解为$m[1,n]$</li><li>递归求解方案：</li></ul><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/矩阵链乘1.png" alt="矩阵链乘" style="zoom: 67%;"></p><ul><li>自顶向上求解<ul><li>代价$O(n^3)$</li></ul></li></ul><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/矩阵链乘2.png" alt="矩阵链乘2" style="zoom:67%;"></p><ul><li><p>重构最优解</p><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/矩阵链乘3.png" alt="矩阵链乘3" style="zoom:67%;"></p></li></ul><h3 id="动态规划原理"><a href="#动态规划原理" class="headerlink" title="动态规划原理"></a>动态规划原理</h3><p>适合用动态规划求解的最优化问题应该具备两个要素：最优子结构和子问题重叠</p><p><strong>最优子结构</strong></p><ul><li><p>如果一个问题的最优解包含其子问题的最优解，我们就称此问题具有最优子结构性质。</p></li><li><p>子问题必须是互相独立的</p><ul><li>一个子问题的解不影响另一个子问题的解，互不相交</li></ul></li></ul><p><strong>重叠子问题</strong></p><p>递归算法会反复地求解相同的子问题，而不是一直产生新的子问题。</p><h3 id="最长公共子序列"><a href="#最长公共子序列" class="headerlink" title="最长公共子序列"></a>最长公共子序列</h3><p>给定两个序列$X=<x_1,x_2,...,x_n>$和$Y=<y_1,y_2,...,y_m>$，求$X$和$Y$的最长公共子序列。</y_1,y_2,...,y_m></x_1,x_2,...,x_n></p><ul><li><p>最优子结构</p><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/LCS1.png" alt="LCS1" style="zoom:80%;"></p></li><li><p>递归式</p><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/LCS2.png" alt="LCS2" style="zoom:80%;"></p></li><li><p>自底向上求解</p><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/LCS3.png" alt="LCS3" style="zoom:67%;"></p></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LeetCode 1143 最长公共子序列：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">max</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> a &gt; b ? a : b;</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">longestCommonSubsequence</span><span class="params">(<span class="built_in">string</span> text1, <span class="built_in">string</span> text2)</span> </span>{</span><br><span class="line">        <span class="keyword">int</span> m = text1.length(), n = text2.length();</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp</span><span class="params">(m, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(n, <span class="number">0</span>))</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n; j++)</span><br><span class="line">            {</span><br><span class="line">                <span class="keyword">if</span>(text1[i] == text2[j])</span><br><span class="line">                {</span><br><span class="line">                    <span class="keyword">if</span> (i == <span class="number">0</span> || j == <span class="number">0</span>)</span><br><span class="line">                        dp[i][j] = <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                        dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span>;</span><br><span class="line">                }</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (i &gt; <span class="number">0</span> &amp;&amp; j &gt;<span class="number">0</span>)</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i<span class="number">-1</span>][j], dp[i][j<span class="number">-1</span>]);</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(i &gt; <span class="number">0</span>)</span><br><span class="line">                    dp[i][j] =dp[i<span class="number">-1</span>][j];</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (j &gt; <span class="number">0</span>)</span><br><span class="line">                    dp[i][j] =dp[i][j<span class="number">-1</span>];</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> dp[m<span class="number">-1</span>][n<span class="number">-1</span>];</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="最优二叉搜索树"><a href="#最优二叉搜索树" class="headerlink" title="最优二叉搜索树"></a>最优二叉搜索树</h3><p>在给定单词搜索频率的前提下，应该如何组织一棵二叉搜索树，使得所有搜索操作访问的节点数最少？（P226)</p><h2 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h2><h3 id="活动选择问题"><a href="#活动选择问题" class="headerlink" title="活动选择问题"></a>活动选择问题</h3><p>给定$n$个活动的集合$S={a_1,a_2,…,a_n}$，每个活动$a_i$有一个开始时间$s_i$和结束时间$f_i$。选出一个最大兼容活动集，假定活动已经按结束时间的单调递增顺序排序。</p><p><strong>贪心算法</strong>：总是选择剩余活动集合中最早结束的活动。</p><ul><li>迭代写法：(复杂度$\Theta(n)$)</li></ul><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/活动选择1.png" alt="活动选择1" style="zoom:80%;"></p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// leetcode 435: 无重叠区间</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">eraseOverlapIntervals</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; intervals)</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(intervals.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> n = intervals.<span class="built_in">size</span>();</span><br><span class="line">        <span class="comment">// 按结束时间排序</span></span><br><span class="line">        sort(intervals.<span class="built_in">begin</span>(), intervals.<span class="built_in">end</span>(), [](<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;a, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;b)-&gt;<span class="keyword">bool</span>{</span><br><span class="line">            <span class="keyword">return</span> a[<span class="number">1</span>] &lt; b[<span class="number">1</span>];</span><br><span class="line">        });</span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">max</span> = <span class="number">1</span>, last_end = intervals[<span class="number">0</span>][<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(intervals[i][<span class="number">0</span>] &gt;= last_end)</span><br><span class="line">            {</span><br><span class="line">                last_end = intervals[i][<span class="number">1</span>];</span><br><span class="line">                <span class="built_in">max</span> ++;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> n - <span class="built_in">max</span>;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="贪心算法原理"><a href="#贪心算法原理" class="headerlink" title="贪心算法原理"></a>贪心算法原理</h3><p>贪心算法步骤：</p><ol><li>将最优化问题转化为这样的形式 : 对其做出一次选择后 , 只剩下一个子问题需要求解 .</li><li>证明做出贪心选择后 , 原问题总是存在最优解 , 即贪心选择总是安全的 .</li><li>证明做出贪心选择后 , 剩余的子问题满足性质 : 其最优解与贪心选择组合即可得到原问题的最优解 , 这样就得到了最优子结构 .</li></ol><p>对比：</p><ul><li>0-1背包问题：只能用动态规划解决</li><li>分数背包问题：可以贪心</li></ul><h3 id="赫夫曼编码"><a href="#赫夫曼编码" class="headerlink" title="赫夫曼编码"></a>赫夫曼编码</h3><p>自底向上，每次选择频率最低的两个结点作为一个新节点的左右子结点。</p><p><img src="/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/赫夫曼编码.png" alt="赫夫曼编码" style="zoom:70%;"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E7%AE%97%E6%B3%95/">算法</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/">算法导论</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E8%B4%AA%E5%BF%83/">贪心</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/">动态规划</category>
      
      
      <comments>https://rubychen0611.github.io/2021/01/23/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E8%B4%AA%E5%BF%83%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【复习笔记】自然语言处理</title>
      <link>https://rubychen0611.github.io/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</link>
      <guid>https://rubychen0611.github.io/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</guid>
      <pubDate>Tue, 05 Jan 2021 11:21:27 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;1-基于规则的传统自然语言处理方法&quot;&gt;&lt;a href=&quot;#1-基于规则的传统自然语言处理方法&quot; class=&quot;headerlink&quot; title=&quot;1 基于规则的传统自然语言处理方法&quot;&gt;&lt;/a&gt;1 基于规则的传统自然语言处理方法&lt;/h2&gt;&lt;h3 id=&quot;什么是自然语言处理&quot;&gt;&lt;a href=&quot;#什么是自然语言处理&quot; class=&quot;headerlink&quot; title=&quot;什么是自然语言处理&quot;&gt;&lt;/a&gt;什么是自然语言处理&lt;/h3&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="1-基于规则的传统自然语言处理方法"><a href="#1-基于规则的传统自然语言处理方法" class="headerlink" title="1 基于规则的传统自然语言处理方法"></a>1 基于规则的传统自然语言处理方法</h2><h3 id="什么是自然语言处理"><a href="#什么是自然语言处理" class="headerlink" title="什么是自然语言处理"></a>什么是自然语言处理<a id="more"></a></h3><ul><li>语⾔<ul><li>自然语⾔（⼈类语⾔）</li><li>程序语⾔（⼈⼯语⾔）</li><li>是⼀种由三部分组成的符号交流系统：记号，意义和连接两者的符码。</li><li>语⾔视作由组合语法规则制约、旨在传达语义的记号形式系统。</li><li>⼀种口头或符号上的⼈类交流系统。</li></ul></li><li><strong>自然语言处理（Natural Language Processing, NLP）</strong><ul><li><strong>利用计算机为⼯具对自然语言进⾏各种加⼯处理、信息提取及应⽤的技术。</strong></li><li><strong>自然语言理解：强调对语言<u>含义和意图的深层次解释</u></strong></li><li>计算语⾔学：强调可计算的语⾔理论</li></ul></li></ul><h3 id="自然语言处理的实现方法"><a href="#自然语言处理的实现方法" class="headerlink" title="自然语言处理的实现方法"></a>自然语言处理的实现方法</h3><ul><li><strong>基于知识工程的理性方法（ Rationalist approach）</strong><ul><li>以<strong>规则</strong>形式表达语言知识。</li><li>基于规则进行符号推理 ，从而实现语言信息处理。</li><li>强调人对语言知识的理性整理（受 Chomsky 主张的<u>人具有先天语言能力观点</u>的影响 ）。</li></ul></li><li><strong>基于数据的经验方法（ Empiricist approach）</strong><ul><li>以大规模语料库为语言知识基础。</li><li>利用统计学习 和基于神经网络的深度学习方法自动获取隐含在语料库中的知识，学习到的知识体现为一系列模型参<br>数。 （训练基于学习到的参数和相应的模型进行语言信息处理。）</li></ul></li><li><p><strong>混合方法</strong></p><ul><li><font color="red"><strong>理性方法的优、缺点</strong></font><ul><li>相应的语言学理论基础好</li><li>语言知识描述精确</li><li>处理效率高（确定性推理）</li><li>知识获取困难（需要专业人员，高级劳动）</li><li>系统鲁棒性差：不完备的规则系统将导致推理的失败</li><li>知识扩充困难，并且很难保证规则之间的一致性</li></ul></li><li><font color="red"><strong>经验方法的优、缺点</strong></font><ul><li>知识获取容易（低级劳动）</li><li>系统鲁棒性好（概率大的作为结果）</li><li>知识扩充容易、一致性容易维护</li><li>相应的语言学理论基础差（可解释性差）</li><li>缺乏对语言学知识的深入描述和利用，过于机械</li><li>处理效率低（大数据、高维度计算）</li></ul></li></ul></li><li><p><font color="red"><strong>自然语言处理的难点</strong></font></p><ul><li><strong>歧义处理</strong><ul><li>自然语言充满了大量的歧义（为什么？）</li><li><u>有限的词汇和规则表达复杂、多样的对象</u></li></ul></li><li>语言知识的表示、获取和运用</li><li>成语和惯用型的处理</li><li><strong>对语言的<u>灵活性和动态性</u>的处理</strong><ul><li>灵活性：同一个意图的不同表达，甚至包含错误的语法等</li><li>动态性：语言在不断的变化，如：新词等</li></ul></li><li>对常识等与语言无关的知识的利用和处理（上下⽂和世界知识（语⾔⽆关）的利⽤和处理）</li></ul></li></ul><h3 id="基于规则的自然语言处理方法（理性方法，传统方法）"><a href="#基于规则的自然语言处理方法（理性方法，传统方法）" class="headerlink" title="基于规则的自然语言处理方法（理性方法，传统方法）"></a>基于规则的自然语言处理方法（理性方法，传统方法）</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><ul><li>以 规则 形式表达语言知识。</li><li>基于规则进行 符号推理 ，从而实现语言信息处理。</li><li>强调人对语言知识的理性整理（知识工程）。</li><li>受计算语言学理论指导。</li><li>语言处理规则作为数据，它与程序分离，程序体现为规则语言的解释器。</li></ul><h4 id="词法分析"><a href="#词法分析" class="headerlink" title="词法分析"></a>词法分析</h4><ul><li>形态还原（针对英语、德语、法语等）<ul><li>把句子中的词还原成它们的基本词形（原形）。</li></ul></li><li>词性标注<ul><li>为句子中的词标上预定义类别集合中的类。</li><li>方法：词典和规则提供候选词性，消歧规则进行消歧</li></ul></li><li>命名实体识别<ul><li>识别出句子中的人名、地名、机构名等。</li></ul></li><li>分词（针对汉语、日语等）<ul><li>识别出句子中的词。</li><li>一般通过分词词典和分词规则库进行分词。</li><li><font color="red">中文分词</font><ul><li>任务描述：分词是指根据 某个分词规范 ，把一个字串划分成词串。</li><li>难点问题<ul><li>切分歧义<ul><li>分类<ul><li>交集型歧义<ul><li>ABC 切分成 AB/C 或 A/BC</li><li>如： 和平等</li></ul></li><li>组合型歧义<ul><li>AB 切分成 AB 或 A/B</li><li>如： 马上</li></ul></li><li>混合型歧义<ul><li>由交集型歧义和组合型歧义嵌套与交叉而成<ul><li>如：“得到达”（交集型、组合型）</li></ul></li></ul></li></ul></li></ul></li><li>一般通过<u>分词词典</u>和<u>分词规则库</u>进行分词。主要方法有：<ul><li>正向最大匹配 (FMM）或逆向最大匹配 (RMM)</li><li>双向最大匹配（能发现交集型歧义）</li><li>正向最大、逆向最小匹配（发现组合型歧义）</li><li>逐词遍历匹配<ul><li>在全句中取最长的词，去掉之，对剩下字符串重复该过程</li></ul></li><li>设立切分标记<ul><li>收集词首字和词尾字，把句子分成较小单位，再用某些方法切分</li></ul></li><li>全切分<ul><li>获得所有可能的切分，选择最大可能的切分</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><h4 id="句法分析"><a href="#句法分析" class="headerlink" title="句法分析"></a>句法分析</h4><ul><li>确定句子的组成<ul><li>词、短语以及它们之间的关系</li></ul></li><li>句法分析任务的类型<ul><li>组块分析（浅层句法分析、部分句法分析）：<ul><li>识别基本短语（非递归的核心成分）</li></ul></li><li><strong>组成分分析（结构分析，完全句法分析）</strong><ul><li>识别词如何构成短语、短语如何构成句子</li><li>句法分析的目的<ul><li>判断句子的合法性（句子识别）</li><li>确定句子的结构（句子中单词相互关联的方式）</li></ul></li><li>基于上下文无关语法（ CFG ）的句法分析<ul><li>CFG 能描述大部分的自然语言结构</li><li>可以构造高效的基于 CFG 的句法分析器</li></ul></li><li>通常采用树形结构来表示句法分析的结果</li></ul></li><li>依存分析<ul><li>识别词之间的依赖（或支配）关系</li></ul></li></ul></li></ul><h4 id="语义分析"><a href="#语义分析" class="headerlink" title="语义分析"></a>语义分析</h4><ul><li><p>语义分析的目的是给出语言表达的含义或意义 ( 。</p></li><li><p>语义分析包括</p><ul><li>词义分析（词义表示及多义词消歧等）</li><li>句义分析（句义表示及句义计算等）</li><li>篇章语义分析（指代、实体关系等）</li></ul></li><li><p>词汇的语义表示：</p><ul><li><p>义项（义位）</p></li><li><p>语义类</p></li><li><p>义素组合</p></li></ul></li></ul><h3 id="往年考题"><a href="#往年考题" class="headerlink" title="往年考题"></a>往年考题</h3><ul><li>语言、自然语言、自然语言处理</li><li><p>你认为自然语言理解和自然语言处理概念本身的区别？</p></li><li><p>相比于程序设计等人工语言，为什么自然语言的处理和理解更难？结合一些示例说明</p></li><li><p>中文分词的任务描述、难点问题以及解决问题的技术和方法</p></li><li><p>规则、统计、神经网络方法的优缺点（结合文本分类）</p><ul><li><p>统计和机器学习方法的特点是，对特征工程的依赖比较大。这是个双刃剑：当训练数据量比较小的时候，我们可以凭着对业务和数据的理解，为模型添加大量先验知识，让它站在我们的肩膀上；当然，这样做会消耗我们的时间和精力。</p></li><li><p>神经网络的突出优势是：</p><p>(1)结构灵活。可以基于常用结构方便地设计出各式各样的结构，以处理各式各样的任务。</p><p>(2)信息容量大。神经网络是众多机器学习模型中，效果上限最高的之一。神经网络可以拟合任意复杂的函数。</p><p>(3)文本表示自动化。神经网络方法大部分情况下会采用词token的分布式表示作为输入特征——这种特征工程方法的泛化性比较强，不需要太多背景知识就可以得到比较好的特征。“万物皆可嵌入”这句口号还是有点道理的。</p></li></ul></li><li><p>比尔盖茨说语言理解是人工智能皇冠上的明珠。给出对于这句话的理解。</p></li></ul><blockquote><p>在以计算、记忆为基础的「运算智能」之上，是以听觉、视觉、触觉为代表的「感知智能」，反映在人工智能技术上为语音识别和图像识别。再之上则是「认知智能」，包含语言、知识和推理。金字塔的顶端，则是「创造智能」，想象一些不存在的事情包括理论、方法、技术，通过实验加以验证，然后提出新的理论。</p><p>语言智能是人工智能皇冠上的明珠，如果语言智能能够突破，与他同属认知智能的知识和推理就会得到长足的发展，整个人工智能体系就会得到很好的推进，也有更多的场景可以落地。</p></blockquote><h2 id="2-语言模型"><a href="#2-语言模型" class="headerlink" title="2 语言模型"></a>2 语言模型</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><ul><li>定义：语言模型是用来刻画一个句子（词串序列）存在可能性的概率模型</li></ul><h3 id="N-Gram-语言模型"><a href="#N-Gram-语言模型" class="headerlink" title="N-Gram 语言模型"></a>N-Gram 语言模型</h3><h4 id="N-Gram模型"><a href="#N-Gram模型" class="headerlink" title="N-Gram模型"></a>N-Gram模型</h4><ul><li><p>基于历史的模型，有限视野假设(Limited Horizon)：当前词出现的概率只和它前面的k个词相关 </p><script type="math/tex; mode=display">P(W)=P(w_{1}w_{2} \ldots w_{n})= \prod _{i=1..n}P(w_{i}|W_{i-k} \cdots W_{i-1})</script><ul><li>二元语言模型：（k=1）1阶马尔可夫链 $ P(W)=P(w_{1}w_{2} \cdots w_{n})=P(w_{1}) \prod _{i=2..n}P(w_{i}|w_{i-1}) $ </li><li>三元语言模型：（k=2）2阶马尔可夫链 $ P(W)=P(w_{1}w_{2} \ldots w_{n})=P(w_{1})P(w_{2}|w_{1}) \prod _{i=3..n}P(w_{i}|w_{i-2}w_{i-1}) $ </li></ul></li><li><p>N-Gram参数估计：相对频率（最大似然）估计</p><script type="math/tex; mode=display">P(w_i|w_{i-1})=\dfrac{P(w_{i-1}w_i)}{P(w_{i-1})}=\dfrac{Count(w_{i-1}w_i)}{\sum_wCount(w_{i-1}w)}=\dfrac{Count(w_{i-1}w_i)}{Count(w_{i-1})}</script></li><li><p>Zipf Law：如果以词频排序，词频和排位的乘积是一个常数。</p><ul><li>Zipf法则隐含的意义：大部分的词都稀有</li><li>语言中频繁出现的事件是有限的，不可能搜集到足够的数据来得到稀有事件的完整概率分布。</li><li>词（一元）如此，对于二元、三元模型更加严重</li><li>零概率还会向下传播，一个2元或者3元文法的零概率，会导致整个句子的零概率</li></ul></li><li>数据稀疏（零概率）：没有足够的训练数据，对于未观测到的数据，出现零概率现象<ul><li>解决方案<ul><li>构造等价类</li><li>参数平滑</li></ul></li></ul></li></ul><h4 id="参数平滑"><a href="#参数平滑" class="headerlink" title="参数平滑"></a>参数平滑</h4><ul><li>平滑是指给没观察到的N元组合赋予一个概率值，以保证词序列总能通过语言模型得到一个概率值。约束总该率和为1。</li><li><p>思想：稍微减少已观察到的事件概率的大小，同时把少量概率分配到没有看到过的事件上，折扣法，使整个事件空间的概率分布曲线更加平滑。改进模型的整体效果。</p></li><li><p>“劫富济贫”：高概率调低点，小概率或者零概率调高点。</p></li><li><p>1、加法平滑方法：简单，效果不好</p><script type="math/tex; mode=display">P_{add- \delta }(x)= \dfrac{c(x)+ \delta }{ \sum _{x^{ \prime }}(c(x^{ \prime })+ \delta )}</script><p>加一平滑前：</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2-1.png" alt="image-20210107200039583"></p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2-2.png" alt="image-20210107200108334"></p><p>加一平滑后：</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2-3.png" alt="2-3" style="zoom:60%;"></p></li><li><p>2、Laplace平滑</p></li><li><p>3、简单线性插值平滑</p></li></ul><h4 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h4><ul><li>困惑度(Perplexity)：对测试集存在的概率</li></ul><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2-4.png" alt="2-4" style="zoom: 80%;"></p><h4 id="N-gram的缺点"><a href="#N-gram的缺点" class="headerlink" title="N-gram的缺点"></a>N-gram的缺点</h4><p>从语言具有的特性看，显得过于简单和幼稚</p><ul><li>数据稀疏、参数空间过大</li><li>基于词的，无长距离依赖，无结构、语义支持</li><li>泛化能力差</li></ul><h3 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h3><ul><li>对比N-gram和FNNLM的优缺点 </li><li>FNNLM：复杂，自动提取特征、泛化能力强</li></ul><h3 id="往年考题-1"><a href="#往年考题-1" class="headerlink" title="往年考题"></a>往年考题</h3><ul><li>自左向右定义一个n-gram，得到一个三元文法，也可以按照相反的顺序自右向左定义得到一个三元文法，证明二者等价。</li><li></li></ul><h2 id="3-文本分类"><a href="#3-文本分类" class="headerlink" title="3 文本分类"></a>3 文本分类</h2><h3 id="朴素贝叶斯模型"><a href="#朴素贝叶斯模型" class="headerlink" title="朴素贝叶斯模型"></a>朴素贝叶斯模型</h3><p>基本思想：<strong>利用特征项和类别的<u>联合概率</u>来估计给定文档的<u>类别概率</u></strong></p><p>D为待分类的文档，$c_k$指第k个类别</p><script type="math/tex; mode=display">\begin{align}argmax_{c_k}P(c_k|D)&=arg \max _{c_{k}}\dfrac{P(D|c_k)P(c_k)}{P(D)}\\&=arg \max _{c_{k}}P(D|c_k)P(c_k) \\\end{align}</script><h4 id="伯努利文档模型"><a href="#伯努利文档模型" class="headerlink" title="伯努利文档模型"></a>伯努利文档模型</h4><ul><li>一个文档被表示成01向量，向量中每一个元素表示相应的单词是否在文档中出现了</li><li>令$D_i$表示第i个文档的01向量；$D_{it}$表示第i个文档的01向量中第t个元素的值（是一个随机变量），即单词$w_t$是否在文档i中出现了</li><li>$P(w_t|c_k)$表示单词$w_t$在$c_k$类文档中出现的概率，估计：$P(w_t|c_k)=\dfrac{c_k中w_t出现的文档个数}{c_k中所有文档的个数}$ </li><li>估计：$ P(c_{k})= \dfrac{c_k类文档数}{所有文档数} $ </li><li>$c_k$类文档中特征$it$出现的条件估计：$P(D_{it}|c_k)=D_{it}P(w_t|c_k)+(1-D_{it})(1-P(w_t|c_k))$ <ul><li>$D_{it}=0$时，$P(D_{it}|c_k)=1-P(w_t|c_k)$</li><li>$D_{it}=1$时，$P(D_{it}|c_k)=P(w_t|c_k)$</li></ul></li><li>$P(D_i|C_k)=\prod_{t=1}^{|V|}P(D_{it}|c_k)$</li></ul><script type="math/tex; mode=display">\begin{align}argmax_{c_k}P(c_k|D)&=arg \max _{c_{k}}\dfrac{P(D|c_k)P(c_k)}{P(D)}\\&=arg \max _{c_{k}}P(D|c_k)P(c_k) \\&=arg \max _{c_{k}}P(c_{k}) \prod _{i=1}^{W_{t}} \left[ D_{jt}P(w_{t}|c_{k})+(1-D_{jt})(1-P(w_{t}|c_{k})) \right]\end{align}</script><p>例：PPT 13-15 </p><h4 id="多项式文档模型（Multinomial-document-model）"><a href="#多项式文档模型（Multinomial-document-model）" class="headerlink" title="多项式文档模型（Multinomial document model）"></a>多项式文档模型（Multinomial document model）</h4><ul><li><p>一个文档被表示成整数向量，向量中每一个元素表示相应的单词在文档中出现了多少次</p><script type="math/tex; mode=display">P(D_{i}|c_{k})= \frac{n_{i}!}{ \prod _{i=1}^{|V|}D_{it}!} \prod _{i=1}^{|V|}P(w_{t}|c_{k})^{D_{it}}</script></li><li><p>估计单词$w_i$在$c_k$类文档出现的概率：$ \widehat{P}(w_{i}|c_{k})= \dfrac{ \sum _{i=1}^{N}D_{ij}z_{ik}}{ \sum _{s=1}^{|V|} \sum _{i=1}^{N}D_{is}z_{ik}} $</p></li></ul><script type="math/tex; mode=display">\begin{align}argmax_{c_k}P(c_k|D)&=arg \max _{c_{k}}\dfrac{P(D|c_k)P(c_k)}{P(D)}\\&=arg \max _{c_{k}}P(D|c_k)P(c_k) \\&=arg \max _{c_{k}}P(c_{k}) \frac{n_{i}!}{ \prod _{i=1}^{|V|}D_{it}!} \prod _{i=1}^{|V|}P(w_{t}|c_{k})^{D_{it}} \\ &=arg \max _{c_{k}}P(c_{k})\prod _{i=1}^{|V|}P(w_{t}|c_{k})^{D_{it}} \\ &=arg \max _{c_{k}}P(c_{k}) \prod _{h=1}^{len(D_{i})}P(u_{h}|c_{k}) \end{align}</script><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><ul><li><p>训练句向量</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/3-1.png" alt="3-1" style="zoom:60%;"></p></li><li><p>每个特征具有一个权重</p></li><li>定义一个线性函数为句子的类别打分：$ score(y,w)=w^{T}f(y) $ </li></ul><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/3-2.png" alt="3-2" style="zoom:70%;"></p><ul><li><p>预测</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/3-3.png" alt="3-3" style="zoom:60%;"></p></li><li><p>朴素贝叶斯模型其实属于线性模型</p></li><li><p>权重学习</p><ul><li>Perceptron: 0-1 loss</li><li>Logistic Regression ( Maximum Entropy ): log-loss</li><li>SVM: hinge-loss</li></ul></li></ul><h3 id="文本表示"><a href="#文本表示" class="headerlink" title="文本表示"></a>文本表示</h3><h4 id="特征选择方法"><a href="#特征选择方法" class="headerlink" title="特征选择方法"></a>特征选择方法</h4><ul><li><p>停词删除</p></li><li><p>基于文档频率(DF)的特征提取法<br>从训练预料中统计出包含某个特征的文档的频率(个数),然后根据设定的阈值，当该特征项的DF值小于某个阈值时，从特征空间中去掉该特征项，因为该特征项使文档出现的频率太低，没有代表性；当该特征项的DF值大于另外一个阈值时，从特征空间中也去掉该特征项，因为该特征项使文档出现的频率太高，没有区分度</p></li><li><p><strong>互信息</strong>（计算？ PPT 39-41）</p><ul><li>互信息越大，特征$t_i$和类别$C_j$共现的程度越大</li></ul><script type="math/tex; mode=display">I(X;Y)= \sum _{y \in Y} \sum _{Y \in X}p(x,y) \log ( \frac{p(x,y)}{p(x)p(y)})</script></li></ul><ul><li><p>$ \chi^{2}(Chi-Square) $ 检验</p><ul><li>衡量特征$t_i$和类别$C_j$之间的相关联程度</li></ul></li></ul><h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><ul><li>TF(词频)<ul><li>出现次数越多的词越重要</li><li>标准化后：</li></ul></li></ul><script type="math/tex; mode=display">TF=\dfrac{某个词在文章中的出现次数}{文章中出现最多词的个数}</script><ul><li><p>IDF(逆文档频率)</p><ul><li>出现在许多不同文档中的词对主题的指示性较差</li><li>文档频率：包括了该词的文档个数/文档总数</li><li>逆文档频率：文档频率倒数</li></ul><script type="math/tex; mode=display">IDF=\log \dfrac{语料库的文档总数}{包含该词的文档数+1}</script></li><li><p>tf-idf</p><ul><li>衡量词$i$在文档$j$中表示的重要程度/区分能力的强弱</li></ul><script type="math/tex; mode=display">W_{ij}=tf_{ij}\cdot idf_{i}=tf_{ij} \cdot \log _{2}(N/df_{i})</script></li></ul><h3 id="文档分类性能评价"><a href="#文档分类性能评价" class="headerlink" title="文档分类性能评价"></a>文档分类性能评价</h3><ul><li>二分类：准确率、查全率、查准率、F1-score</li><li>微平均和宏平均</li></ul><h2 id="4-情感分类"><a href="#4-情感分类" class="headerlink" title="4 情感分类"></a>4 情感分类</h2><ul><li>情感分析中如何表示词和句子，才有可能得到更好的情感分类？</li></ul><h2 id="5-表示学习"><a href="#5-表示学习" class="headerlink" title="5 表示学习"></a>5 表示学习</h2><p>文本表示的作用就是将这些非结构化的信息转化为结构化的信息。</p><h3 id="符号编码（Symbolic-Encoding）"><a href="#符号编码（Symbolic-Encoding）" class="headerlink" title="符号编码（Symbolic Encoding）"></a>符号编码（Symbolic Encoding）</h3><h4 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h4><ul><li><p>独热编码（One-hot encoding）</p><ul><li>一位有效编码</li><li>假设词之间没有相似性，所有向量正交</li></ul></li><li><p>词袋模型：文档中所有单词独热编码的和</p><ul><li>优点：简单</li><li><font color="red"> <strong>缺点：</strong></font><ul><li><strong>忽略了词的相似性（同义词、多义词）</strong></li><li><strong>维度灾难（可用特征选择方法减轻）</strong></li></ul></li></ul></li><li><p>N-gram的词袋模型</p><ul><li><p>Vocab = set of all n-grams in corpus</p></li><li><p>Document = n-grams in document w.r.t vocab with multiplicity </p></li><li><p>例：</p><ul><li><p>Sentence 1: “The cat sat on the hat” </p><p>Sentence 2: “The dog ate the cat and the hat” </p><p>Vocab = { the cat, cat sat, sat on, on the, the hat, the dog, dog ate, ate the, cat and, and the} </p><p>Sentence 1: { 1, 1, 1, 1, 1, 0, 0, 0, 0, 0} </p><p>Sentence 2 : { 1, 0, 0, 0, 0, 1, 1, 1, 1, 1} </p></li></ul></li><li><p>维度过高</p><ul><li>特征工程、特征过滤</li></ul></li></ul></li><li><p>它没有考虑词与词之间内在的联系性，如何表达词之间的相似性？</p><ul><li>借助词分类相关的外部知识如WordNet</li></ul></li><li><p>能否用一个<strong>连续的稠密向量去刻画一个word的特征?</strong></p></li></ul><h3 id="潜在语义索引（Latent-Semantic-Index）"><a href="#潜在语义索引（Latent-Semantic-Index）" class="headerlink" title="潜在语义索引（Latent Semantic Index）"></a>潜在语义索引（Latent Semantic Index）</h3><ul><li><p>刻画单词的分布式语义表示</p></li><li><p>“单词-文档”共现矩阵</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/5-1.png" alt="5-1" style="zoom:60%;"></p></li><li><p>奇异值分解：$ X=U \sum V^{T} $ </p></li><li><p>对角阵$\sum$中的奇异值由大到小排列，由于值越大的奇异值越能代表矩阵的主要特征，因此可以忽略较小的奇异值，从而压缩空间。忽略后重新计算三个矩阵的乘积，得到新的”单词-文档”共现矩阵$ X_{k}=U_{k} \sum _{k}V_{k}^{T} $ (<strong>降维</strong>)</p></li><li><p>词相似度矩阵：$XX^T$</p></li><li><p>文档相似度矩阵：$X^TX$</p></li><li><p>优点</p><ul><li>剔除噪声</li><li>为单词和文档之间建立语义关联<ul><li>两篇具有相似词分布的文档可以被认为是有着相近的主题。</li><li><strong>上下文环境相似的两个词有着相近的语义</strong></li></ul></li></ul></li><li><p>缺点</p><ul><li>计算开销大</li><li>线性模型，不能处理非线性依赖</li><li>k值难设置</li><li>未考虑词的顺序</li><li>难以增加新的词和文档</li></ul></li></ul><h3 id="词嵌入向量（WordEmbedding）"><a href="#词嵌入向量（WordEmbedding）" class="headerlink" title="词嵌入向量（WordEmbedding）"></a>词嵌入向量（WordEmbedding）</h3><ul><li><p>WordEmbedding矩阵给每个单词分配一个固定长度的向量表示，这个长度可以自行设定，比如300，实际上会远远小于字典长度（比如10000）。而且两个单词向量之间的夹角值可以作为他们之间关系的一个衡量。通过简单的余弦函数，我们就可以计算两个单词之间的相关性，简单高效：</p><script type="math/tex; mode=display">similarity= \cos ( \theta )= \frac{A \cdot B}{||A||_{2}||B||_{2}}</script></li><li><p>优势</p><ul><li>维度小</li><li>语意相似的词在向量空间上也会比较相近。</li><li>通用性很强，可以用在不同的任务中。</li></ul></li><li>应用<ul><li>计算相似度，比如man和woman的相似度比man和apple的相似度高；</li><li>在一组单词中找出与众不同的一个，例如在如下词汇列表中：[dog, cat, chicken, boy]，利用词向量可以识别出boy和其他三个词不是一类；</li><li>直接进行词的运算，例如经典的：woman+king-man =queen；</li><li>由于携带了语义信息，还可以计算一段文字出现的可能性，也就是说，这段文字是否通顺。</li></ul></li></ul><h4 id="NNLM"><a href="#NNLM" class="headerlink" title="NNLM"></a>NNLM</h4><ul><li><p>一个简单的前向反馈神经网络$ f(w_{t-n+1}, \ldots ,w_{t}) $ 来拟合一个词序列的条件概率$ p(w_{t}|w_{1},w_{2}, \ldots ,w_{t-1}) $ </p><ul><li>首先是一个线性的Embedding层。它将输入的N−1个one-hot词向量，通过一个共享的D×V的矩阵C，映射为N−1个分布式的词向量（distributed vector）。其中，V是词典的大小，D是Embedding向量的维度（一个先验参数）。<strong>C矩阵里存储了要学习的word vector。</strong></li><li>其次是一个简单的前向反馈神经网络g。它由一个tanh隐层和一个softmax输出层组成。通过将Embedding层输出的N−1个词向量映射为一个长度为V的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估</li></ul><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/5-2.png" alt="5-2" style="zoom:80%;"></p></li></ul><h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><ul><li><p>CBOW (Continuous Bag-of-Words Model)</p><p>通过上下文来预测当前值。相当于一句话中扣掉一个词，让你猜这个词是什么。</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/MyBlog\source\_drafts\【复习笔记】自然语言处理\5-4.png" alt="5-4" style="zoom:80%;"></p><p>过程简单介绍如下：<br>（1）输入为C个V维的vector。其中C为上下文窗口的大小，V为原始编码空间的规模。例如，示例中的C=2，V=4.两个vector分别为4维的He和is的one-hot编码形式；<br>（2）激活函数相当简单，在输入层和隐藏层之间，每个input vector分别乘以一个VxN维度的矩阵，得到后的向量各个维度做平均，得到隐藏层的权重。隐藏层乘以一个NxV维度的矩阵，得到output layer的权重；<br>（3）隐藏层的维度设置为理想中压缩后的词向量维度。示例中假设我们想把原始的4维的原始one-hot编码维度压缩到2维，那么N=2；<br>（4）输出层是一个softmax层，用于组合输出概率。所谓的损失函数，就是这个output和target之间的的差（output的V维向量和input vector的one-hot编码向量的差），该神经网络的目的就是最小化这个loss；<br>（5）优化结束后，隐藏层的N维向量就可以作为Word-Embedding的结果。如此一来，便得到了既携带上下文信息，又经过压缩的稠密词向量。</p></li><li><p>Skip-gram  (Continuous Skip-gram Model)</p><p>用当前词来预测上下文。相当于给你一个词，让你猜前面和后面可能出现什么词。</p></li></ul><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/5-3.png" alt="5-3" style="zoom:60%;"></p><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><ul><li><p>LTSM：缓解梯度爆炸、消失问题</p></li><li><p>GRU（Gated Recurrent Unit）</p></li></ul><h3 id="往年考题-2"><a href="#往年考题-2" class="headerlink" title="往年考题"></a>往年考题</h3><ul><li>（1）tf-idf的计算公式。<br>（2）词的分布式表示word embedding相比较于one-hot表示的优势。</li></ul><h2 id="6-词性标注与隐马尔科夫模型"><a href="#6-词性标注与隐马尔科夫模型" class="headerlink" title="6 词性标注与隐马尔科夫模型"></a>6 词性标注与隐马尔科夫模型</h2><h3 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h3><ul><li>定义及任务描述<ul><li>词性又称词类，是指词汇基本的语法属性。</li><li>划分词类的依据：词的形态、词的语法功能、词的语法意义</li></ul></li><li>词性标注的问题－ 标注歧义（兼类词）<ul><li>一个词具有两个或者两个以上的词性</li><li>对兼类词消歧</li></ul></li><li>词性标注之重要性<ul><li>句法分析的预处理</li><li>机器翻译</li><li>Text – Speech （record）</li></ul></li><li>词性标注方法<ul><li>规则方法：<ul><li>词典提供候选词性</li><li>人工整理标注规则</li></ul></li><li>基于错误驱动的方法<ul><li>错误驱动学习规则</li><li>利用规则重新标注词性</li></ul></li><li>统计方法<ul><li>问题的形式化描述</li><li>建立统计模型<ul><li>HMM方法</li><li>最大熵方法</li><li>条件随机场方法</li><li>结构化支持向量机方法</li></ul></li></ul></li></ul></li><li>词性标注的性能指标<ul><li>性能指标：标注准确率</li><li>当前方法正确率可以达到97%<ul><li>正确率基线(Baseline)可以达到90%</li><li>基线的做法：<ul><li>给每个词标上它最常见的词性</li><li>所有的未登录词标上名词词性</li></ul></li></ul></li></ul></li><li>形式化为一个分类问题<ul><li>词串：$x_1x_2…x_n$; 词性串：$y_1y_2…y_n$</li></ul></li><li>决定一个词词性的因素<ul><li>从语言学角度：由词的用法以及在句中的语法功能决定</li><li>统计学角度：<ul><li>和上下文的词性（前后词的标注）相关</li><li>和上下文单词（前后词）相关</li></ul></li></ul></li></ul><h3 id="隐马尔科夫模型（HMM）"><a href="#隐马尔科夫模型（HMM）" class="headerlink" title="隐马尔科夫模型（HMM）"></a>隐马尔科夫模型（HMM）</h3><h4 id="马尔科夫模型"><a href="#马尔科夫模型" class="headerlink" title="马尔科夫模型"></a>马尔科夫模型</h4><ul><li><p>一个系统有$N$个有限状态$S=\{s_1,s_2,…s_N\}$，$Q=(q_1,q_2,…q_T)$是一个随机变量序列。随机变量的取值<br>为状态集$S$中的某个状态。</p></li><li><p>系统在时间$t$处于状态$s_j$的概率取决于其在时间$1,2,…,t-1$的状态，该概率为：</p><script type="math/tex; mode=display">P(q_t=s_j|q_{t-1}=s_i,q_{t-2}=s_k,...,q_1=s_h)</script></li><li><p>假设1：有限视野假设</p></li><li><p>假设2：时间独立性(No change over time)</p></li><li><p>示例： 天气预报</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-2.png" alt="6-2" style="zoom:60%;"></p><ul><li><p>预测：计算未来天气（序列的概率）</p><ul><li><p>晴-雨-晴-雨-晴-多云-晴，未来七天天气是这种情况的概率</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-3.png" alt="6-3" style="zoom:60%;"></p></li></ul></li></ul></li></ul><h4 id="隐马尔科夫模型"><a href="#隐马尔科夫模型" class="headerlink" title="隐马尔科夫模型"></a>隐马尔科夫模型</h4><ul><li><p>词串W，词性串S</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-1.png" alt="6-1" style="zoom:40%;"></p></li><li><p>HMM是一阶马尔可夫模型的扩展</p><ul><li>隐藏的状态序列满足一阶马尔可夫模型</li><li>观察值与状态之间存在概率关系</li></ul></li><li><p>相对于markov模型的又一假设：输出独立性</p><p>$ P(O_{1}, \ldots O_{T}|S_{1}, \ldots S_{T})= \prod _{t}P(O_{t}|S_{t}) $ </p></li><li><p>HMM模型用于词性标注</p><ul><li>$S$：状态集<ul><li>预先定义的词性标注集</li></ul></li><li>$V$：观察集<ul><li>文本中的词汇</li></ul></li><li>$A$：词性之间的转移概率</li><li>$B$：某个词性生成某个词的概率<ul><li>例，P(我|“代词”)</li></ul></li><li>$π$：初始概率</li><li>可见的观察序列为$w_1w_2…w_T$</li></ul></li></ul><h3 id="隐马尔科夫模型的三个基本问题"><a href="#隐马尔科夫模型的三个基本问题" class="headerlink" title="隐马尔科夫模型的三个基本问题"></a>隐马尔科夫模型的三个基本问题</h3><h4 id="求解观察序列的概率"><a href="#求解观察序列的概率" class="headerlink" title="求解观察序列的概率"></a>求解观察序列的概率</h4><p>给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$,计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$</p><ul><li>暴力算法：枚举所有可能的状态序列，依次算出其生成观察序列$O$的概率，求和<ul><li>指数爆炸</li></ul></li></ul><h5 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h5><ul><li><p>定义前向概率:</p><p>给定隐马尔科夫模型$\lambda$,定义到时刻$t$部分观测序列为$o_1,o_2,…,o_t$且状态为$s_i$的概率为前向概率,记作</p><script type="math/tex; mode=display">\alpha_t(i)=P(o_1,o_2,...,o_t,q_t=s_i|\lambda)</script></li><li><p>迭代计算前向概率</p><script type="math/tex; mode=display">\alpha_{t+1}(j)=\left[ \sum_{i=1}^N\alpha_t(i)a_{ij}\right]b_j(o_{t+1}), i=1,2,...N</script><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-4.png" alt="6-4" style="zoom:75%;"></p></li><li><p>$P(O|\lambda)$可由前向概率计算而得：$P(O|\lambda)=\sum_{i=1}^{N}\alpha_T(i)$</p><ul><li>在时间$T$，HMM输出了序列$o_1,o_2,…,o_T$，且位于每个状态$s_i$的概率之和</li></ul></li></ul><blockquote><p>输入:隐马尔科夫模型$\lambda$,观测序列$O$</p><p>输出:观测序列概率$P(O|\lambda)$</p><p>(1) 初值：初始位于$s_i$的概率$\times s_i$生成$o_1$的概率</p><script type="math/tex; mode=display">\alpha_1(i)=\pi_ib_i(o_1), i=1,2,...,N</script><p>(2) 递推　对t=1,2,…,T-1</p><script type="math/tex; mode=display">\alpha_{t+1}(j)=\left[ \sum_{i=1}^N\alpha_t(i)a_{ij}\right]b_j(o_{t+1}), i=1,2,...N</script><p>(3) 终止</p><script type="math/tex; mode=display">P(O|\lambda)=\sum_{i=1}^{N}\alpha_T(i)</script></blockquote><ul><li>复杂度：$O(N^2T)$</li></ul><h5 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h5><ul><li>定义后向概率：给定隐马尔科夫模型$\lambda$,定义在时刻$t$状态为$q_i$的条件下，从$t+1$到$T$的部分观测序列为$o_{t+1},o_{t+2},…,o_T$的概率为后向概率，记作</li></ul><script type="math/tex; mode=display">\beta_t(i)=P(o_{t+1},o_{t+2},...,o_T|q_t=s_i,\lambda)</script><blockquote><p>输入:隐马尔可夫模型$\lambda$,观测序列$O$</p><p>输出:观测序列概率$P(O|\lambda)$</p><p>(1) 初值</p><script type="math/tex; mode=display">\beta_T(i)=1,i=1,2,...,N</script><p>(2)递推：对$t=T-1,T-2,…,1$</p><script type="math/tex; mode=display">\beta_t(i)=\sum_{j=1}^{N}a_{ij}b_j(o_{t+1})\beta_{t+1}(j),i=1,2...N</script><p>(3)求和终结</p><script type="math/tex; mode=display">P(O|\lambda)=\sum_{i=1}^N\pi_ib_i(o_1)\beta_1(i)</script></blockquote><h4 id="求解最优状态序列"><a href="#求解最优状态序列" class="headerlink" title="求解最优状态序列"></a>求解最优状态序列</h4><p>已知模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$,求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,…,i_T)$.即给定观测序列，求最有可能的对应的状态序列。</p><p><strong>维特比算法</strong></p><ul><li><p>定义维特比变量：在时间$t$时，HMM沿着某一条路径到达状态$s_i$，并输出观察序列$O_1O_2…O_t$的最大概率：</p><script type="math/tex; mode=display">\delta _{t}(i)=\max _{q_1,q_2,...,q_{t-1}}P(q_1,q_2,...,q_t=s_{i},O_{1}O_{2} \cdots O_{t}| \mu )</script></li><li><p>与前向算法类似的递推关系：</p><script type="math/tex; mode=display">\delta _{t+1}(i)= \max_j \left[ \delta _{i}(j) \cdot a_{ij} \right] \cdot b_{i}(O_{t+1})</script></li></ul><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-5.png" alt="6-5" style="zoom:67%;"></p><h4 id="HMM参数估计"><a href="#HMM参数估计" class="headerlink" title="HMM参数估计"></a>HMM参数估计</h4><p>已知观测序列$O=(o_1,o_2,…,o_T)$，估计模型$\lambda=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大。即用极大似然估计的方法估计参数。</p><h5 id="有指导学习模型参数——从标注语料中学习"><a href="#有指导学习模型参数——从标注语料中学习" class="headerlink" title="有指导学习模型参数——从标注语料中学习"></a>有指导学习模型参数——从标注语料中学习</h5><ul><li><p>最大似然估计</p><script type="math/tex; mode=display">\bar{\pi}_i=\delta(q_1,s_i)</script><script type="math/tex; mode=display">\begin{aligned}    \bar{a}_{ij}&=\dfrac{Q中从状态q_i转移到q_j的次数}{Q中所有从状态q_i转移到另一状态(包括q_i自身)的次数}\\    &=\dfrac{\sum_{t=1}^{T-1}\delta(q_t,s_i)*\delta(q_{t+1},s_j)}{\sum_{t=1}^{T-1}\delta(q_t,s_i)}\end{aligned}</script><script type="math/tex; mode=display">\bar{b}_j(k)=\dfrac{Q中从状态q_j输出符号v_k的次数}{Q到达q_j的次数}</script></li></ul><h5 id="无指导学习模型参数——Welch-Baum-算法（前向后向算法）"><a href="#无指导学习模型参数——Welch-Baum-算法（前向后向算法）" class="headerlink" title="无指导学习模型参数——Welch-Baum 算法（前向后向算法）"></a>无指导学习模型参数——Welch-Baum 算法（前向后向算法）</h5><p>由于HMM中的状态序列Q是观察不到的(隐变量),因此，这种最大似然估计的方法不可行。所幸的是，期望最大化(expectation maximization,EM)算法可以用于含有隐变量的统计模型的参数最大似然估计。</p><p>基本思想：随机给出模型参数的初始化值，得到最初的模型λ0，然后利用初始模型λ0得到某一状态转移到另一状态的期望次数，然后利用期望次数对模型进行重新估计，由此得到模型λ1，如此循环迭代，重新估计，直至模型参数收敛（模型最优）。</p><h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><ul><li>与HMM比较</li></ul><h3 id="往年考题-3"><a href="#往年考题-3" class="headerlink" title="往年考题"></a>往年考题</h3><ul><li>HMM，“隐”在何处？适合处理哪一类问题？维特比算法的内容？<ul><li>我们不知道模型所经过的状态序列，只知道状态的概率函数，观察到的事件是状态的随机函数。</li></ul></li><li>给定一个HMM模型<br>（1）计算某个序列存在的概率<br>（2）画个图，描述一下HMM的转移过程</li><li>天气晴雨</li></ul><h2 id="7-句法分析"><a href="#7-句法分析" class="headerlink" title="7 句法分析"></a>7 句法分析</h2><h3 id="句法分析简介"><a href="#句法分析简介" class="headerlink" title="句法分析简介"></a>句法分析简介</h3><ul><li>语言视作由组合语法规则制约、旨在传达语义的记号形式系统</li><li>句法：指一门语言里支配句子结构，决定词、短语、从句等句子成分如何组成其上级成分，直到组成句子的规则或过程。[1]这一概念常常与语法混淆，盖因语法研究里面很大一部分都是关于句法的内容，但语法不仅关注句子结构的形成，也关注句子成分的语法功能和语法意义。</li><li>形式语言：给出了语言的语法规则和分类的形式化方法（形式文法），通过对字母、单词、句子的定义，我们用精确的、数学可描述或机器可处理的公式来定义语言</li><li>上下文无关文法<ul><li>示例 PPT12</li><li>歧义<ul><li>程序设计语言<ul><li>Very constrained grammars</li><li>LL(1) grammar, LR Grammar</li><li>推导或规约过程中可以通过一些约束选择正确的产生式规则</li></ul></li><li>自然语言<ul><li>难以避免的歧义</li></ul></li></ul></li></ul></li></ul><h3 id="概率上下文无关文法（PCFG）"><a href="#概率上下文无关文法（PCFG）" class="headerlink" title="概率上下文无关文法（PCFG）"></a>概率上下文无关文法（PCFG）</h3><ul><li>Input: 文法$G$，输入串$string$（句子）</li><li>Output: 推导序列或语法树</li><li>概率上下文无关文法$G=(N,T,S,R,P)$<ul><li>$N$是非终结符号集合</li><li>$T$是终结符号集合</li><li>$S$是开始符号</li><li>$R$是产生式规则，均形如$X→Y_1Y_2…Y_n$，for $n≥0$, $X∈V,Yi ∈(V∪T)$</li><li>$P(R)$ 每条产生式规则赋予的概率</li></ul></li><li>句法结构（子树）的概率=每个节点概率的乘积</li></ul><h3 id="PCFG的三个基本问题"><a href="#PCFG的三个基本问题" class="headerlink" title="PCFG的三个基本问题"></a>PCFG的三个基本问题</h3><h4 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h4><p>给定一个句子$W=w_1w_2…w_n$和文法$G$,如何快速计算概率$P(W|G)$</p><h5 id="内向算法"><a href="#内向算法" class="headerlink" title="内向算法"></a><font color="red">内向算法</font></h5><ul><li><p>利用动态规划算法计算非终结符$A$推导出$W$中子串$w_iw_{i+1}…w_j$的概率$a_{ij}(A)$</p></li><li><p>则有：$ P(W|G)=P(S \Rightarrow W|G)= \alpha _{1n}(S) $ </p></li><li><p>有递推公式如下:</p><script type="math/tex; mode=display">a_{ii}(A)=P(A\rightarrow w_i)</script><script type="math/tex; mode=display">a_{ij}(A)=\sum_{B,C}\sum_{i\le k\le j}P(A\rightarrow BC)\cdot a_{ik}(B)\cdot a_{(k+1)j}(C)</script><p>算法:</p><blockquote><p>输入:PCFG $G(S)$和句子$W=w_1w_2…w_n$</p><p>输出:$a_{ij}(A),1\le i \le j\le n$</p><p>步1 初始化:$a_{ii}(A)=P(A\rightarrow w_i),1\le i\le n$</p><p>步2 归纳计算:$j=1…n,i=1…n-j$,重复下列计算:</p><script type="math/tex; mode=display">   a_{i(i+j)}(A)=\sum_{B,C}\sum_{i\le k \le i+j-1}P(A\rightarrow BC)*a_{ik}(B)*a_{(k+1)(i+j)}(C)</script><p>步3 终结:$P(S\rightarrow w_1w_2…w_n)=a_{1n}(S)$</p></blockquote></li></ul><h5 id="外向算法"><a href="#外向算法" class="headerlink" title="外向算法"></a>外向算法</h5><ul><li><p>定义外向变量$\beta_{ij}(A)$为初始非终结符$S$在推导出语句$W=w_1w_2…w_n$的过程中，产生符号串$w_1…w_{i-1}Aw_{j+1}…w_n$的概率（隐含着$A \rightarrow_* w_i…w_j$</p></li><li><p>有如下递推公式:</p><script type="math/tex; mode=display">\beta_{1n}(A)=\left\{\begin{array}{rcl}1 & & {A=S}\\0 & & {A\neq S}\\\end{array} \right.</script><script type="math/tex; mode=display">\begin{aligned} % requires amsmath; align* for no eq. number\beta_{ij}(A) & =\sum_{B,C}\sum_{k\gt j}P(B\rightarrow AC)\alpha_{(j+1)k}(C)\beta_{ik}(B) \\   & \ \ \ +\sum_{B,C}\sum_{k\lt i}P(B\rightarrow CA)\alpha_{k(i-1)}(C)\beta_{kj}(B)\\\end{aligned}</script></li></ul><h4 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h4><p>给定一个句子$W=w_1w_2…w_n$和文法$G$,如何选择该句子的最佳结构？即选择句法结构树$t$使其具有最大概率:$argmax_tP(t|W,G)$</p><h5 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a><font color="red">维特比算法</font></h5><ul><li><p>定义维特比变量$ \gamma _{i,j}(A) $ :由给定非终结符号$A$生成子串$W_{ij}$的最大概率。</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/7-1.png" alt="7-1" style="zoom:75%;"></p></li></ul><h5 id="自底向上分析器-CKY算法"><a href="#自底向上分析器-CKY算法" class="headerlink" title="自底向上分析器(CKY算法)"></a>自底向上分析器(CKY算法)</h5><ul><li>空间复杂度：|symbols|*n^2 doubles</li><li>时间复杂度：|rules|*n^3</li></ul><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/7-2.png" alt="7-2" style="zoom: 67%;"></p><ul><li>例：PPT44,书190</li></ul><h4 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h4><p>给定PCFG G和句子$W=w_1w_2…w_n$,如何调节$G$的概率参数，使句子的概率最大?即求解$argmax_GP(W|G)$</p><ul><li><p>有监督：从树库中计数得到</p><ul><li><p>相对频率：$ P(A \rightarrow \alpha )= \dfrac{Numbcr(A \rightarrow \alpha )}{ \sum _{ \gamma }Number(A \rightarrow \gamma )} $ </p><ul><li>例如：</li></ul><p>$P(VP \rightarrow Vi)= \dfrac{Number(VP \rightarrow Vi)}{Number(VP \rightarrow Vi)+Number(VP \rightarrow VT\quad NP)+Number(VP \rightarrow VP\quad P)} $ </p></li></ul></li><li><p>无监督：没有树库</p></li><li><p>EM估计– 利用向内向外概率</p></li><li><p>其他</p><ul><li>文法来源<ul><li>树库中抽取文法规则</li></ul></li></ul></li></ul><h3 id="句法分析的性能评价"><a href="#句法分析的性能评价" class="headerlink" title="句法分析的性能评价"></a>句法分析的性能评价</h3><ul><li><p>三元组：[ label, start, end ]</p></li><li><p>计算precision和recall</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/7-3.png" alt="7-3" style="zoom:70%;"></p></li></ul><h3 id="往年考题-4"><a href="#往年考题-4" class="headerlink" title="往年考题"></a>往年考题</h3><ul><li>Cut the envelope with sciors。使用内向算法，PCFG进行分析。</li><li>给出一个上下文无关文法，基于该PCFG，画出句子的最优句法分析树。从语义的角度判断句子的最优句法分析是否合理？如果不合理，该如何解决？</li></ul><h2 id="8-信息抽取"><a href="#8-信息抽取" class="headerlink" title="8 信息抽取"></a>8 信息抽取</h2><ul><li>信息抽取（Information Extraction，简称IE，⼜译信息截取技术）主要是从⼤量⽂字资料中⾃动抽取特定消息（Particular Information），以作为数据库访问（Database Access）之⽤的技术。</li><li>⾮结构化⽂本→结构化数据 →知识库（知识）</li></ul><h3 id="实体识别"><a href="#实体识别" class="headerlink" title="实体识别"></a>实体识别</h3><p>○ ⼈名、地名、机构名…<br>○ 药物名、蛋⽩质名、基因名…<br>○ 领域专有名词</p><ul><li>规则方法</li><li>统计学习方法：<br>○ 任务描述<br>○ 任务形式化<br>○ 模型<br>○ 特征<br>○ 评价<br>○ ……</li></ul><h3 id="关系抽取"><a href="#关系抽取" class="headerlink" title="关系抽取"></a>关系抽取</h3><p>○ 给定头实体和尾实体，识别两者的关系</p><h4 id="封闭域关系抽取"><a href="#封闭域关系抽取" class="headerlink" title="封闭域关系抽取"></a>封闭域关系抽取</h4><ul><li>有限的实体类型，有限的关系类型</li><li>有监督统计学习方法：<br>○ 任务描述及任务形式化<br>○ 训练数据<br>○ 模型<br>○ 特征<br>○ 评价<br>○ ……</li></ul><h4 id="开放域关系抽取"><a href="#开放域关系抽取" class="headerlink" title="开放域关系抽取"></a>开放域关系抽取</h4><ul><li>包含数以千计的关系类型，百万千万级的实体</li><li>样本不平衡性和稀疏性</li><li>语⾔表达多样性和有限性</li></ul><h4 id="基于远程监督方法的关系分类"><a href="#基于远程监督方法的关系分类" class="headerlink" title="基于远程监督方法的关系分类"></a>基于远程监督方法的关系分类</h4><ul><li><p>关系分类的任务描述</p><ul><li>预测实体之间的关系</li><li>例：<ul><li>Sentence : Obama was born in Honolulu, Hawaii, USA as he has always said.</li><li>Relation : born-in</li><li>Fact triplet : <obama, born-in,="" usa=""></obama,></li></ul></li></ul></li><li><p>传统监督学习算法</p><ul><li>依赖大量标注的数据</li></ul></li><li><p>基于远程监督的关系分类方法 （Distant Supervision for Relation Classification）</p><ul><li><p>目标：自动生成大量标注数据</p></li><li><p>假设：如果两个实体在知识库中有关系，所有包含这两个实体的句子都会表达这个关系。</p><p><img src="/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/8-1.png" alt="image-20210109234117470"></p></li></ul></li><li><p>挑战：数据中的噪音</p><ul><li>假设过强：<ul><li>误报（False Positives）：不是每个包含两个实体的句子都在知识库中提到相同的关系。</li><li>False Negatives：由于知识库中没有关系事实，两个实体被错误地标记为无关系(NA)，尽管它们表达了目标关系。</li></ul></li><li>解决<ul><li>抑制噪声（Suppress Noise）:<ul><li>de-emphasize the false positive sentences.</li><li>至少有一个假设:至少有一个提到这两个实体的句子表达了他们的关系。</li><li>多实例学习:将所有提到同一实体对的句子放入一个包中，并以包为单位训练模型。标签只分配给实例的袋子。实例没有标签。<ul><li>在二分类情况下，如果袋子中至少有一个实例是正的，那么袋子被标记为正，如果袋子中所有的实例都是负的，那么袋子被标记为负的</li></ul></li><li>缺点：不能处理所有的句子并非描述同一关系的袋子，也不能处理句子级别的预测。</li></ul></li><li>消除噪声(Remove Noise)<ul><li>消除False Positives</li><li>强化学习<ul><li>从一个句子包中识别并去除错误标记的句子，用清理过的数据集训练关系分类器。</li><li>由于没有直接信号表明句子是否被标错，本文提出了一种强化学习方法来解决这一问题</li><li>通过强化学习来学习agent，以识别每种关系类型的误报，并重新分配训练数据集。</li><li>奖励是通过关系分类器的性能变化来反映的，之前工作的奖励是通过预测的可能性来计算的。</li></ul></li><li>缺点：只针对False Positives，未考虑False Negatives</li></ul></li><li>纠正噪声（Rectify Noise）<br>■ 纠正False Positives和False Negatives<ul><li>以往的工作只是抑制或消除了误报，得到了次优决策边界。</li><li>False Negatives表达了与 positive数据相似的语义信息，为目标关系提供了证据。</li><li>因此，对含噪句子的标签进行修正有助于获得最优决策边界。</li></ul></li></ul></li></ul></li></ul><h3 id="知识库填充"><a href="#知识库填充" class="headerlink" title="知识库填充"></a>知识库填充</h3><p>○ 实体消歧，实体链接等</p><h3 id="事件抽取"><a href="#事件抽取" class="headerlink" title="事件抽取"></a>事件抽取</h3><h2 id="9-机器翻译"><a href="#9-机器翻译" class="headerlink" title="9 机器翻译"></a>9 机器翻译</h2><h3 id="基于规则的机器翻译（since-1950s）"><a href="#基于规则的机器翻译（since-1950s）" class="headerlink" title="基于规则的机器翻译（since 1950s）"></a>基于规则的机器翻译（since 1950s）</h3><ul><li>由语⾔学⽅⾯的专家进⾏规则的制订<br>– ⼀般包含词典和⽤法等组成部分</li><li>需要语⾔学家大量的⼯作；维护难度大；翻译规则容易发生冲突</li></ul><h3 id="基于实例的机器翻译（since-1980s）"><a href="#基于实例的机器翻译（since-1980s）" class="headerlink" title="基于实例的机器翻译（since 1980s）"></a>基于实例的机器翻译（since 1980s）</h3><ul><li>从语料库中学习翻译实例<ul><li>查找接近的翻译实例，并进⾏逐词替换进⾏翻译</li><li>利⽤类⽐思想analogy，避免复杂的结构分析</li></ul></li></ul><h3 id="统计机器翻译（since-1990s）"><a href="#统计机器翻译（since-1990s）" class="headerlink" title="统计机器翻译（since 1990s）"></a>统计机器翻译（since 1990s）</h3><ul><li>从双语平⾏语料中⾃动进⾏翻译规则的学习和应⽤</li><li>词对齐<ul><li>–没有⼤规模标记数据，采⽤⽆监督⽅法学习</li><li>从词语的共现中发掘翻译关系</li></ul></li><li>机器翻译的自动评价<ul><li>评价以何为标准？<ul><li>⼈⼯翻译的结果作为参考译⽂</li><li>使⽤多个参考译⽂增强评价结果的鲁棒性</li></ul></li><li>Word Error Rate (WER)：编辑距离(insertion, deletion, substitution)<ul><li>$ WER= \dfrac{I+D+S}{N} $ </li><li>对流畅性把握较好</li><li>对充分性把握较差</li></ul></li><li>Position-Independent WER (PER)<ul><li>WER对顺序有很强的敏感性，但没有考虑可能发⽣的整体顺序偏移</li><li>PER：忽略顺序，只考虑单词的匹配(unigram matching)</li></ul></li><li>BLEU<ul><li>Unigram Precision of a candidate translation: $\dfrac{C}{N}$<ul><li>N is number of words in the candidate,</li><li>C is the number of words in the candidate which are in at least one reference translation</li><li>存在问题</li></ul></li><li>⽤“Clipping”来进⾏修正：同一单词的count不能超过译文中该单词总数</li><li>长度惩罚因子</li></ul></li></ul></li><li><strong>总结统计机器翻译</strong><ul><li>可以⼀定程度上从数据中⾃动挖掘翻译知识</li><li>流程相对复杂，其中各个部分都不断被改进和优化</li><li>翻译性能遇到瓶颈，难以⼤幅度提升</li></ul></li></ul><h3 id="神经网络机器翻译"><a href="#神经网络机器翻译" class="headerlink" title="神经网络机器翻译"></a>神经网络机器翻译</h3><ul><li>从单词序列到单词序列的翻译⽅式<ul><li>简单直接的把句⼦看做单词序列</li><li>不再依赖⼤量从语料库中学习得到的有噪⾳规则（不需要建模规则的组合关系）</li></ul></li><li>神经网络的引⼊从统计稀疏性和建模两个⽅⾯提升了机器翻译系统</li><li>神经网络机器翻译是⼀种能够更加充分发挥机器⻓处的⾃动翻译⽅法</li></ul><h2 id="往年考题-5"><a href="#往年考题-5" class="headerlink" title="往年考题"></a>往年考题</h2><ul><li><p>建模题</p><ul><li><p>给定任意多个字，拼成一个合理的句子，如何用语言模型描述和解决，概率如何估计？</p></li><li><p>机器已经可以作对联，作诗，甚至写新闻稿，为机器作对联任务进行建模，给出可行的解决方案。</p></li><li><p>设计一个输入法（音字转换）方案，从建模、参数学习、解码这几个方面阐述你的方案</p><ul><li><p>比如<strong>语音识别</strong>，给你一段音频数据，需要识别出该音频数据对应的文字。这里音频数据就是观测变量，文字就是隐藏变量。我们知道，对单个文字而言，虽然在不同语境下有轻微变音，但大致发音是有统计规律的。另一方面，当我们说出一句话时，文字与文字之间也是有一些转移规律的。比如，当我们说出“比”这个字时，下一个大概率的字一般是“如”“较”等。虽然文字千千万，但文字与文字之间的转移却是有章可循的。有了文字的发音特征，以及文字与文字之间的转移规律，那么从一段音频中推测出对应的文字也就可以一试了。插一句，在当前深度学习一统江湖的时代，已经很少有人还在用HMM做语音识别了。</p><p>除了语音识别，你可能已经想到了另一个与之相近的例子了，<strong>输入法</strong>。就拿中文拼音输入法来说，给你一段从键盘输入的字符，你需要从中推测出用户输入的文字是什么，如果有多种可能的文字，你甚至需要给出每段候选文字的概率是多少。这里输入字符序列就是观测变量，要推断的输入文字就是隐藏变量。我们知道，对单个文字而言，与之对应的字符输入序列是有统计规律的。比如，我要打“张”这个字，一般可能的输入是“zh”、“zhang”、“zhagn”等。另一方面，和语音识别的例子一样，文字与文字之间也是有一些转移规律的。利用单个文字的输入统计规律、以及文字与文字之间的转移规律这两方面的信息，从一段字符序列推断对应的输入文字也不是什么难事了。对HMM而言，一般观测序列越长，推断越准。比如，我想输入“从一段字符序列推断对应的输入文字”这句话，当我输入“cong”时，输入法给我的候选字很多，</p></li></ul></li></ul></li><li><p>隐马尔科夫模型和维特比算法</p><p>4.计算题<u>隐马尔科夫模型</u>，viterbi算法</p><p>4.<u>隐马尔科夫模型</u>，给出了状态间的转移概率。 (1)计算当前序列X = abbbaba 存在的概率。 (2)给出观察序列对应的隐藏状态序列，计算生成X的最优状态序列</p></li><li><p>PCFG内向算法、最优句法分析树</p><p>给一个<u>PCFG</u> 利用内向算法 计算句子概率</p><p>2.给一个<u>PCFG</u> 画出最优句法分析树。 从语义角度看是否合理？</p></li><li><p>朴素贝叶斯法则</p><p>文本分类和机器翻译中如何引入<u>朴素贝叶斯法则</u>的，其背后的<u>基本思想和目的</u>分别是什么？结合符号和公式说明</p></li><li><p>文本表示</p><p>5.(1)写出<u>TF-IDF</u> 计算公式  (2)简述word embedding 比 one-hot 表示的优势</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/">复习笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</category>
      
      
      <comments>https://rubychen0611.github.io/2021/01/05/%E3%80%90%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【复习笔记】神经网络及其应用</title>
      <link>https://rubychen0611.github.io/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/</link>
      <guid>https://rubychen0611.github.io/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/</guid>
      <pubDate>Fri, 25 Dec 2020 11:34:02 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;一、生物神经系统与人工神经网络&quot;&gt;&lt;a href=&quot;#一、生物神经系统与人工神经网络&quot; class=&quot;headerlink&quot; title=&quot;一、生物神经系统与人工神经网络&quot;&gt;&lt;/a&gt;一、生物神经系统与人工神经网络&lt;/h2&gt;&lt;h3 id=&quot;生物神经系统的结构&quot;&gt;&lt;a href=&quot;#生物神经系统的结构&quot; class=&quot;headerlink&quot; title=&quot;生物神经系统的结构&quot;&gt;&lt;/a&gt;生物神经系统的结构&lt;/h3&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="一、生物神经系统与人工神经网络"><a href="#一、生物神经系统与人工神经网络" class="headerlink" title="一、生物神经系统与人工神经网络"></a>一、生物神经系统与人工神经网络</h2><h3 id="生物神经系统的结构"><a href="#生物神经系统的结构" class="headerlink" title="生物神经系统的结构"></a>生物神经系统的结构<a id="more"></a></h3><ul><li><p>什么是意识？</p><ul><li>意识是人脑对大脑内外表象的觉察</li><li>人的头脑对于客观物质世界的反映，也是感觉、思维等各种心理过程的总和</li><li>我：意识是一种信息</li></ul></li><li><p>人工神经网络</p><ul><li><p>能从输入数据中学习“信息”</p></li><li><p>信息以某种方式存储在神经网络这个“黑匣子”中</p></li><li><p>人工神经网络能利用学到的“信息”完成很多任务</p><p><u>人工神经网络在“信息”意义上已经具有了某种“意识”</u></p></li></ul></li><li><p>意识是如何产生的？</p><ul><li>二元论：心理和身体是两种不同的物质，他们独立存在——违背了能量守恒定律</li><li>一元论：宇宙只是由一种物质构成<ul><li>唯物主义：所有存在的物质都是物质的、有形的</li><li>唯心主义：没有心理去感知，物理世界将不会存在</li><li>同一性观点：心理过程和某些大脑活动过程是一样的，只不过用不同的术语来描述</li></ul></li></ul></li></ul><h3 id="人工神经网络简介"><a href="#人工神经网络简介" class="headerlink" title="人工神经网络简介"></a>人工神经网络简介</h3><ul><li>神经网络：模拟人类大脑的模型<ul><li>大规模并行分布处理器，由一些简单的处理单元（神经元）组成，能够保存经验知识，并且能够利用这些经验知识完成一些任务</li><li>通过“学习”来从环境中积累知识，知识被保存在神经元之间的连接上</li></ul></li><li><strong>有两个方面类似于人脑</strong>：<ol><li>知识由网络通过学习过程得到；</li><li>神经元间的互联，用来存储知识的已知的联合权值</li></ol></li><li><strong>神经网络的特点</strong><ul><li>面向神经元和联结性</li><li>信息的分布表示<ul><li>并行、分布处理结构</li><li>一个处理单元的输出可以被任意分枝，且大小不变</li><li>记忆效应</li></ul></li><li>运算的全局并行和局部操作<ul><li>处理单元完全的局部操作</li><li>集体效应</li></ul></li><li>处理的非线性性<ul><li>可以模拟任意的数学模型</li></ul></li><li>自组织性</li></ul></li><li><strong>神经网络的能力</strong><ul><li>学习能力<ul><li>学习从环境中获取的信息</li></ul></li><li>泛化能力<ul><li>对从未学习过的输入产生可信的输出</li></ul></li><li>容错能力<ul><li>系统受到局部损伤时仍然能够正常工作</li></ul></li></ul></li></ul><h2 id="二、人工神经元"><a href="#二、人工神经元" class="headerlink" title="二、人工神经元"></a>二、人工神经元</h2><h3 id="MP神经元"><a href="#MP神经元" class="headerlink" title="MP神经元"></a>MP神经元</h3><ul><li><p>结构（f为阶跃函数）</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-1.png" alt="2-1" style="zoom:60%;"></p></li><li><p>特征</p><ul><li>二值网络（binary：fire-1, or not fire-0）：自变量及其函数的值、向量分量的值只取0和1函数、向量。</li><li>由有方向的、带权重的路径联系</li><li>权为正：刺激；权为负：抑制</li><li>全或无：每个神经元有一个固定的阈值，如果输入大于阈值，就fires</li><li>绝对抑制：阈值被设为使得抑制为绝对的，即，非0的抑制输入将阻止该神经元兴奋。</li><li>花费一个时间单位使得信号通过一个连接</li><li>任意命题逻辑函数都可由一两层的MP模型计算</li><li><font color="red">所有的命题逻辑函数都可以用MP AND逻辑门、MP OR逻辑门、MP NOT逻辑门予以表达和实现（*重点）</font> </li><li>MP模型具有神经计算模型一般和普遍的特性，可表达一般人工神经网络的赋权联结和相对抑制</li></ul></li><li><p>与</p><ul><li>W = 1，threshold = -2</li></ul><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-2.png" alt="2-2" style="zoom:50%;"></p></li><li><p>或</p><ul><li><p>W = 1，threshold = -1</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-6.png" alt="2-6" style="zoom:40%;"></p></li></ul></li><li><p>非</p><ul><li><p>W=-1,threshold=0</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-7.png" alt="2-7" style="zoom: 50%;"></p></li></ul></li><li><p>与非</p><ul><li>W = 2, p = 1, threshold = 2</li></ul></li><li><p>异或</p><ul><li><p>X1 XOR x2 &lt;-&gt; (x1 AND NOT x2) OR (x2 AND NOT x1)</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-8.png" alt="2-8" style="zoom:50%;"></p></li><li><p>X1 XOR x2 &lt;-&gt; NOT (NOT x1 OR x2) OR NOT(x1 OR NOT x2)</p></li></ul></li></ul><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><h4 id="最小二乘法（-重点）"><a href="#最小二乘法（-重点）" class="headerlink" title="最小二乘法（*重点）"></a><font color="red">最小二乘法（*重点）</font></h4><p>假设$h(x)=wx+b$，均方误差$ E= \frac{1}{n} \sum _{i=1}^{n} \left[ h(x_{i})-y_{i} \right] ^{2} $ ，问题就被化为求$ =argmin_{w,b}\frac{1}{n} \sum _{i=1}^{n}(wx_{i}+b-y_{i})^{2} $ 。令导数为0，解得：</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-3.png" alt="2-3" style="zoom:50%;"></p><p>神经元表示：</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-4.png" alt="2-4" style="zoom:67%;"></p><h3 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h3><p>线性分类需要更复杂的神经元来解决，两分类问题可用一个神经元来表示，但不同于线性回归，神经元中对数据加权求和后需要在进行一步非线性操作，即用单位阶跃函数对求和结果进行映射。输出𝑦也不再是一个常数，两分类中用0,1表示不同的类别</p><h3 id="感知器神经元"><a href="#感知器神经元" class="headerlink" title="感知器神经元"></a>感知器神经元</h3><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><ul><li><p>功能：将神经元的加权输入线性/非线性转换成一个输出的激活函数</p></li><li><p>阶跃式激活函数</p><ul><li>Identity function (同一函数)</li><li>Threshold function (阈值函数)<ul><li>f(x)=1, if x&gt;=阈值</li><li>f(x)=-1, if x&lt;阈值</li></ul></li><li>Piecewise linear function(分段线性函数)<ul><li>f(x)=1, if x&gt;=阈值</li><li>f(x)=x, if -阈值&lt;x&lt;阈值</li><li>f(x)=-1, if x&lt;=-阈值</li></ul></li></ul></li><li><p>非线性激活函数：保持在上限和下限之间的非线性的连续函数<br>（1）非线性：函数的输出随输入做非线性的变化，<br>（2）连续函数：函数中没有顶点或者中断，可以从始至终进行微分</p><p>$ Sigmoid (z)= \frac{1}{1+e^{-z}} $ </p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-5.png" alt="2-5" style="zoom:50%;"></p></li></ul><h3 id="FT神经元"><a href="#FT神经元" class="headerlink" title="FT神经元"></a>FT神经元</h3><p>启发来源：神经元A接收到来自神经元B的刺激信号后的响应，不仅取决于神经元B的轴突传递强度，还依赖与神经元A的树突浓度，这与神经元A的记忆单元有关。</p><h2 id="三、人工神经元的学习"><a href="#三、人工神经元的学习" class="headerlink" title="三、人工神经元的学习"></a>三、人工神经元的学习</h2><h3 id="生物神经元的学习"><a href="#生物神经元的学习" class="headerlink" title="生物神经元的学习"></a>生物神经元的学习</h3><p>学习的两种具体类型</p><ul><li>经典条件作用<ul><li>由一个刺激或事件预示另一个刺激或事件的到来</li><li>巴浦洛夫的狗——巴浦洛夫条件作用</li><li>刺激泛化：反应自动扩展到与刺激相似的新刺激上</li></ul></li><li>操作性条件作用<ul><li>操作：自发产生的行为，可按照他作用于环境并使环境发生了可观察的结果来描述其特点</li><li>效果律：带来满意结果的反应出现的概率会越来越大，带来不满意结果的反应出现的概率越来越小</li><li>桑代克的迷笼（猫）——指向成功的特定冲动则因愉快的结果而保留下来 </li></ul></li></ul><h3 id="感知器学习方法"><a href="#感知器学习方法" class="headerlink" title="感知器学习方法"></a>感知器学习方法</h3><h4 id="随机学习"><a href="#随机学习" class="headerlink" title="随机学习"></a>随机学习</h4><ul><li><p>随机更新权矩阵𝑾和偏置矩阵𝑩，然后慢慢对赋值进行修正</p></li><li><p>尽管随机学习效率很低，但其思想简单，实现容易，且具有能找到全局最优解等特点，在对其搜索方法进行改进后，也能够具有一定的实际应用意义</p></li></ul><h4 id="Hebbian学习"><a href="#Hebbian学习" class="headerlink" title="Hebbian学习"></a>Hebbian学习</h4><ul><li>一个网络里的信息被储存在神经元之间的权值中；</li><li>假定两个神经元之间<u>的权值变换是与它们神经元的输出成比例</u>的；</li><li>假定随着通过重复和激励一组弱连接神经元而发生学习时，它们之间权值的强度和模式经历逐步增加改变，最终导致神经元形成强连接集合形式。</li><li>假设两个神经元有x和y的输出，如果x激励y，它们之间的连接强度就会增加。两个神经元之间的权值改变Δw与x和y成比例：$\Delta w= \beta x \cdot y$<ul><li>比例系数𝛽叫做“学习率”，决定学习发生的速度。𝛽越大，权值改变得越快。</li></ul></li></ul><h4 id="实例1：作为分类器的有监督学习的感知器（PPT-29-36，-会计算"><a href="#实例1：作为分类器的有监督学习的感知器（PPT-29-36，-会计算" class="headerlink" title="实例1：作为分类器的有监督学习的感知器（PPT 29-36， *会计算)"></a><font color="red">实例1：作为分类器的有监督学习的感知器（PPT 29-36， *会计算)</font></h4><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/2-9.png" alt="2-9" style="zoom:45%;"></p><h4 id="实例2：线性神经元预报器-PPT-38-42，-会计算"><a href="#实例2：线性神经元预报器-PPT-38-42，-会计算" class="headerlink" title="实例2：线性神经元预报器 (PPT 38-42， *会计算)"></a><font color="red">实例2：线性神经元预报器 (PPT 38-42， *会计算)</font></h4><p>上述方法尝试利用一个样本来更新w权重，为何有效？</p><ul><li>其本质是利用单个样本在平方差损失函数$ L(t,y)= \frac{1}{2}(y-t)^{2} $ 上的梯度信息，进行更新。</li></ul><h4 id="梯度下降法学习"><a href="#梯度下降法学习" class="headerlink" title="梯度下降法学习"></a><font color="red">梯度下降法学习</font></h4><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/3-1.png" alt="3-1" style="zoom: 50%;"></p><ul><li><p><strong>学习率调整方法：</strong></p><ul><li><p>（1）学习率衰减</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/3-2.png" alt="3-2" style="zoom: 60%;"></p></li><li><p>（2）学习率预热</p><p>为了让初始阶段的学习更加稳定：在初始的几轮，采用较小的学习率。梯度下降到一定程度之后，再恢复到初始设置的学习率</p></li><li><p>（3）周期学习率</p><p>循环学习率（Cyclic）是让学习率在一个区间内周期性的增大和缩小</p></li><li><p>（4）自适应调整学习率：AdaGrad，RMSprop，AdaDelta</p></li></ul></li><li><p><strong>梯度下降面临的困难</strong></p><ul><li>很难选择一个合适学习率</li><li>对于所有的参数，均使用相同的学习率。不同参数的梯度大小有差异。</li><li>在非凸函数的优化过程中，我们往往希望模型能够跳过那些局部极值点，去找一个更好的极值。实际问题中，鞍点问题很难解决。</li></ul></li><li><p>感知机学习算法</p><ul><li>算法的收敛性：证明经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。</li><li>感知机算法存在许多解，既依赖于初值，也依赖迭代过程中误分类点的选择顺序。</li><li>为得到唯一分离超平面，需要增加约束。</li><li>线性不可分数据集，迭代震荡。</li></ul></li></ul><h3 id="ADALINE-Adaptive-Linear-Element"><a href="#ADALINE-Adaptive-Linear-Element" class="headerlink" title="ADALINE: Adaptive Linear Element"></a>ADALINE: Adaptive Linear Element</h3><ul><li>结构<ul><li>与感知器网络结构非常相似</li><li>激活函数不同：<strong>线性函数</strong></li></ul></li><li>ADALINE的学习：LMS（Least-Mean-Square learning algorithm）</li><li><strong>ADALINE和感知器的比较</strong><ul><li>神经元模型不同<ul><li>感知器：非线性模型，只能输出两种可能的值，其激活函数是<strong>阈值函数</strong></li><li>ADALINE：线性模型，输出可以取任意值，其激活函数是线性函数</li></ul></li><li>功能不同<ul><li>Perceptron:感知器只能做简单的分类</li><li>LMS：还可以实现拟合或逼近</li></ul></li><li><strong>分类性能不同</strong><ul><li>LMS算法得到的分类边界往往处于两类模式的正中间</li><li>感知器学习算法在刚刚能正确分类的位置就停下来，使分类边界离一些模式距离过近，使系统对误差更敏感</li></ul></li></ul></li></ul><h2 id="四、神经元的连接"><a href="#四、神经元的连接" class="headerlink" title="四、神经元的连接"></a>四、神经元的连接</h2><h3 id="神经元的联结综述"><a href="#神经元的联结综述" class="headerlink" title="神经元的联结综述"></a>神经元的联结综述</h3><ul><li>神经元的连接方式<ul><li>层级结构和互联网型结构：通常同一层不具有连接、两个相邻层完全连接(每一层的每一个神经元到另一层的每个神经元)。</li><li>层（级）内联结模式：用来加强和完成层内神经元之间的竞争</li><li>互联网型连接模式：最典型的就是马可夫链和Hopfield网络(HN)。</li></ul></li><li>神经元计算方式<ul><li>循环联结模式</li><li>卷积计算模式</li></ul></li></ul><h3 id="宽度扩展：以多输出的单层感知器为例"><a href="#宽度扩展：以多输出的单层感知器为例" class="headerlink" title="宽度扩展：以多输出的单层感知器为例"></a>宽度扩展：以多输出的单层感知器为例</h3><p>算法思想：将单输出感知器的处理逐个地用于多输出感知器输出层的每一个神经元的处理。</p><ul><li><p>离散多输出感知器训练算法</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/3-3.png" alt="3-3" style="zoom:50%;"></p></li><li><p>连续多输出感知器训练算法</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/3-4.png" alt="3-4" style="zoom:50%;"></p></li></ul><h3 id="深度扩展：多层感知器"><a href="#深度扩展：多层感知器" class="headerlink" title="深度扩展：多层感知器"></a>深度扩展：多层感知器</h3><p>在输入层和输出层之间至少有一层（称之为隐藏层，hidden layer）</p><ul><li>和单层神经网络相比能解决更加复杂的问题</li><li>训练更加困难</li><li>有些情况下能解决单层不能解决的问题</li></ul><p>代表：多层感知器、CNN</p><ul><li><font color="red">手工搭建并设置单隐藏层网络模型：拟合sin函数 (*计算,PPT53-60)</font><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/4-1.png" alt="4-1"></p></li></ul><h3 id="其他连接方式"><a href="#其他连接方式" class="headerlink" title="其他连接方式"></a>其他连接方式</h3><ul><li>卷积神经网络（Convolutional Net）</li><li>竞争神经网络（Competitive Net ）</li><li>循环神经网络（Recurrent Net ）</li><li>其他类型网络</li></ul><h3 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h3><ul><li><p>阈值函数为什么不适合作为激活函数？</p><ul><li>理由，大多数激活函数都需要满足以下条件（当然也存在不完全满足的，比如relu函数）<ol><li>神经网络的激活函数需要满足的条件之一就是函数是<strong>连续可导</strong>的，这样才能使用梯度下降法更新网络参数，这里该函数在x=0处不是连续可导的</li><li>神经网络激活函数还需要满足函数是<strong>单调增/减</strong>的，这里也不满足</li></ol></li><li><u>不适合，因为它不连续、不光滑，导致在x=0处不可微，并且在其他地方的导数是零，无法使用基于梯度的参数优化方法。</u></li></ul></li><li><p>能否用一个神经元拟合二次曲线吗？如果能，请给出实例。如果不能，请说明至少需要多少个神经元才能拟合二次曲<br>线。</p><ul><li><p>能，考虑二次函数的一般形式 $y=ax^2 + bx + c$，可设计如下单个神经元结构就能拟合二次曲线。</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/4-2.png" alt="4-2" style="zoom: 33%;"></p></li></ul></li></ul><h2 id="五、多层感知器"><a href="#五、多层感知器" class="headerlink" title="五、多层感知器"></a>五、多层感知器</h2><h3 id="BP网络的训练—基本算法"><a href="#BP网络的训练—基本算法" class="headerlink" title="BP网络的训练—基本算法"></a>BP网络的训练—基本算法</h3><ul><li><font color="red">弱点：训练速度非常慢、局部极小点的逃离问题、算法不一定收敛</font></li><li><font color="red">优点：广泛的适应性和有效性</font><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/5-1.png" alt="5-1" style="zoom:80%;"></p></li></ul><font color="red">计算例子（PPT 21-24）</font><h3 id="BP算法的改进"><a href="#BP算法的改进" class="headerlink" title="BP算法的改进"></a>BP算法的改进</h3><ul><li><p>BP针对单个样例进行更新</p><ul><li>不同样例的更新的效果可能“抵消”。BP网络接受样本的顺序对训练结果有较大影响。它更“偏爱”较后出现的样本。给样本安排一个适当的顺序，是非常困难的。</li><li><strong>用多个样本的“总效果”修改权重</strong>$ \Delta w_{ij}^{(k)}= \sum _{p} \Delta w_{ij}^{(k)} $ </li></ul></li><li><p>消除样本顺序影响的BP算法:</p><ul><li>较好地解决了因样本的顺序引起的精度问题和训练的抖动问题</li><li>收敛速度：比较慢</li><li>偏移量：给每一个神经元增加一个偏移量来加快收敛速度</li><li>冲量：<u>联接权的本次修改要考虑上次修改的影响，以减少抖动问题</u></li></ul></li></ul><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/5-2.png" alt="5-2" style="zoom: 50%;"></p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/5-3.png" alt="5-3" style="zoom:55%;"></p><ul><li>设置冲量的BP算法</li></ul><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/5-4.png" alt="5-4" style="zoom:60%;"></p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/5-5.png" alt="5-5" style="zoom:94%;"></p><h3 id="算法的理论基础（-掌握）"><a href="#算法的理论基础（-掌握）" class="headerlink" title="算法的理论基础（*掌握）"></a><font color="red">算法的理论基础（*掌握）</font></h3><ul><li><font color="red">实例：拟合正弦波的第一个1/4周期波形(PPT 42-63)</font></li><li><p>局部极小点问题</p><ul><li>修改初值：从多个初始点开始进行搜索。</li><li>模拟退火：以一定概率接受比当前结果更差的解。</li><li><strong>随机梯度下降：计算梯度值时，加入了随机因素，极小值点处计算的梯度不为0，有机会跳出局部极小。</strong></li><li>遗传算法</li></ul></li><li>几个问题的讨论<ul><li>收敛速度问题</li><li>网络瘫痪问题<ul><li>在训练中，权可能变得很大，这会使神经元的网络输入变得很大，从而又使得其激活函数的导函数在此点上的取值很小。根据相应式子，此时的训练步长会变得非常小，进而将导致训练速度降得非常低，最终导致网络停止收敛</li></ul></li><li>稳定性问题<ul><li>用修改量的综合实施权的修改</li><li>连续变化的环境，它将变成无效的</li></ul></li><li>步长问题<ul><li>BP网络的收敛是基于无穷小的权修改量</li><li>步长太小，收敛就非常慢</li><li>步长太大，可能会导致网络的瘫痪和不稳定</li><li>自适应步长，使得权修改量能随着网络的训练而不断变化。[1988年，Wasserman]<ul><li>AdaGrad方法：在模型训练初期，参数变化快（学习率大），而在模型训练后期，参数变化慢且梯度更新值小。</li></ul></li></ul></li></ul></li></ul><h3 id="作业-1"><a href="#作业-1" class="headerlink" title="作业"></a>作业</h3><ul><li>进一步提高神经网络识别率的方法：</li></ul><ol><li>增加隐藏层数；</li><li>增加隐藏层神经元个数；</li><li>增加训练轮数；</li><li>获取更多训练数据；</li><li>使用卷积神经网络等其他网络结构</li></ol><h2 id="六、正则化、归一化和预训练"><a href="#六、正则化、归一化和预训练" class="headerlink" title="六、正则化、归一化和预训练"></a>六、正则化、归一化和预训练</h2><h3 id="数据、模型与特征工程"><a href="#数据、模型与特征工程" class="headerlink" title="数据、模型与特征工程"></a>数据、模型与特征工程</h3><ul><li>常见的数据问题<ul><li>不同维度差异过大（数据中心偏置）</li><li>正负例样本不均衡</li></ul></li><li>模型问题<ul><li>过/欠拟合</li><li>梯度消失/梯度爆炸</li><li>模型过大，难以重新训练等</li></ul></li><li>解决上述问题常用的手段有：<br>1、正则化，规一化<br>2、预训练和迁移学习<br>3、特征预处理（特征工程）</li></ul><h3 id="正则化与归一化"><a href="#正则化与归一化" class="headerlink" title="正则化与归一化"></a>正则化与归一化</h3><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ul><li><p>过拟合和欠拟合</p><ul><li>过拟合：模型过于复杂，参数过多或训练数据过少，噪声过多。</li><li>欠拟合：模型比较简单，特征维度过少。</li></ul></li><li><p>偏差和方差：定量分析</p><ul><li><p>偏差：衡量模型预测值和实际值之间的偏离关系，即模型在样本上拟合得好不好。</p></li><li><p>方差：描述模型在整体数据上表现的稳定情况，在训练集和验证集/测试集上表现是否一致</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/6-1.png" alt="6-1" style="zoom:60%;"></p></li></ul></li><li><p>正则化：<strong>通过限制模型的复杂度，避免过拟合，提高泛化能</strong></p></li><li><font color="red">不同的正则化方法</font></li></ul><p>1、<font color="red">L1和L2正则化（*掌握）</font></p><ul><li><p>l1正则化：$ argmin_{ \theta } \frac{1}{N}( \sum _{i}^{N} L(y_{i},f(x_{i}, \theta ))+ \lambda | \theta |) $ </p><ul><li>目的：<strong>使𝜃更容易为0，整体权重矩阵更为稀疏，抑制过拟合。</strong></li></ul></li><li><p>l2正则化：$ argmin_{ \theta } \frac{1}{N}( \sum _{i}^{N}L(y_{i},f(x_{i}, \theta ))+ \lambda \theta ^{2}) $ </p></li><li><p>目的：<strong>使得权重变得更小</strong></p></li></ul><ul><li>参数$\lambda$的意义<ul><li>$\lambda$：正则化项系数，用来控制正则化项的“力度”，<u>平衡损失函数和正则化项之间的关系</u>。<ul><li>若ƛ项越大，正则化项在损失函数中所占比例更大，因此损失函数更倾向于优化正则化项，对原始的损失函数V产生较小影响，易导致模型简单，发生欠拟合；</li></ul></li><li>若ƛ项越小，相比于原始的损失函数，正则化项在损失函数中所占比例很小，几乎<br>不起作用，容易发生过拟合问题。</li></ul></li></ul><p>  2、权重衰减法</p><ul><li><p>每次参数更新时，都先对参数进行一定衰减：$ \theta :=(1-w) \theta - \alpha d \theta $ （其中w为权重衰减系数）</p><p>3、丢弃法</p></li><li><p>在训练时，以概率p随机丢弃部分神经元。</p></li><li><p>1.相当于取平均的作用，取每次丢弃后子网络的平均结果。</p><ul><li>2.降低神经元之间的敏感度，增加整体鲁棒性。</li></ul><p>4、提前停止</p></li><li><p>思路：提早结束训练</p></li><li><p>验证集错误率基本不下降时或有反增趋势时，可以提前停止训练。</p><p>5、数据增强</p></li><li><p>在不实质增加数据的情况下，对当前数据执行一些操作达到数据增加的效果。</p><ul><li>图像数据：翻转、旋转、镜像、裁剪、增加高斯白噪声等</li><li>文本数据：同义词替换、随机插入、随机交换、随机删除等</li></ul></li></ul><h4 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h4><ul><li><p>为什么要归一化？</p><ul><li>提升模型的收敛速度<ul><li>所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛</li></ul></li></ul></li><li><font color="red">最常用的归一化方法</font><ul><li><p>Min-max归一化：将结果映射到[0,1]之间$ \widehat{x}= \frac{x- \min (x)}{ \max (x)- \min (x)} $ </p></li><li><p>Z-score归一化（标准归一化）：$ \widehat{x}= \frac{x- \mu }{ \sigma } $ </p></li><li><p><strong>批归一化（Batch Normalization，BN）</strong></p><ul><li>思路：逐层归一化方法，对神经网络中任意的中间层进行归一化操作。使得净输入的分布一致（例如正态分布），一般应用在激活函数之前。</li><li>加快了模型的收敛速度，而且更重要的是在一定程度缓解了深层网络中“梯度弥散”的问题，从而使得训练深层网络模型更加容易和稳定。</li><li>BN的输出服从什么分布？<ul><li>标准正态分布，N(0,1)：均值为0，方差为1.</li></ul></li><li><strong>Batch normalization为什么归一化后还有放缩（γ ）和平移（β ）？</strong><ul><li>平移和放缩是变换的反操作。通过反操作将标准化后的数据尽可能恢复。如果批标准化没有发挥作用，通过放缩和平移可以抵消一部分标准化的作用。防止网络表达能力下降，恢复数据。弥补归一化后模型损失的表征能力。</li><li>规范化操作让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是我们通过变换操作改变了原有数据的信息表达，使得底层网络学习到的参数信息丢失。另一方面，通过让每一层的输入分布均值为0，方差为1，会使得输入在经过sigmoid或tanh激活函数时，容易陷入非线性激活函数的线性区域。放缩和平移这两个参数对规范化后的数据进行线性变换，是为了<u>恢复数据本身的表达能力，保留原始输入特征的分布信息。</u></li></ul></li><li>算法</li></ul><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/6-2.png" alt="6-2" style="zoom: 60%;"></p></li><li><p>层归一化（Layer Normalization，LN）</p><ul><li>思路：对中间层的所有神经元进行归一化</li></ul></li><li><p>实例归一化（Instance Normalization，IN）</p><ul><li>主要用于依赖于某个图像实例的任务。</li></ul></li><li><p>N：batch维度，C：特征通道维度，H、W：特征图高和宽维度。</p><ul><li>对每个样本的H和W的数据求均值和标准化，保留N、C维度。</li></ul></li><li><p>组归一化（Group Normalization，GN）</p><ul><li>把特征通道分为G组，每组有C/G个特征通道，在组内归一</li></ul></li><li><p>可转换归一化（Switchable Normalization，SN）</p><ul><li>将批归一化、层归一化、实例归一化结合起来的方法，使网络自适应学习如何组合起来的权重。</li></ul><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/6-3.png" alt="6-3" style="zoom:80%;"></p></li></ul></li></ul><h3 id="初始化、预训练和迁移学习"><a href="#初始化、预训练和迁移学习" class="headerlink" title="初始化、预训练和迁移学习"></a>初始化、预训练和迁移学习</h3><h4 id="网络参数初始化"><a href="#网络参数初始化" class="headerlink" title="网络参数初始化"></a>网络参数初始化</h4><ul><li><p>不同的初始化参数，计算得到的“全局最小”差距很大。好的初始化值能够帮助网络更快地计算得到最优值，更容易收敛到目标函数。</p></li><li><p>目前没有发现一种初始化方式可以适用于任何网络结构。初始化需要避免“对称权重”现象（唯一确知的特性）</p></li><li><p>权重矩阵的初始化</p><ul><li>均匀分布初始化<ul><li>主要思想：在区间(-r,r)的均匀分布U(-r,r)中随机选取所有网络权值</li><li>如何确定区间范围？尽量保持梯度能够在多层网络中传播</li><li>规则1：对于任意网络权值$w_{ij}$，从如下分布中随机选取初始值：$ w_{ij} \sim U(- \frac{1}{ \sqrt{n_{i}}}, \frac{1}{ \sqrt{n_{i}}}) $ (其中$n_i$表示第i层的神经元数量。)</li><li>规则2：Xavier 初始化</li></ul></li><li>高斯分布初始化<ul><li>主要思想：在固定均值和固定方差的高斯分布中随机选取所有网络权值</li><li>缺陷：深层模型会非常难以收敛</li></ul></li><li>稀疏初始化<ul><li>主要思想：稀疏初始化降低连接数量，使得初始化的数值不会太小</li></ul></li><li>正交初始化<ul><li>主要思想：正交初始化可以避免训练开始时就出现梯度消失或梯度爆炸现象</li></ul></li></ul></li><li><p>偏置矩阵初始化</p><ul><li>偏置矩阵通常不需要考虑破坏对称性的问题，通常我们可以把偏置矩阵初始化为<u>全0矩阵</u>；</li><li>除了一些例外情况：<ul><li>偏置作为输出单元，初始化偏置以获得正确的输出边缘的统计是有利的；</li><li>需要选择偏置以避免初始化引起的太大饱和</li></ul></li></ul></li><li><p><strong>不合理的初始化会导致梯度消失或爆炸现象</strong><br>（1）大的梯度会使网络十分不稳定，会导致权重成为一个特别大的值，最终导致溢出而无法学习<br>（2）小的梯度传过多层网络，达到靠近输入的隐藏层后会越来越小，导致隐藏层无法正常地进行学习。</p></li></ul><h4 id="网络预训练和迁移学习"><a href="#网络预训练和迁移学习" class="headerlink" title="网络预训练和迁移学习"></a>网络预训练和迁移学习</h4><ul><li>网络预训练：采用相同结构的，并且已经训练好的网络权值作为初始值，在当前任务上再次进行训练</li><li>为什么使用网络预训练？<ul><li>为了能够在更短时间内训练得到更好的网络性能</li><li>相似的任务之间，训练好的神经网络可以复用，通常作为特征提取器</li></ul></li><li>预训练方法<ul><li>无监督预训练<ul><li>玻尔兹曼机</li><li>自编码器</li></ul></li><li>有监督预训练：迁移学习<ul><li>主要思想：通过大量带标签的数据集，大幅减少网络收敛的训练时间。站在巨人的肩膀上，复用已经得到的研究成果</li><li>使用预训练模型的方式：<ul><li>（1）<strong>直接作为特征提取网络</strong>：即将网络直接用于数据的特征提取，将输出作为特征，根据任务目标进行后续的分类、回归等操作，不再对这些预训练层进行进一步的学习，可以看作预训练模型“冻结（frozen）”了；</li><li>（2）<strong>作为初始化模型进行微调</strong>：部分或全部使用这个模型作为初始化模型，根据手头的数据对这个模型进行再次训练，这个过程被称为“精调（fine-tune）”。</li></ul></li></ul></li></ul></li></ul><h2 id="七、神经动力学系统"><a href="#七、神经动力学系统" class="headerlink" title="七、神经动力学系统"></a>七、神经动力学系统</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul><li><p>神经动力学：研究神经系统随时间的变化过程和规律</p><ul><li><p>确定性神经动力学：神经网络有确定的行为。用一组非线性微分方程描述。</p></li><li><p>统计性神经动力学：神经网络受到噪声扰动，在数学上采用随机性的非线性微分方程来描述系统的行为，方程的解用概率表示</p></li></ul></li><li><p>动力学系统</p><ul><li>大型的非线性动力学系统的动力特性可用下面的微分方程表示： $\frac{d}{dt}V(t)=F(V(t)) $</li><li>如果一个非线性动力系统的向量函数$F(V(t))$不隐含地依赖于时间$t$，则此系统称为自治系统，否则不是自治的。</li></ul></li></ul><h3 id="离散Hopfield网络（重点）"><a href="#离散Hopfield网络（重点）" class="headerlink" title="离散Hopfield网络（重点）"></a><font color="red">离散Hopfield网络（重点）</font></h3><ul><li><p>离散Hopfield网络是单层全互连的，其表现形式有两种</p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/7-1.png" alt="7-1" style="zoom:60%;"></p><ul><li>神经元可取二值{0/1}或{−1/1}</li><li><strong>条件</strong>：神经元之间联接是对称的，即$ W_{ij}=W_{ji} $ ；神经元自身无联接,即$W_{ii} = 0$</li><li>每个神经元都将其输出通过突触权值传递给其它的神经元，同时每个神经元又都接收其它神经元传来的信息</li><li>对每个神经元来说，其输出信号经过其它神经元后又有可能反馈给自己，所以Hopfield网络是一种<strong>反馈神经网络</strong>。</li><li>输出计算：$ u_{i}(t)= \sum _{j=1,j\neq i}^{n}w_{ij}v_{j}(t)+b_{i} $ , $ v_{i}(t+1)=f(u_{i}(t)) $ <ul><li>其中的激励函数$f(\cdot)$可取阶跃函数或符号函数</li></ul></li></ul></li><li><p>运行规则：Hopfield网络的工作方式主要有两种形式</p><ul><li><p><font color="red"><strong>串行（异步）工作方式</strong></font>：在任一时刻$t$，只有某一神经元$i$（随机的或确定的选择）依上式变化，而其他神经元的状态不变。</p><blockquote><p>第一步：对网络进行初始化<br>第二步：从网络中随机选取一个神经元$i$<br>第三步：求出该神经元$i$的输入 $ u_{i}(t)= \sum _{j=1,j\neq i}^{n}w_{ij}v_{j}(t)+b_{i} $</p><p>第四步：求出该神经元$i$的输出$ v_{i}(t+1)=f(u_{i}(t)) $ ，此时网络中的其它神经元的输出保持不变</p><p>第五步：判断网络是否达到稳定状态，若达到稳定状态或满足给定条件，则结束；否则转到第二步继续运行。这里网络的稳定状态定义为：若网络从某一时刻以后，状态不再发生变化，则称网络处于稳定状态。</p></blockquote><ul><li><p>Hopfield网络“能量函数”（Lyapunov函数）的“能量”在网络运行过程中应不断地降低，<strong>最后达到稳定的平衡状态</strong></p><ul><li>能量函数：$ E=- \frac{1}{2} \sum _{i=1,i\neq j}^{n} \sum _{j=1,j\neq i}^{n}w_{ji}v_{j}v_{j}+ \sum _{j=1}^{n}b_{i}v_{i} $ </li><li><p>上式所定义的“能量函数”值应单调减小。所以在满足参数条件下，Hopfield网络状态是向着能量函数减小的方向演化。由于能量函数有界，所以系统必然会趋于稳定状态，该稳定状态即为Hopfield网络的输出</p></li><li><p>能量函数的变化曲线含有全局最小点和局部最小点。将这些极值点作为记忆状态，可将Hopfield网络用于<u>联想记忆</u>；将能量函数作为代价函数，全局最小点看成最优解，则Hopfield网络可用于<u>最优化计算</u></p></li></ul></li></ul></li><li><p>并行（同步）工作方式：在任一时刻t，部分神经元或全部神经元的状态同时改变。</p></li></ul></li></ul><h3 id="连续Hopfield网络"><a href="#连续Hopfield网络" class="headerlink" title="连续Hopfield网络"></a>连续Hopfield网络</h3><p>激励函数为连续函数</p><h3 id="联想记忆"><a href="#联想记忆" class="headerlink" title="联想记忆"></a>联想记忆</h3><ul><li><p>人工神经网络的联想就是指系统在给定一组刺激信号的作用下，该系统能联系出与之相对应的信号。联想是以记忆为前提的，即首先信息存储起来，再按某种方式或规则将相关信息取出。联想记忆的过程就是信息的存取过程。</p></li><li><p>所谓的联想记忆也称为<strong>基于内容的存取</strong>（Content-addressed memory），信息被分布于生物记忆的内容之中，而不是某个确定的地址。</p><ul><li>（1）信息的存贮是按内容存贮记忆的(content addressable memory CAM)，而传统的计算机是基于地址存贮的（Addressable Memory）即一组信息对应着一定的存储单元。<br>（2）信息的存贮是分布的，而不是集中的。</li></ul></li><li><p>联想记忆的分类：联想记忆可分为自联想与异联想。Hopfield网络属于自联想。</p><ul><li><strong>自联想</strong>记忆(Auto-AssociativeMemory)<br>自联想能将网络中输入模式映射到存贮在网络中不同模式中的一种。联想记忆网络不仅能将输入模式映射为自己所存贮的模式，而且还能对具有缺省/噪音的输入模式有一定的容错能力。<ul><li>设在学习过程中给联想记忆网络存入$M$个样本,若给联想记忆网络加以输入$ X^{ \prime }=X^{m}+V $ ，其中$X^{m}$是$M$个学习样本之一，$V$是偏差项（可代表噪声、缺损与畸变等），通过自联想联想记忆网络的输出为$X^{m}$，即使之复原（比如，破损照片→完整照片）</li></ul></li><li><strong>异联想</strong>网络：在受到具有一定噪音的输入模式激发时，能通过状态的演化联想到原来样本的模式对(如从破损照片得到某人的姓名)</li></ul></li><li><p>联想记忆的工作过程</p><ul><li><p>（1）<strong>记忆阶段</strong>：在记忆阶段就是通过设计或学习网络的权值，使网络具有<u>若干个稳定的平衡状态</u>，这些稳定的平衡状态也称为<strong>吸引子（Attractor）</strong></p><ul><li>吸引子有一定的吸引域（Basin of Attraction），吸引子的吸引域就是能够稳定该吸引子的<u>所有初始状态的集合</u>，吸引域的大小用吸引半径来描述，吸引半径可定义为：吸引域中所含所有状态之间的最大距离或吸引子所能吸引状态的最大距离</li><li>吸引子也就是<u>联想记忆网络能量函数的极值点</u>，记忆过程就是将<u>要记忆和存储的模式设计或训练成网络吸引子的过程</u></li></ul></li><li><p>（2）<strong>联想阶段</strong></p><ul><li>联想过程就是给定输入模式，联想记忆网络通过动力学的演化过程达到稳定状态，即收敛到吸引子，回忆起已存储模式的过程。</li></ul><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/7-2.png" alt="7-2" style="zoom:40%;"></p><ul><li>吸引子的数量代表着AM的记忆容量（Memory Capacity）或存储容量（Storage Capacity），存储容量就是在一定的联想出错概率容限下，网络中存储互不干扰样本的最大数目</li><li>吸引子具有一定的吸引域，吸引域是衡量网络容错性的指标，<u>吸引域越大网络的容错性能越好，或者说网络的联想能力就越强</u></li></ul></li></ul></li><li><p><strong>Hopfield联想记忆网络</strong></p><ul><li><p>将Hopfield网络作为AM需要设计或训练网络的权值，使吸引子存储记忆模式。常用的设计或学习算法有：外积法（OuterProductMethod）、投影学习法（ProjectionLearningRule）、伪逆法（PseudoInverseMethod）以及特征结构法（EigenStructureMethod）等</p></li><li><font color="red">Hopfield联想记忆网络运行步骤</font><ul><li><p>第一步：设定记忆模式。将欲存储的模式进行编码，得到取值为1和−1的记忆模式($m&lt;n$)：</p><p>$ U_{k}= \left[ u_{1}^{k},u_{2}^{k}, \cdots ,u_{i}^{k}, \cdots ,u_{n}^{k} \right] ^{T}k=1,2, \cdots ,m $ </p></li><li><p>第二步：设计网络的权值</p><p>$ w_{ij}= \begin{cases} \frac{1}{N} \sum _{ \mu =1}^{M}u_{i}^{k}u_{j}^{k},j \neq i \\\\ 0,j=i \end{cases} $ </p></li><li><p>第三步：初始化网络状态。将欲识别模式$ U^{ \prime }= \left[ u_{1}’,u_{2}’, \cdots ,u_{i}’, \cdots ,u_{n}’\right] ^{T} $ 设为网络状态的初始状态，即：$ \nu _{i}(0)=u_{i} $ ，是网络中任意神经元$i$在$t=0$时刻的状态</p></li><li><p>第四步：迭代收敛，随机地更新某一神经元的状态$ \nu _{i}(t+1)=Sgn \left[ \sum _{j=1}^{N}w_{ij}x_{j}(n) \right] $ 。反复迭代直至网络中所有神经元的状态不变为止</p></li><li><p>第五步：网络输出，这时的网络状态（稳定状态），即为网络的输出$y=v_i(T)$</p></li></ul></li><li><p>Hopfield联想记忆网络的记忆容量：就是在一定的联想出错概率容限下，网络中存储互不干扰样本的最大数目。记忆容量$\alpha$反映所记忆的模式$m$和神经元的数目$N$之间的关系：$\alpha=m/N$。记忆$m$个模式所需的神经元数$N=m/\alpha$，联接权值数目为$(m/\alpha)^2$，若$\alpha$增加一倍，联接权值数目降为原来的$1/4$，这是一对矛盾。在技术实现上也是很困难的。实验和理论研究表明Hopfield联想记忆网络记忆容量的上限为$0.15N$。</p></li><li><p>Hopfield AM网络存在伪状态（Spurious States），伪状态是指除记忆状态之外网络多余的稳定状态。</p></li></ul></li></ul><h3 id="最优化计算"><a href="#最优化计算" class="headerlink" title="最优化计算"></a>最优化计算</h3><p>将Hopfield神经网络应用于求解组合优化问题，就是把目标函数转化为网络的能量函数，把问题的变量对应于网络的状态，当网络的能量函数收敛于极小值时，网络的状态就对应于问题的最优解。</p><h2 id="八、随机神经网络"><a href="#八、随机神经网络" class="headerlink" title="八、随机神经网络"></a>八、随机神经网络</h2><h3 id="基本的非确定方法"><a href="#基本的非确定方法" class="headerlink" title="基本的非确定方法"></a>基本的非确定方法</h3><ul><li><p>非确定的方法：生物神经网络按照概率运行（别称：统计方法（Statistical Method））</p></li><li><p>既可以用于训练，又可以用于运行</p></li><li><p>基本思想</p><ul><li>从所给的网络中“随机地选取一个联接权”</li><li>对该联接权提出一个“伪随机调整量”</li><li>当用此调整量对所选的联接权进行修改后<ul><li>如果“被认为”修改改进了网络的性能，则保留<br>此调整；</li><li>否则放弃本次调整。</li></ul></li></ul></li><li><p>算法1 基本统计训练算法</p><p>1、从样本集$S$中取一样本$(X,Y)$；</p><p>2、将$X$输入到网络中，计算出实际输出$O$；</p><p>3、求出网络关于$Y$、$O$的误差测度$E$；</p><p>4、随机地从$W ^ { ( 1 ) } W ^ { ( 2 ) }, \ldots, W ^ { ( L ) }$中选择一个联接权$ w_{ij}^{(p)} $ ；</p><p>5、生成一个小随机数$ \Delta w_{ij}^{(p)} $ ；</p><p>6、用$ \Delta w_{ij}^{(p)} $ 修改$ w_{ij}^{(p)} $ ；</p><p>7、用修改后的$W ^ { ( 1 ) } W ^ { ( 2 ) }, \ldots, W ^ { ( L ) }$重新计算$X$对应的实际输出$O’$；<br>8、求出网络关于$Y$、$O’$的误差测度$E’$ ；<br>9、如果$E’&lt;E$，则保留本次对$W ^ { ( 1 ) } W ^ { ( 2 ) }, \ldots, W ^ { ( L ) }$的修改，否则，根据概率判断本次修改是否有用，如果认为有用，则保留本次对$W ^ { ( 1 ) } W ^ { ( 2 ) }, \ldots, W ^ { ( L ) }$的修改，如果认为本次修改无用，则放弃它；<br>重复上述过程，直到网络满足要求。</p></li></ul><h3 id="模拟退火（重点）"><a href="#模拟退火（重点）" class="headerlink" title="模拟退火（重点）"></a><font color="red">模拟退火（重点）</font></h3><ul><li>金属中原子的能量与温度有关。原子能量高的时候，有能力摆脱其原来的能量状态而最后达到一个更加稳定的状态——全局极小能量状态</li><li>在金属的退火过程中，能量的状态分布$ P(E) \propto exp(- \frac{E}{kT}) $ <ul><li>$P(E)$——系统处于具有能量$E$的状态的概率；</li><li>$k$——Boltzmann常数；</li><li>$T$——系统的绝对温度(Kelvin)</li></ul></li><li>高温情况下：$T$足够大，对系统所能处的任意能量状态$E$，有$exp(- \frac{E}{kT}) $ 将趋于1</li><li>中温情况下：$T$比较小，$E$的大小对$P(E)$有较大的影响，设$E_1&gt;E_2,P(E_2)&gt;P(E_1)$，即，系统处于高能量状态的可能性小于处于低能量状态的可能性</li><li>低温情况下：$T$非常小，$E$的大小对影响非常大，设$E_1&gt;E_2,P(E_2)&gt;&gt;P(E_1)$，即，当温度趋近于0时，系统几乎不可能处于高能量状态</li></ul><h4 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h4><ul><li>首先在高温下进行搜索，此时各状态出现概率相差不大，可以很快进入“热平衡状态”，这时进行的是一种“粗搜索”，也就是大<u>致找到系统的低能区</u></li><li>随着温度的逐渐降低，各状态出现概率的差距逐渐被扩大，搜索精度不断提高。这就可以<u>越来越准确地找到网络能量函数的全局最小点</u></li></ul><h4 id="模拟退火与传统迭代最优算法的比较"><a href="#模拟退火与传统迭代最优算法的比较" class="headerlink" title="模拟退火与传统迭代最优算法的比较"></a>模拟退火与传统迭代最优算法的比较</h4><p>（1）当系统在非零温度下时，从局部最优中跳出是非常可能的，因此不会陷入局部最优。<br>（2）系统最终状态的总特征可以在较高温度下看到，而状态的好的细节却在低温下表现，因此，模拟退火是自适应的。</p><h4 id="模拟退火原理"><a href="#模拟退火原理" class="headerlink" title="模拟退火原理"></a>模拟退火原理</h4><p>1.Metropolis抽样过程</p><ul><li>ΔE表示系统从状态vi转移至状态vj所引起的能量差。如果能量差ΔE为负，这种转移就导致状态能量的降低，这种转移就被接受。接下来，新状态作为算法下一步的起始点。</li><li>若能量差为正，算法在这一点进行概率操作。首先，选定一个在[0,1]内服从均匀分布的随机数ξ。如果$ξ&lt;e^{-ΔE/T}$，则接受这种转移，否则，拒绝这种转移；即在算法的下一步中拒绝旧的状态。如此反复，达到系统在此温度下的热平衡。</li></ul><p>2.退火过程（降温过程）</p><ul><li>Metropolis抽样过程中温度$T$缓慢地降低。</li><li>初始温度值：初始温度值$T_0$要选得足够高，保证模拟退火算法中所有可能的转移都能被接受。</li><li>温度的下降：原先使用指数函数实现温度的下降。但是这种方法使降温幅度过小，从而延长搜索时间。在实际中，通常使用下式：$ T_{k}= \lambda T_{k-1},k=1,2, \cdots $ </li><li>终止温度：如果在连续的若干个温度下没有可接受的新状态，系统冻结或退火停止。</li></ul><h3 id="模拟退火算法用于组合优化问题"><a href="#模拟退火算法用于组合优化问题" class="headerlink" title="模拟退火算法用于组合优化问题"></a>模拟退火算法用于组合优化问题</h3><p>第一步：初始化。依据所要解决的组合优化问题，确定代价函数𝐶(﹒)的表达式，随机选择初始状态𝑉=𝑉(0)，设定初始温度$T_0$，终止温度$T_{final}$，概率阈值ξ。</p><p>第二步：Metropolis抽样过程<br>（1）在温度𝑇下依据某一规定的方式，根据当前解所处的状态𝑉，产生一个近邻子集𝑁(𝑉)（可包括𝑉，也可不包括𝑉），在𝑁(𝑉)内随机寻找一个新状态𝑆’作为下一个当前解的候选解，计算Δ𝐶’=𝐶(𝑉’)−𝐶(𝑉)。</p><p>（2）若Δ𝐶’&lt;0,则𝑉=𝑉’，作为下一状态；若Δ𝐶’&gt;0，则计算概率$ e^{- \Delta C^{ \prime }/T} $ ，若其大于给定概率阈值ξ，则取下一状态为𝑉=𝑉’，否则，保留这一状态。<br>（3）<u>按某一给定的收敛算法检查算法在温度T下是否应停止，若符合收敛条件则表示已达到热平衡，转向第三步的退火过程，若不符合收敛条件，则转向（1）继续迭代，直至在此温度下收敛。</u><!--0,则𝑉=𝑉’，作为下一状态；若Δ𝐶’--></p><p>第三步：退火过程。<br>按照一定的降温方法得到一个新的温度𝑇，检查𝑇是否小于给定的温度终止阈值$T_{final}$。若小于，则退火过程结束，当前状态𝑉即为算法最终输出解。若温度𝑇大于等于给定阈值，则转至Metropolis抽样过程，在新的温度下搜索状态。</p><h3 id="玻尔兹曼机"><a href="#玻尔兹曼机" class="headerlink" title="玻尔兹曼机"></a>玻尔兹曼机</h3><h4 id="随机神经网络与其他网络的比较"><a href="#随机神经网络与其他网络的比较" class="headerlink" title="随机神经网络与其他网络的比较"></a>随机神经网络与其他网络的比较</h4><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/8-1.png" alt="8-1" style="zoom: 67%;"></p><ul><li>BP网络是一种“贪心”算法，容易陷入局部最小点。</li><li>Hopfield网络很难避免出现伪状态，网络是严格按照能量减小的方向运行的，容易陷入局部极小点，而无法跳出。</li><li>所以，在用BP网络和Hopfield网络进行最优化的计算时，由于限定条件的不足，往往会使网络稳定在误差或能量函数的局部最小点，而不是全局最小点，即所得的解不是最优解。</li></ul><h4 id="随机神经网络的基本思想"><a href="#随机神经网络的基本思想" class="headerlink" title="随机神经网络的基本思想"></a>随机神经网络的基本思想</h4><font color="red">网络向误差或能量函数减小方向运行的概率大，同时向误差或能量函数增大方向运行的概率存在，这样网络跳出局部极小点的可能性存在，而且向全局最小点收敛的概率最大。</font><h4 id="Boltzmann机的网络结构"><a href="#Boltzmann机的网络结构" class="headerlink" title="Boltzmann机的网络结构"></a>Boltzmann机的网络结构</h4><ul><li>Boltzmann机由输入部、输出部和中间部构成。输入神经元和输出神经元可称为显见神经元，它们是网络与外部环境进行信息交换的媒介。中间部的神经元称为隐见神经元，它们通过显见神经元与外部进行信息交换。</li><li>每一对神经元之间的信息传递是双向对称的，即$w_{ij}=w_{ji}$，而且自身无反馈即$w_{ii}=0$。学习期间，显见神经元将被外部环境“约束”在某一特定的状态，而中间部隐见神经元则不受外部环境约束。</li></ul><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/8-2.png" alt="8-2" style="zoom:67%;"></p><ul><li><p><strong>单个神经元的运行特性</strong></p><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/8-3.png" alt="8-3" style="zoom:50%;"></p><ul><li>神经元$i$的全部输入信号的总和为$u_i$为：$ u_{i}= \sum _{j}^{n}w_{ij}v_{j}+b_{i} $ </li><li>神经元的输出$v_i$依概率取1或0：<ul><li>$v_i$取1的概率：$ P( v _{i}=1)=1/(1+e^{-u_{i}/T}) $ </li><li>$v_i$取0的概率：$ P( v _{i}=0)=1-P( v _{i}=1)$</li></ul></li><li>由此可见，$v_i$取1的概率受两个因素的影响：<br>(1) $u_i$越大$v_i$则取1的概率越大，而取0的概率越小；<br>(2)参数$T$称为“温度”，在不同的温度下$v_i$取1的概率$P$随$u_i$的变化如图所示。<ul><li>可见，$T$越高时，曲线越平滑，因此，即使$u_i$有很大变动，也不会对$v_i$取1的概率变化造成很大的影响；反之，$T$越低时，曲线越陡峭，当$u_i$有稍许变动时就会使概率有很大差异。<u>即温度高时状态变化接近随机，随着温度的降低向确定性的动作靠近。</u></li><li>当$T$→0时，每个神经元不再具有随机特性，而具有确定的特性，<u>激励函数变为阶跃函数，这时Boltzmann机趋向于Hopfield网络。</u></li></ul></li></ul></li></ul><p><img src="/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/8-4.png" alt="8-4" style="zoom:50%;"></p><h4 id="Boltzmann机的工作原理"><a href="#Boltzmann机的工作原理" class="headerlink" title="Boltzmann机的工作原理"></a>Boltzmann机的工作原理</h4><ul><li>Boltzmann机采用下式所示的能量函数作为描述其状态的函数: $ E=- \frac{1}{2} \sum _{i,j}w_{ij}v_{i} \nu _{j} $ </li><li>Boltzmann机在运行时，假设每次只改变一个神经元的状态，如第$i$个神经元，设$v_i$取0和取1时系统的能量函数分别为0和$ - \sum _{j}w_{ij}v_{j} $ ，它们的差值为$\Delta E_i$。$ \Delta E_{i}=E|_{v_{i}=0}-E|_{v_{i}=1}=0-(- \sum _{j}w_{ij} \nu _{j})= \sum _{j}w_{ij} \nu _{j} $ </li><li>$\Delta E_i$的取值可能有两种情况：$\Delta E_i$&gt;0或$\Delta E_i$&lt;0<ul><li>当$\Delta E_i&gt;0$时，即$ u_{i}= \sum _{j}w_{ij}v_{j}&gt;0 $ 时，$E|_{v_{i}=0}&gt;E|_{v_{i}=1}$<ul><li>神经元取1的概率：$ P( v _{i}=1)=1/(1+e^{-u_{i}/T}) $ </li><li>神经元取0的概率：$P( v_{i}=0)=e^{-u_{i}/T}/(1+e^{-u_{i}/T})$</li><li>$u_i&gt;0$时，$ e^{-u_{i}/T}&lt;1 $ ，$ p( v _{i}="1)"&gt;P( v _{i}=0) $ <!--1--></li><li>这时神经元$i$的状态取1的可能性比取0的可能性大，即网络状态取能量低的可能性大。</li></ul></li><li>当$\Delta E_i&lt;0$时，即$ u_{i}= \sum _{j}w_{ij}v_{j}&lt;0 $ 时，$E|_{v_{i}=0}&lt;E|_{v_{i}=1}$<ul><li>此时，$ P( v _{i}=1)&lt;P( v _{i}=0) $ </li><li>神经元$i$的状态取0的可能性比取1的可能性大，即网络状态取能量低的可能性大。</li></ul></li></ul></li><li><font color="red">网络状态取能量低的可能性大。运行过程中总的趋势是朝能量下降的方向运动，但也存在能量上升的可能性。</font></li></ul><h4 id="Boltzmann机的运行步骤"><a href="#Boltzmann机的运行步骤" class="headerlink" title="Boltzmann机的运行步骤"></a>Boltzmann机的运行步骤</h4><ul><li>第一步：对网络进行初始化。设定初始温度$T_0$、终止温度$T_{final}$和阈值𝜉，以及网络各神经元的连接权值$w_{ij}$。</li><li>第二步：在温度$T_m$条件下（初始温度为$T_0$）随机选取网络中的一个神经元$i$，计算神经元$i$的输入信号总和$u_i$：$ u_{i}= \sum _{j=1,i\neq j}^{n}w_{ij}v_{j}+b_{i} $ </li><li>第三步：若$u_i$&gt;0，即能量差$Δ𝐸_𝑖$&gt;0，取$v_i=1$为神经元$i$的下一状态值。若$u_i$&lt;0，计算概率：$ P_{i}=1/(1+e^{-u_{i}/T}) $ </li><li>第四步：判断网络在温度$T_m$是否达到稳定，若未达到稳定，则继续在网络中随机选取另一神经元$j$，令$j=i$，转至第二步重复计算，直至网络在$T_m$下达到稳定。若网络在$T_m$下已达到稳定则转至第五步计算。</li><li>第五步：以一定规律降低温度，使$T_{m+1}&lt;T_m$ ，判断$T_{m+1}$是否小于$T_{final}$，若$T_{m+1}$大于等于$T_{final}$，则$T_m=T_{m+1}$，转至第二步重复计算；若$T_{m+1}$小于$T_{final}$，则运行结束。此时在$T_m$下所求得的网络稳定状态，即为网络的输出。</li></ul><h4 id="Boltzmann机的学习规则"><a href="#Boltzmann机的学习规则" class="headerlink" title="Boltzmann机的学习规则"></a>Boltzmann机的学习规则</h4><ul><li>Boltzmann机是一种随机神经网络，可使用概率中的似然函数量度其模拟外界环境概率分布的性能。因此，Boltzmann机的学习规则就是根据最大似然规则，通过调整权值$w_{ij}$，最小化似然函数或其对数</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/">复习笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</category>
      
      
      <comments>https://rubychen0611.github.io/2020/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【算法复习】数据结构</title>
      <link>https://rubychen0611.github.io/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</link>
      <guid>https://rubychen0611.github.io/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</guid>
      <pubDate>Tue, 22 Dec 2020 07:37:41 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;基本数据结构&quot;&gt;&lt;a href=&quot;#基本数据结构&quot; class=&quot;headerlink&quot; title=&quot;基本数据结构&quot;&gt;&lt;/a&gt;基本数据结构&lt;/h2&gt;&lt;h3 id=&quot;栈和队列&quot;&gt;&lt;a href=&quot;#栈和队列&quot; class=&quot;headerlink&quot; title=&quot;栈和队列&quot;&gt;&lt;/a&gt;栈和队列&lt;/h3&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="基本数据结构"><a href="#基本数据结构" class="headerlink" title="基本数据结构"></a>基本数据结构</h2><h3 id="栈和队列"><a href="#栈和队列" class="headerlink" title="栈和队列"></a>栈和队列<a id="more"></a></h3><h4 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h4><ul><li><p>后进先出</p></li><li><p>S.top指向最新插入的元素</p><ul><li>S.top=0：栈为空</li></ul></li></ul><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/栈1.png" alt="栈1"></p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/栈2.png" alt="栈2"></p><h4 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h4><ul><li><p>先进先出</p></li><li><p>Q.head指向队头元素，Q.tail指向下一个新元素将插入的位置</p></li><li><p>Q.head=Q.tail时：队列为空</p></li><li><p>初始时：Q.head=Q.tail=1</p></li><li><p>Q.head=Q.tail+1时：队满</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/队列.png" alt="队列"></p></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LeetCode 622：实现循环队列</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCircularQueue</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; Q;</span><br><span class="line">    <span class="keyword">int</span> head, tail;</span><br><span class="line">    <span class="keyword">int</span> <span class="built_in">size</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    MyCircularQueue(<span class="keyword">int</span> k) {</span><br><span class="line">        <span class="built_in">size</span> = k + <span class="number">1</span>;</span><br><span class="line">        Q.resize(<span class="built_in">size</span>);</span><br><span class="line">        head = <span class="number">0</span>;</span><br><span class="line">        tail = <span class="number">0</span>;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">enQueue</span><span class="params">(<span class="keyword">int</span> value)</span> </span>{</span><br><span class="line">        <span class="keyword">if</span> (isFull())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        Q[tail] = value;</span><br><span class="line">        tail = (tail + <span class="number">1</span>) % <span class="built_in">size</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">deQueue</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">if</span> (isEmpty())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        head = (head + <span class="number">1</span>) % <span class="built_in">size</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Front</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(isEmpty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">return</span> Q[head];</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Rear</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(isEmpty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">return</span> Q[(tail - <span class="number">1</span> + <span class="built_in">size</span>) % <span class="built_in">size</span>];</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isEmpty</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">return</span> head == tail;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isFull</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">return</span> (tail + <span class="number">1</span>) % <span class="built_in">size</span> == head;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LeetCode 641：实现循环双端队列</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCircularDeque</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; D;</span><br><span class="line">    <span class="keyword">int</span> <span class="built_in">size</span>, head, tail;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/** Initialize your data structure here. Set the size of the deque to be k. */</span></span><br><span class="line">    MyCircularDeque(<span class="keyword">int</span> k) {</span><br><span class="line">        <span class="built_in">size</span> = k + <span class="number">2</span>;</span><br><span class="line">        head = <span class="number">0</span>;</span><br><span class="line">        tail = <span class="number">1</span>;</span><br><span class="line">        D.resize(<span class="built_in">size</span>);</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Adds an item at the front of Deque. Return true if the operation is successful. */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">insertFront</span><span class="params">(<span class="keyword">int</span> value)</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(isFull())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        D[head] = value;</span><br><span class="line">        head = (head - <span class="number">1</span> + <span class="built_in">size</span>) % <span class="built_in">size</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Adds an item at the rear of Deque. Return true if the operation is successful. */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">insertLast</span><span class="params">(<span class="keyword">int</span> value)</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(isFull())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        D[tail] = value;</span><br><span class="line">        tail = (tail + <span class="number">1</span>) % <span class="built_in">size</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Deletes an item from the front of Deque. Return true if the operation is successful. */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">deleteFront</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(isEmpty())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        head = (head + <span class="number">1</span>) % <span class="built_in">size</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Deletes an item from the rear of Deque. Return true if the operation is successful. */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">deleteLast</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(isEmpty())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        tail = (tail - <span class="number">1</span> + <span class="built_in">size</span>) % <span class="built_in">size</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Get the front item from the deque. */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getFront</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">if</span> (isEmpty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">return</span> D[(head + <span class="number">1</span>) % <span class="built_in">size</span>];</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Get the last item from the deque. */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getRear</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(isEmpty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">return</span> D[(tail - <span class="number">1</span> + <span class="built_in">size</span>) % <span class="built_in">size</span>];</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Checks whether the circular deque is empty or not. */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isEmpty</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">return</span> (head + <span class="number">1</span>) % <span class="built_in">size</span> == tail;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Checks whether the circular deque is full or not. */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isFull</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">return</span> (tail + <span class="number">1</span>) % <span class="built_in">size</span> == head;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h3><ul><li><p>搜索：$O(n)$</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/链表搜索.png" alt="链表搜索"></p></li><li><p>插入：$O(1)$</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/链表插入.png" alt="链表插入"></p></li><li><p>删除：$O(1)$</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/链表删除.png" alt="链表删除"></p></li><li><p>哨兵</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/哨兵.png" alt="哨兵" style="zoom:80%;"></p></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LeetCode 707. 设计链表</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">int</span> val;</span><br><span class="line">    Node* prev, *next;</span><br><span class="line">    Node(<span class="keyword">int</span> x): val(x), prev(<span class="literal">NULL</span>), next(<span class="literal">NULL</span>) {}</span><br><span class="line">};</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLinkedList</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Node* dummy;    <span class="comment">// 哨兵</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/** Initialize your data structure here. */</span></span><br><span class="line">    MyLinkedList()</span><br><span class="line">    {</span><br><span class="line">        dummy = <span class="keyword">new</span> Node(<span class="number">-1</span>);</span><br><span class="line">        dummy-&gt;prev = dummy;    <span class="comment">// dummy-&gt;prev指向表尾</span></span><br><span class="line">        dummy-&gt;next = dummy;    <span class="comment">// dummy-&gt;next指向表头</span></span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Get the value of the index-th node in the linked list. If the index is invalid, return -1. */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">get</span><span class="params">(<span class="keyword">int</span> index)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        Node* p = dummy -&gt; next;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index; i ++)</span><br><span class="line">        {</span><br><span class="line">            p = p -&gt; next;</span><br><span class="line">            <span class="keyword">if</span>(p == dummy)</span><br><span class="line">                <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> p -&gt; val;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Add a node of value val before the first element of the linked list. After the insertion, the new node will be the first node of the linked list. */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">addAtHead</span><span class="params">(<span class="keyword">int</span> val)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        Node* n = <span class="keyword">new</span> Node(val);</span><br><span class="line">        n -&gt; next = dummy -&gt; next;</span><br><span class="line">        n -&gt; prev = dummy;</span><br><span class="line">        dummy -&gt; next -&gt; prev = n;</span><br><span class="line">        dummy -&gt; next = n;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Append a node of value val to the last element of the linked list. */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">addAtTail</span><span class="params">(<span class="keyword">int</span> val)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        Node* n = <span class="keyword">new</span> Node(val);</span><br><span class="line">        n -&gt; next = dummy;</span><br><span class="line">        n -&gt; prev = dummy -&gt; prev;</span><br><span class="line">        dummy -&gt; prev -&gt; next = n;</span><br><span class="line">        dummy -&gt; prev = n;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Add a node of value val before the index-th node in the linked list. If index equals to the length of linked list, the node will be appended to the end of linked list. If index is greater than the length, the node will not be inserted. */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">addAtIndex</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">int</span> val)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span> (index &lt;= <span class="number">0</span>)</span><br><span class="line">        {</span><br><span class="line">            addAtHead(val);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        }</span><br><span class="line">        Node* p = dummy -&gt; next;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index; i ++)</span><br><span class="line">        {</span><br><span class="line">            p = p -&gt; next;</span><br><span class="line">            <span class="keyword">if</span>(p == dummy)</span><br><span class="line">            {</span><br><span class="line">                <span class="keyword">if</span> (i + <span class="number">1</span> == index) {</span><br><span class="line">                    addAtTail(val);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                }</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">return</span>;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        Node* n = <span class="keyword">new</span> Node(val);</span><br><span class="line">        n -&gt; next = p;</span><br><span class="line">        n -&gt; prev = p -&gt; prev;</span><br><span class="line">        p -&gt; prev -&gt; next = n;</span><br><span class="line">        p -&gt; prev = n;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Delete the index-th node in the linked list, if the index is valid. */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">deleteAtIndex</span><span class="params">(<span class="keyword">int</span> index)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        Node* p = dummy -&gt; next;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index; i ++)</span><br><span class="line">        {</span><br><span class="line">            p = p -&gt; next;</span><br><span class="line">            <span class="keyword">if</span>(p == dummy)</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">        }</span><br><span class="line">        p -&gt; prev -&gt; next = p -&gt; next;</span><br><span class="line">        p -&gt; next -&gt; prev = p -&gt; prev;</span><br><span class="line">        <span class="keyword">delete</span> p;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//LeetCode 206 反转链表：</span></span><br><span class="line"><span class="comment">// 递归写法</span></span><br><span class="line"><span class="function">ListNode* <span class="title">reverseList</span><span class="params">(ListNode* head)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span>(head == <span class="literal">NULL</span> || head-&gt; next == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    ListNode* plast = reverseList(head -&gt; next);</span><br><span class="line">    ListNode* p = plast; </span><br><span class="line">    head-&gt; next -&gt;next = head;</span><br><span class="line">    head -&gt; next = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">return</span> plast;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 非递归写法</span></span><br><span class="line"><span class="function">ListNode* <span class="title">reverseList</span><span class="params">(ListNode* head)</span> </span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    ListNode* prev = <span class="literal">NULL</span>, *curr = head, *next;</span><br><span class="line">    <span class="keyword">while</span>(curr != <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        next = curr-&gt;next;</span><br><span class="line">        curr-&gt;next = prev;</span><br><span class="line">        prev = curr;</span><br><span class="line">        curr = next;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> prev;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="有根树"><a href="#有根树" class="headerlink" title="有根树"></a>有根树</h3><ul><li>二叉树</li><li>多叉树<ul><li>左孩子右兄弟表示法</li></ul></li></ul><h2 id="散列表"><a href="#散列表" class="headerlink" title="散列表"></a>散列表</h2><ul><li>实现<strong>字典</strong>操作的一种有效数据结构</li><li>关键字为$k$的元素放置在槽$h(k)$中</li><li>装载因子：对能存放$n$个元素，$m$个槽位的散列表$T$，$T$的装载因子定义为$\alpha=n/m$</li></ul><h3 id="冲突解决方法1：链接法"><a href="#冲突解决方法1：链接法" class="headerlink" title="冲突解决方法1：链接法"></a>冲突解决方法1：链接法</h3><ul><li><p>把散列到同一个槽中的所有元素都放在一个链表中</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/散列表1.png" alt="散列表1" style="zoom:70%;"></p></li><li><p>插入操作最坏运行时间：$O(1)$</p></li><li><p>删除操作：$O(1)$（直接删除，双向链表）</p></li><li><p>查找操作：最坏$O(n)$（所有关键字映射到同一个槽）</p><ul><li>平均性能依赖于散列函数</li><li>简单均匀散列<ul><li>一次成功/不成功查找的平均时间都为$\Theta(1+\alpha)$</li><li>则所有操作都能在$O(1)$时间内完成</li></ul></li></ul></li><li><p>散列函数</p><ul><li>除法散列法</li><li>乘法散列法</li><li>全域散列法</li></ul></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LeetCode 705: 设计哈希集合</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;list&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyHashSet</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt;&gt; hash_table;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> <span class="built_in">size</span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">hash_function</span><span class="params">(<span class="keyword">int</span> key)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> key % <span class="built_in">size</span>;</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/** Initialize your data structure here. */</span></span><br><span class="line">    MyHashSet():<span class="built_in">size</span>(<span class="number">100</span>){</span><br><span class="line">        hash_table.resize(<span class="built_in">size</span>);    <span class="comment">// 注意resize和reserve区别</span></span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> key)</span> </span>{</span><br><span class="line">        <span class="keyword">if</span> (!contains(key))</span><br><span class="line">            hash_table[hash_function(key)].push_back(key);</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">remove</span><span class="params">(<span class="keyword">int</span> key)</span> </span>{</span><br><span class="line">        <span class="keyword">int</span> hash_idx = hash_function(key);</span><br><span class="line">        <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt; &amp;l = hash_table[hash_idx];</span><br><span class="line">        <span class="keyword">if</span> (l.empty())</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt; ::iterator it = <span class="built_in">find</span>(l.<span class="built_in">begin</span>(), l.<span class="built_in">end</span>(), key);</span><br><span class="line">        <span class="keyword">if</span>( it != l.<span class="built_in">end</span>())</span><br><span class="line">        {</span><br><span class="line">            l.erase(it);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Returns true if this set contains the specified element */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">contains</span><span class="params">(<span class="keyword">int</span> key)</span> </span>{</span><br><span class="line">        <span class="keyword">int</span> hash_idx = hash_function(key);</span><br><span class="line">        <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt; &amp;l = hash_table[hash_idx];</span><br><span class="line">        <span class="keyword">if</span> (l.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="built_in">list</span>&lt;<span class="keyword">int</span>&gt; ::iterator it = <span class="built_in">find</span>(l.<span class="built_in">begin</span>(), l.<span class="built_in">end</span>(), key); <span class="comment">//list没有find</span></span><br><span class="line">        <span class="keyword">return</span> it != l.<span class="built_in">end</span>();</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LeetCode 706: 设计哈希映射</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;list&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="comment">//leetcode submit region begin(Prohibit modification and deletion)</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Pair</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">int</span> key, value;</span><br><span class="line">    Pair(<span class="keyword">int</span> k, <span class="keyword">int</span> v): key(k), value(v){}</span><br><span class="line">};</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyHashMap</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">list</span>&lt;Pair&gt;&gt; hash_table;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> <span class="built_in">size</span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">hash_function</span><span class="params">(<span class="keyword">int</span> key)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> key % <span class="built_in">size</span>;</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/** Initialize your data structure here. */</span></span><br><span class="line">    MyHashMap():<span class="built_in">size</span>(<span class="number">100</span>){</span><br><span class="line">        hash_table.resize(<span class="built_in">size</span>);</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** value will always be non-negative. */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(<span class="keyword">int</span> key, <span class="keyword">int</span> value)</span> </span>{</span><br><span class="line">        <span class="keyword">int</span> hash_idx = hash_function(key);</span><br><span class="line">        <span class="built_in">list</span>&lt;Pair&gt;&amp; l = hash_table[hash_idx];</span><br><span class="line">        <span class="keyword">if</span>(l.empty()) {</span><br><span class="line">            l.push_back(Pair(key, value));</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">for</span>(<span class="built_in">list</span>&lt;Pair&gt;::iterator it = l.<span class="built_in">begin</span>(); it != l.<span class="built_in">end</span>(); it++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (it -&gt; key == key)</span><br><span class="line">            {</span><br><span class="line">                it -&gt; value = value;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        l.push_back(Pair(key, value));</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Returns the value to which the specified key is mapped, or -1 if this map contains no mapping for the key */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">get</span><span class="params">(<span class="keyword">int</span> key)</span> </span>{</span><br><span class="line">        <span class="keyword">int</span> hash_idx = hash_function(key);</span><br><span class="line">        <span class="built_in">list</span>&lt;Pair&gt;&amp; l = hash_table[hash_idx];</span><br><span class="line">        <span class="keyword">if</span>(l.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="built_in">list</span>&lt;Pair&gt;::iterator it = l.<span class="built_in">begin</span>(); it != l.<span class="built_in">end</span>(); it++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (it -&gt; key == key)</span><br><span class="line">            {</span><br><span class="line">                <span class="keyword">return</span> it -&gt; value;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Removes the mapping of the specified value key if this map contains a mapping for the key */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">remove</span><span class="params">(<span class="keyword">int</span> key)</span> </span>{</span><br><span class="line">        <span class="keyword">int</span> hash_idx = hash_function(key);</span><br><span class="line">        <span class="built_in">list</span>&lt;Pair&gt;&amp; l = hash_table[hash_idx];</span><br><span class="line">        <span class="keyword">if</span>(l.empty())</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="built_in">list</span>&lt;Pair&gt;::iterator it = l.<span class="built_in">begin</span>(); it != l.<span class="built_in">end</span>(); it++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (it -&gt; key == key)</span><br><span class="line">            {</span><br><span class="line">                l.erase(it);</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="冲突解决方法2：开放寻址法"><a href="#冲突解决方法2：开放寻址法" class="headerlink" title="冲突解决方法2：开放寻址法"></a>冲突解决方法2：开放寻址法</h3><ul><li><p>散列表可能会被填满，装载因子$\alpha$不能超过1</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/散列表2.png" alt="散列表2" style="zoom:67%;"></p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/散列表3.png" alt="散列表3" style="zoom:67%;"></p></li><li><p>删除操作比较困难：用特定值DELETED代替NIL来标记</p><ul><li>一般需要删除时还是用链接法更好</li></ul></li><li><p>探查序列的方法</p><ul><li>必须保证探查序列是&lt;0,1,...,m-1&gt;的一个排列<!--0,1,...,m-1--></li><li>均匀散列假设：每个关键字的探查序列等可能地为&lt;0,1,...,m-1&gt;的m!种排列中的任一种<ul><li>插入：至多$1/(1-\alpha)$次探查</li><li>不成功查找：至多$1/(1-\alpha)$次探查</li><li>成功查找：$\frac { 1 } { \alpha } \ln \frac { 1 } { 1 - \alpha }$</li></ul><!--0,1,...,m-1--></li><li>三种探查技术<ul><li>线性探查：$h ( k , i ) = ( h ^ { \prime } ( k ) + i ) \bmod m, i = 0,1 ,\cdots m - 1$<ul><li>m种不同的探查序列</li></ul></li><li>二次探查：$h ( k , i ) = ( h ^ { \prime } ( k ) + c _ { 1 } i + c _ { 2 } i ^ { 2 } ) \bmod m$</li><li>双重探查：$h ( k , i ) = ( h _ { 1 } ( k ) + i h _ { 2 } ( k ) ) \bmod m$</li></ul></li></ul></li></ul><h3 id="完全散列"><a href="#完全散列" class="headerlink" title="完全散列"></a>完全散列</h3><ul><li><p>最坏情况$O(1)$</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/散列表4.png" alt="散列表4" style="zoom:60%;"></p><ul><li>二级散列表，需要确保第二级散列表不发生冲突（略）</li></ul></li></ul><h2 id="二叉搜索树"><a href="#二叉搜索树" class="headerlink" title="二叉搜索树"></a>二叉搜索树</h2><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/二叉树6.png" alt="二叉树6" style="zoom:67%;"></p><ul><li><p>设 x 是二叉搜索树中的一个结点。</p><ul><li>如果 y 是 x 左子树中的一个结点，那么 y.key ≤x.key 。</li><li>如果 y 是 x 右子树中的一个结点，那么 y.key ≥ x.key 。</li></ul></li><li><p>遍历二叉树（$\Theta (n)$）</p><ul><li><p>中序遍历</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/二叉树1.png" alt="二叉树1" style="zoom: 67%;"></p></li><li><p>先序遍历</p></li><li><p>后序遍历</p></li></ul></li></ul><h3 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h3><ul><li><p>查询（O(h)）</p><ul><li>递归写法：</li></ul><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/二叉树2.png" alt="二叉树2" style="zoom:67%;"></p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 700 二叉搜索树中的搜索</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">searchBST</span><span class="params">(TreeNode* root, <span class="keyword">int</span> val)</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(!root)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        <span class="keyword">if</span>(root -&gt; val == val)</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        <span class="keyword">if</span>(root -&gt; val &gt; val)</span><br><span class="line">            <span class="keyword">return</span> searchBST(root -&gt; left, val);</span><br><span class="line">        <span class="keyword">return</span> searchBST(root -&gt; right, val);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure></li></ul><ul><li><p>迭代写法：</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/二叉树3.png" alt="二叉树3" style="zoom:67%;"></p></li></ul><ul><li><p>最大、最小元素（O(h)）</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/二叉树4.png" alt="二叉树4" style="zoom:67%;"></p></li></ul><ul><li><p>前驱和后继（O(h)）</p><ul><li><font color="red">分两种情况：</font><ul><li><p>若结点x的右子树非空，那么x的后继是x右子树中的最左节点</p></li><li><p><u>若结点x的右子树为空，向上遍历直到遇到一个祖先节点，它是其父结点的左孩子，返回父结点的值</u></p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/二叉树5.png" alt="二叉树5" style="zoom:67%;"></p></li></ul></li></ul></li></ul><h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><ul><li>先搜索，直到nil，然后将nil替换为新节点（$O(h)$）</li></ul><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/二叉树7.png" alt="二叉树7" style="zoom:67%;"></p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// leetcode 701：二叉搜索树中的插入操作</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(TreeNode* &amp;root, <span class="keyword">int</span> val)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="literal">nullptr</span>)</span><br><span class="line">        {    </span><br><span class="line">            root = <span class="keyword">new</span> TreeNode(val);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span>(root -&gt; val &gt; val)</span><br><span class="line">            insert(root -&gt; left, val);</span><br><span class="line">        insert(root -&gt; right, val);</span><br><span class="line">            </span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">insertIntoBST</span><span class="params">(TreeNode* root, <span class="keyword">int</span> val)</span> </span>{</span><br><span class="line">        insert(root, val);</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><ul><li><p>四种情况</p><ul><li>z没有孩子结点：直接删除</li><li>z只有一个孩子：将孩子提升到z的位置</li><li><font color="red">z有两个孩子</font>：查找z的后继y（右子树种寻找中序下的第一个节点）<ul><li>y就是z的右孩子：y直接替换z</li><li>y位于z的右子树但不是z的右孩子：先用y的右孩子替换y，然后用y替换z</li></ul></li></ul><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/二叉树8.png" alt="二叉树8" style="zoom:80%;"></p></li><li><p>实现</p><ul><li><p>用子树v替换u</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/二叉树9.png" alt="二叉树9" style="zoom:67%;"></p></li><li><p>删除($O(h)$)</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/二叉树删除.png" alt="二叉树删除" style="zoom:67%;"></p></li></ul></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//leetcode 450: 删除二叉搜索树中的节点</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">deleteBSTNode</span><span class="params">(TreeNode* &amp;root, <span class="keyword">int</span> key)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span>(!root)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">if</span>(root -&gt; val &gt; key)</span><br><span class="line">            deleteBSTNode(root -&gt; left, key);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (root -&gt; val &lt; key)</span><br><span class="line">            deleteBSTNode(root -&gt; right, key);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (root -&gt; left &amp;&amp; root -&gt; right)</span><br><span class="line">        {</span><br><span class="line">            TreeNode* tmp = root -&gt; right;  <span class="comment">//寻找右子树的最左节点</span></span><br><span class="line">            <span class="keyword">while</span>(tmp -&gt; left != <span class="literal">nullptr</span>)</span><br><span class="line">                tmp = tmp -&gt; left;</span><br><span class="line">            root -&gt; val = tmp -&gt; val;</span><br><span class="line">            deleteBSTNode(root -&gt; right, tmp -&gt; val);   <span class="comment">//在右子树中删除tmp</span></span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">else</span>{</span><br><span class="line">            TreeNode* tmp = root;</span><br><span class="line">            <span class="keyword">if</span>(root -&gt; left)</span><br><span class="line">                root = root -&gt; left;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(root -&gt; right)</span><br><span class="line">                root = root -&gt; right;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                root = <span class="literal">nullptr</span>;</span><br><span class="line">            <span class="keyword">delete</span> tmp;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">deleteNode</span><span class="params">(TreeNode* root, <span class="keyword">int</span> key)</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(!root)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        deleteBSTNode(root, key);</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">            </span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="随机构建二叉搜索树"><a href="#随机构建二叉搜索树" class="headerlink" title="随机构建二叉搜索树"></a>随机构建二叉搜索树</h3><ul><li>如果n个关键字按严格递增的顺序被插入，则这棵树一定是高度为n-1的一条链</li><li>随机顺序插入：性能较好</li></ul><h2 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h2><ul><li><p>平衡搜索树中的一种，可以保证在最坏情况下基本动态集合操作的时间复杂度为$O(lgn)$</p></li><li><p>一棵红黑树是满足下面红黑性质的二叉搜索树 :<br>1 . 每个结点或是红色的 , 或是黑色的 .<br>2 . 根结点是黑色的 .<br>3 . 每个叶结点 ( NIL ) 是黑色的 .<br>4 . 如果一个结点是红色的 , 则它的两个子结点都是黑色的 .<br>5 . 对每个结点 , 从该结点到其所有后代叶结点的简单路径上 , 均包含相同数目的黑色结点 .</p></li></ul><h2 id="数据结构的扩张"><a href="#数据结构的扩张" class="headerlink" title="数据结构的扩张"></a>数据结构的扩张</h2><h3 id="顺序统计树"><a href="#顺序统计树" class="headerlink" title="顺序统计树"></a>顺序统计树</h3><ul><li><p>每个节点增加一个信息：x.size</p><ul><li>根为x的子树的结点数</li></ul></li><li><p>查找根为x、秩为i（第i小）的结点</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/顺序统计树1.png" alt="顺序统计树1" style="zoom:67%;"></p></li><li><p>确定一个元素的秩（P194）</p><ul><li>遍历祖先节点，遇到是右子树的情况，加上左边所有节点的个数</li></ul><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/顺序统计树2.png" alt="顺序统计树2" style="zoom:80%;"></p></li><li><p>维护子树规模</p><ul><li>插入时每个父结点size+1</li></ul></li></ul><h3 id="区间树"><a href="#区间树" class="headerlink" title="区间树"></a>区间树</h3><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/区间树1.png" alt="区间树1" style="zoom:67%;"></p><ul><li><p>搜索与区间i有重叠的区间：</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/区间树2.png" alt="区间树2" style="zoom:80%;"></p></li></ul><h2 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h2><p>维护一个不相交动态集的集合$S$，用一个代表来标识每个集合。</p><h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><ul><li>MAKE_SET(x)：建立一个新的集合，唯一成员是x</li><li>UNION(x,y)：合并包含x和y的两个集合</li><li>FIND-SET(x)：返回包含x的唯一集合的代表</li></ul><h3 id="链表表示"><a href="#链表表示" class="headerlink" title="链表表示"></a>链表表示</h3><ul><li>MAKE_SET和FIND_SET：O(1)</li><li>UNION：与合并的链表长度呈线性关系</li><li>启发式策略：总是把较短的表拼接到较长的表上<ul><li>总开销:$O(m+nlgn)$</li></ul></li></ul><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/MyBlog\source\_posts\【算法复习】图算法\并查集1.png" alt="并查集1" style="zoom:67%;"></p><h3 id="有根树表示"><a href="#有根树表示" class="headerlink" title="有根树表示"></a>有根树表示</h3><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/并查集2.png" alt="并查集2" style="zoom:67%;"></p><p><strong>启发式策略</strong></p><ul><li><p>按秩合并：对每个节点，维护一个秩，它表示该结点高度的一个上界，让具有较小的秩的根指向具有较大秩的根</p></li><li><p>路径压缩：FIND-SET时让查找路径中的每个结点直接指向根</p><p><img src="/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/并查集3.png" alt="并查集3" style="zoom:80%;"></p></li></ul><p>运行时间：$O(m\alpha(n))$,其中$\alpha(n) &lt; 4$</p><ul><li>m：总操作次数</li><li>n：结点数</li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 并查集的C++实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UFSet</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; parent;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    UFSet(<span class="keyword">int</span> n)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">this</span>-&gt; n = n;</span><br><span class="line">        parent.resize(n, <span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">while</span>(parent[i] &gt;= <span class="number">0</span>)    <span class="comment">// 注意是大于等于</span></span><br><span class="line">            i = parent[i];</span><br><span class="line">        <span class="keyword">return</span> i;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">collapsing_find</span><span class="params">(<span class="keyword">int</span> i)</span>  <span class="comment">// 路径压缩</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="comment">// 搜索根root</span></span><br><span class="line">        <span class="keyword">int</span> root = i;</span><br><span class="line">        <span class="keyword">while</span>(parent[root] &gt;= <span class="number">0</span>)</span><br><span class="line">            root = parent[root];</span><br><span class="line">        <span class="keyword">while</span>(i != root)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> tmp = parent[i];</span><br><span class="line">            parent[i] = root;</span><br><span class="line">            i = tmp;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set_union</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">int</span> root1 = <span class="built_in">find</span>(i), root2 = <span class="built_in">find</span>(j);</span><br><span class="line">        <span class="keyword">if</span>(root1 != root2)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> tmp = parent[root1] + parent[root2];</span><br><span class="line">            <span class="keyword">if</span>(parent[root1] &gt; parent[root2])</span><br><span class="line">            {</span><br><span class="line">                parent[root1] = root2;</span><br><span class="line">                parent[root2] = tmp;</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            {</span><br><span class="line">                parent[root2] = root1;</span><br><span class="line">                parent[root1] = tmp;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">union_root</span><span class="params">(<span class="keyword">int</span> root1, <span class="keyword">int</span> root2)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span>(root1 != root2)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> tmp = parent[root1] + parent[root2];</span><br><span class="line">            <span class="keyword">if</span>(parent[root1] &gt; parent[root2])</span><br><span class="line">            {</span><br><span class="line">                parent[root1] = root2;</span><br><span class="line">                parent[root2] = tmp;</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            {</span><br><span class="line">                parent[root2] = root1;</span><br><span class="line">                parent[root1] = tmp;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h2 id="Trie字典树"><a href="#Trie字典树" class="headerlink" title="Trie字典树"></a>Trie字典树</h2><p>由于一个英文单词的长度n 通常在10以内，如果我们使用字典树，则可以在O(n)时间内完成搜索，且额外开销非常小。</p><ul><li><p>插入</p><ul><li><p>时间复杂度：O(m)，其中 m 为键长。在算法的每次迭代中，我们要么检查要么创建一个节点，直到到达键尾。只需要 m 次操作。</p><p>空间复杂度：O(m)。最坏的情况下，新插入的键和 Trie 树中已有的键没有公共前缀。此时需要添加 m 个结点，使用 O(m) 空间。</p></li></ul></li><li><p>查找</p><ul><li>时间复杂度 : O(m)。算法的每一步均搜索下一个键字符。最坏的情况下需要 m 次操作。</li><li>空间复杂度 : O(1)。</li></ul></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LeetCode 208. 实现 Trie (前缀树)</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TrieNode</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    TrieNode* childNode[<span class="number">26</span>];</span><br><span class="line">    <span class="keyword">bool</span> isVal;        <span class="comment">// 此节点的前缀是否是一个单词</span></span><br><span class="line">    TrieNode():isVal(<span class="literal">false</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">26</span>; i++)</span><br><span class="line">            childNode[i] = <span class="literal">nullptr</span>;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">};</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trie</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    TrieNode* root;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/** Initialize your data structure here. */</span></span><br><span class="line">    Trie() {</span><br><span class="line">        root = <span class="keyword">new</span> TrieNode();</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Inserts a word into the trie. */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="built_in">string</span> <span class="keyword">word</span>)</span> </span>{</span><br><span class="line">        TrieNode* p = root;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">word</span>.length(); i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(!p -&gt; childNode[<span class="keyword">word</span>[i] - <span class="string">'a'</span>])</span><br><span class="line">                p -&gt; childNode[<span class="keyword">word</span>[i] - <span class="string">'a'</span>] = <span class="keyword">new</span> TrieNode();</span><br><span class="line">            p = p -&gt; childNode[<span class="keyword">word</span>[i] - <span class="string">'a'</span>];</span><br><span class="line">        }</span><br><span class="line">        p -&gt; isVal = <span class="literal">true</span>; <span class="comment">// 是一个词</span></span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Returns if the word is in the trie. */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">search</span><span class="params">(<span class="built_in">string</span> <span class="keyword">word</span>)</span> </span>{</span><br><span class="line">        TrieNode* p = root;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">word</span>.length(); i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(p == <span class="literal">nullptr</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            p = p -&gt; childNode[<span class="keyword">word</span>[i] - <span class="string">'a'</span>];</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span> (!p || !(p -&gt; isVal))</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Returns if there is any word in the trie that starts with the given prefix. */</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">startsWith</span><span class="params">(<span class="built_in">string</span> prefix)</span> </span>{</span><br><span class="line">        TrieNode* p = root;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; prefix.length(); i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(p == <span class="literal">nullptr</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            p = p -&gt; childNode[prefix[i] - <span class="string">'a'</span>];</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> p != <span class="literal">nullptr</span>;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h2 id="线段树"><a href="#线段树" class="headerlink" title="线段树"></a>线段树</h2><p>线段树是算法竞赛中常用的用来维护 <strong>区间信息</strong> 的数据结构。</p><p>线段树可以在$O(logN)$的时间复杂度内实现单点修改、区间修改、区间查询（区间求和，求区间最大值，求区间最小值）等操作。</p><p>建树 ：数组空间4N</p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LeetCode 307. 区域和检索 - 数组可修改</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NumArray</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tree;   <span class="comment">// 线段树</span></span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">buildTree</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> cur, <span class="keyword">int</span> start, <span class="keyword">int</span> <span class="built_in">end</span>)</span>       <span class="comment">// 建树：O(4N)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="comment">// [start, end] 为当第cur个节点包含的区间</span></span><br><span class="line">        <span class="keyword">if</span>(start == <span class="built_in">end</span>)    </span><br><span class="line">            tree[cur] = nums[start];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> mid = (start + <span class="built_in">end</span>) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> leftChild = <span class="number">2</span> * cur + <span class="number">1</span>, rightChild = <span class="number">2</span> * cur + <span class="number">2</span>;</span><br><span class="line">            buildTree(nums, leftChild, start, mid);</span><br><span class="line">            buildTree(nums, rightChild, mid + <span class="number">1</span>, <span class="built_in">end</span>);</span><br><span class="line">            tree[cur] = tree[leftChild] + tree[rightChild];</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">query</span><span class="params">(<span class="keyword">int</span> cur, <span class="keyword">int</span> start, <span class="keyword">int</span> <span class="built_in">end</span>, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span>    <span class="comment">// 查询区间[i, j]的和</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="comment">// [start, end] 为当前节点包含的区间（建树时已经固定）</span></span><br><span class="line">        <span class="keyword">if</span>(i &gt; <span class="built_in">end</span> || j &lt; start)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(i &lt;= start &amp;&amp; <span class="built_in">end</span> &lt;= j)  <span class="comment">// 当前区间为询问区间的子集时直接返回当前区间的和</span></span><br><span class="line">            <span class="keyword">return</span> tree[cur];</span><br><span class="line">        <span class="keyword">int</span> mid = (start + <span class="built_in">end</span>) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">int</span> leftChild = <span class="number">2</span> * cur + <span class="number">1</span>, rightChild = <span class="number">2</span> * cur + <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">int</span> left_sum = query(leftChild, start, mid, i, j); <span class="comment">// 左子结点包含的区间：[start, mid]</span></span><br><span class="line">        <span class="keyword">int</span> right_sum = query(rightChild, mid + <span class="number">1</span>, <span class="built_in">end</span>, i, j); <span class="comment">// 右子节点包含的区间[mid+1, end]</span></span><br><span class="line">        <span class="keyword">return</span> left_sum + right_sum;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">update_tree</span><span class="params">(<span class="keyword">int</span> cur, <span class="keyword">int</span> start, <span class="keyword">int</span> <span class="built_in">end</span>, <span class="keyword">int</span> i, <span class="keyword">int</span> val)</span> <span class="comment">// 更新第i个结点值为val</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span>(start == <span class="built_in">end</span>)</span><br><span class="line">            tree[cur] = val;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> mid = (start + <span class="built_in">end</span>) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> leftChild = <span class="number">2</span> * cur + <span class="number">1</span>, rightChild = <span class="number">2</span> * cur + <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(i &gt;= start &amp;&amp; i &lt;= mid) <span class="comment">// i在左子树中</span></span><br><span class="line">                update_tree(leftChild, start, mid, i, val);</span><br><span class="line">            <span class="keyword">else</span>  <span class="comment">// i在右子树中</span></span><br><span class="line">                update_tree(rightChild, mid+<span class="number">1</span>, <span class="built_in">end</span>, i, val);</span><br><span class="line">            tree[cur] = tree[leftChild] +tree[rightChild];</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    NumArray(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) {</span><br><span class="line">        <span class="keyword">if</span>(nums.empty())</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        n = nums.<span class="built_in">size</span>();</span><br><span class="line">        tree.resize(<span class="number">4</span> * n + <span class="number">1</span>);</span><br><span class="line">        buildTree(nums, <span class="number">0</span>, <span class="number">0</span>, n<span class="number">-1</span>);</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">update</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">int</span> val)</span> </span>{</span><br><span class="line">        update_tree(<span class="number">0</span>, <span class="number">0</span>, n<span class="number">-1</span>, index, val);</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">sumRange</span><span class="params">(<span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>{</span><br><span class="line">        <span class="keyword">return</span> query(<span class="number">0</span>, <span class="number">0</span>, n<span class="number">-1</span>, left, right);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E7%AE%97%E6%B3%95/">算法</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/">算法导论</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</category>
      
      
      <comments>https://rubychen0611.github.io/2020/12/22/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【算法复习】排序和顺序统计量</title>
      <link>https://rubychen0611.github.io/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/</link>
      <guid>https://rubychen0611.github.io/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/</guid>
      <pubDate>Tue, 15 Dec 2020 07:37:41 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;排序&quot;&gt;&lt;a href=&quot;#排序&quot; class=&quot;headerlink&quot; title=&quot;排序&quot;&gt;&lt;/a&gt;排序&lt;/h2&gt;&lt;h3 id=&quot;排序算法总结&quot;&gt;&lt;a href=&quot;#排序算法总结&quot; class=&quot;headerlink&quot; title=&quot;排序算法总结&quot;&gt;&lt;/a&gt;排序算法总结&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/小结.png&quot; alt=&quot;小结&quot; style=&quot;zoom: 80%;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;任何比较排序在最坏情况下都要经过$\Omega(nlgn)$次比较，因此归并排序和堆排序都是渐近最优的&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="排序算法总结"><a href="#排序算法总结" class="headerlink" title="排序算法总结"></a>排序算法总结</h3><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/小结.png" alt="小结" style="zoom: 80%;"></p><ul><li>任何比较排序在最坏情况下都要经过$\Omega(nlgn)$次比较，因此归并排序和堆排序都是渐近最优的</li></ul><a id="more"></a><h3 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h3><ul><li>把第j个数作为key插入到A[1…j-1]的有序数列中：<u>从后向前查找</u>，将大于key的数向后移</li></ul><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/插入排序.png" alt="插入排序"></p><ul><li>时间开销：$\Theta(n^2)$</li><li>空间开销：原址的（仅有常数个元素 需要在排序过程中存储在数组之外）</li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InsertionSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; a.<span class="built_in">size</span>(); i++)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">int</span> key = a[i];</span><br><span class="line">        <span class="keyword">int</span> j = i - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(j &gt;= <span class="number">0</span> &amp;&amp; a[j] &gt; key)</span><br><span class="line">        {</span><br><span class="line">            a[j + <span class="number">1</span>] = a[j];</span><br><span class="line">            j--;</span><br><span class="line">        }</span><br><span class="line">        a[j + <span class="number">1</span>] = key;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="合并排序"><a href="#合并排序" class="headerlink" title="合并排序"></a>合并排序</h3><ul><li><p>思想</p><ul><li>分解（Divide）：将n个元素分成各含n/2个元素的子序列；</li><li>解决（Conquer）：用合并排序法对两个子序列递归地排序；</li><li>合并（Combine）：合并两个已排序的子序列以得到排序结果。</li></ul></li><li><p>辅助过程<strong>MERGE</strong>：（合并两个子数组A[p…q], A[q+1…r]）</p><ul><li>把两个子数组复制到两个新的数组L和R，合并两数组</li></ul></li></ul><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/合并排序1.png" alt="合并排序1"></p><ul><li><p>递归过程：MERGE-SORT</p><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/合并排序2.png" alt="合并排序2" style="zoom:60%;"></p></li><li><p>递归式分析代价：<script type="math/tex">T(n)= \begin{cases} \Theta(1)& \text{n=1}\\ 2T(n/2)+\Theta(n) & \text{n>1} \end{cases}</script></p><ul><li><p>递归树</p><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/合并排序3.png" alt="合并排序3" style="zoom:60%;"></p></li></ul></li><li><p>时间开销：$\Theta(nlgn)$</p></li><li><p>空间开销：非原址</p></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> mid, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> n1 = mid - left + <span class="number">1</span>, n2 = right - mid;</span><br><span class="line">    vector&lt;int&gt; L(n1), R(n2);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n1; i++)</span><br><span class="line">        L[i] = a[left + i];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n2; i++)</span><br><span class="line">        R[i] = a[mid + <span class="number">1</span> + i];</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> k = left; k &lt;= right; k++)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (i &lt; n1 &amp;&amp; j &lt; n2)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (L[i] &lt;= R[j])</span><br><span class="line">                a[k] = L[i++];</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                a[k] = R[j++];</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (i == n1)</span><br><span class="line">            a[k] = R[j++];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            a[k] = L[i++];</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MergeSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span> (left &lt; right)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">int</span> mid = (left + right) / <span class="number">2</span>;</span><br><span class="line">        MergeSort(a, left, mid);</span><br><span class="line">        MergeSort(a, mid+<span class="number">1</span>, right);</span><br><span class="line">        Merge(a, left, mid, right);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><ul><li><p>思想：反复交换相邻的未按次序排序的元素</p><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/冒泡排序1.png" alt="冒泡排序1" style="zoom:67%;"></p></li><li><p>时间开销：$\Theta(n^2)$</p></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BubbleSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.<span class="built_in">size</span>() - <span class="number">1</span>; i++)   <span class="comment">// 已到位数字数目</span></span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; a.<span class="built_in">size</span>() - i - <span class="number">1</span>; j++)   <span class="comment">//遍历剩余的相邻项</span></span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(a[j] &gt; a[j + <span class="number">1</span>])</span><br><span class="line">            {</span><br><span class="line">                <span class="keyword">int</span> temp = a[j];</span><br><span class="line">                a[j] = a[j + <span class="number">1</span>];</span><br><span class="line">                a[j + <span class="number">1</span>] = temp;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><h4 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h4><p>一个近似的完全二叉树，除了最底层外，该树是完全充满的。</p><ul><li>$A.length$：数组长度</li><li>$A.heap-size$：当前有效元素个数</li><li>根节点：$A[1]$</li><li>给定一个节点下标$i$，计算父节点、左孩子、右孩子的下标：</li></ul><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/堆1.png" alt="堆1" style="zoom: 80%;"></p><p><strong>堆的性质：</strong></p><ul><li>最大堆性质：除了根节点以外所有节点$i$满足：$A[PARENT(i)]\geq A[i]$</li><li>最小堆性质：除了根节点以外所有节点$i$满足：$A[PARENT(i)]\leq A[i]$</li><li>树的高度：一个包含$n$个元素的堆的高度为$\Theta(lgn)$</li></ul><h4 id="维护堆的性质"><a href="#维护堆的性质" class="headerlink" title="维护堆的性质"></a>维护堆的性质</h4><ul><li>输入：堆$A$，节点$i$，<u>假设此时以左右子节点为根的子树都已是最大堆</u></li><li>输出：当节点$i$违背最大堆性质（值小于其子节点时），让其值在最大堆中逐级下降，从而使得<strong>以$i$为根节点的子树</strong>重新遵循最大堆的性质。</li><li>在父节点、左孩子、右孩子中选出最大的，若最大的不是父节点，则将该子节点与父节点交换，对该子节点递归地调用函数。</li><li>代价：$O(lgn)$</li></ul><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/维护堆的性质.png" alt="维护堆的性质"></p><h4 id="建堆"><a href="#建堆" class="headerlink" title="建堆"></a>建堆</h4><p>从最后一个非叶节点到根节点，依次调用MAX-HEAPIFY方法</p><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/建堆.png" alt="建堆"></p><ul><li>代价：$O(n)$</li></ul><h4 id="堆排序-1"><a href="#堆排序-1" class="headerlink" title="堆排序"></a>堆排序</h4><p>首先调用BUILD-MAX-HEAP构造最大堆，此时最大元素在$A[1]$中，交换$A[1]$和$A[n]$，此时新的根节点可能违背了最大堆性质，调用MAX_HEAPIFY(A,1)，从而在$A[1…n-1]$上构造一个新的最大堆。以此类推，直到堆的大小降到2。</p><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/堆排序.png" alt="堆排序"></p><ul><li><p>时间开销：$O(nlgn)$</p></li><li><p>空间开销：原址的</p></li></ul><h4 id="最大优先队列"><a href="#最大优先队列" class="headerlink" title="最大优先队列"></a>最大优先队列</h4><ul><li>$MAXIMUM(S)$：返回$S$中最大键字的元素。（return A[1]）<ul><li>$\Theta(1)$</li></ul></li><li>$EXTRACT-MAX(S)$：去掉并返回S中具有最大键字的元素。<ul><li>交换A[1]和A[n]，调用MAX_HEAPIFY(A,1)</li><li>$O(lgn)$</li></ul></li><li>$INCREASE(S,x,k)$：将元素x的关键字值增加到k（假设k不小于x节点原关键字值）<ul><li>将新关键字值不断与父节点进行比较，大于则交换</li><li>$O(lgn)$</li></ul></li><li>$INSERT(S,x)$：把元素x插入到S中。<ul><li>在最后增加一个大小为$-\infin$节点，调用$INCREASE(A,A.heapsize,key)$</li><li>$O(lgn)$</li></ul></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaxHeap</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; heap;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getParent</span><span class="params">(<span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> (i - <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getLeftChild</span><span class="params">(<span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span> * i + <span class="number">1</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getRightChild</span><span class="params">(<span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span> * i + <span class="number">2</span>;</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    MaxHeap(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="built_in">array</span>)</span><br><span class="line">    {</span><br><span class="line">        heap = <span class="built_in">array</span>;</span><br><span class="line">        buildMaxHeap();</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">heapify</span><span class="params">(<span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">int</span> left_child = getLeftChild(i), right_child = getRightChild(i);</span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">max</span> = i;</span><br><span class="line">        <span class="keyword">if</span>(left_child &lt; heap.<span class="built_in">size</span>() &amp;&amp; heap[left_child] &gt; heap[i])</span><br><span class="line">            <span class="built_in">max</span> = left_child;</span><br><span class="line">        <span class="keyword">if</span>(right_child &lt; heap.<span class="built_in">size</span>() &amp;&amp; heap[right_child] &gt; heap[<span class="built_in">max</span>])</span><br><span class="line">            <span class="built_in">max</span> = right_child;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">max</span> != i)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> tmp = heap[i];</span><br><span class="line">            heap[i] = heap[<span class="built_in">max</span>];</span><br><span class="line">            heap[<span class="built_in">max</span>] = tmp;</span><br><span class="line">            heapify(<span class="built_in">max</span>);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">buildMaxHeap</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">int</span> i = getParent(heap.<span class="built_in">size</span>()<span class="number">-1</span>);<span class="comment">//最后一个非叶节点；</span></span><br><span class="line">        <span class="keyword">for</span>(; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">        {</span><br><span class="line">            heapify(i);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">extractMax</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">max</span> = heap[<span class="number">0</span>];</span><br><span class="line">        heap[<span class="number">0</span>] = heap[heap.<span class="built_in">size</span>() - <span class="number">1</span>];</span><br><span class="line">        heap.pop_back();</span><br><span class="line">        heapify(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getMax</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> heap[<span class="number">0</span>];</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">remove</span><span class="params">(<span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        heap[i] = heap[heap.<span class="built_in">size</span>() - <span class="number">1</span>];</span><br><span class="line">        heap.pop_back();</span><br><span class="line">        heapify(i);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">increase</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> k)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        heap[i] = k;</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> parent = getParent(i);</span><br><span class="line">        <span class="keyword">while</span>(heap[parent] &lt; heap[i])</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> tmp = heap[parent];</span><br><span class="line">            heap[parent] = heap[i];</span><br><span class="line">            heap[i] = tmp;</span><br><span class="line">            <span class="keyword">if</span>(parent == <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            i = parent;</span><br><span class="line">            parent = getParent(i);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">int</span> k)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        heap.push_back(<span class="number">-0xFFFFFFF</span>);</span><br><span class="line">        increase(heap.<span class="built_in">size</span>()<span class="number">-1</span>, k);</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getSize</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> heap.<span class="built_in">size</span>();</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><p>PARTITION：将数组$A[p…r]$划分成两个子数组$A[p…q-1]$和$A[q+1…r]$，使得$A[p…q-1]$中每个元素都小于等于$A[q]$，$A[q]$也小于等于$A[q+1…r]$中每个元素。</p><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/快速排序.png" alt="快速排序" style="zoom:80%;"></p><ul><li><p>PARTITION过程</p><ul><li>选择$x=A[r]$作为主元</li><li>$A[p…i]$：小于等于主元的部分</li><li>$A[i+1…j-1]$：大于主元的部分</li><li>$A[j…r-1]$：尚未考虑的部分</li><li>复杂度：$\Theta(n)$，其中$n=r-p+1$</li></ul></li><li><p>时间开销：最坏$\Theta(n^2)$，最好$O(nlgn)$</p></li></ul><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> x = a[right];   <span class="comment">// 主元</span></span><br><span class="line">    <span class="keyword">int</span> i = left - <span class="number">1</span>;       <span class="comment">// i标记小于x、大于x的分界点（a[i]是最后一个小于主元的元素，a[i+1]是第一个大于主元的元素）</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = left; j &lt;= right - <span class="number">1</span>; j++)     <span class="comment">// j遍历每个除主元外的元素</span></span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span>(a[j] &lt;= x)</span><br><span class="line">        {</span><br><span class="line">            i ++;        </span><br><span class="line">            swap(a[i], a[j]);        <span class="comment">//将新的a[j]与第一个大于主元的元素交换</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    swap(a[i + <span class="number">1</span>], a[right]);    <span class="comment">// 放置主元</span></span><br><span class="line">    <span class="keyword">return</span> i + <span class="number">1</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">QuickSort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span> (left &lt; right)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">int</span> pivot = Partition(a, left, right);</span><br><span class="line">        QuickSort(a, left, pivot - <span class="number">1</span>);</span><br><span class="line">        QuickSort(a, pivot + <span class="number">1</span>, right);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h3><p>假设$n$个输入元素都是在0到$k$区间内的一个整数。对于每一个输入元素$x$，确定小于x的元素个数，利用这一信息，就可以直接把x放到它在输出数组中的位置上了。例如有17个元素小于x，则x就应该在第18个输出位置上。</p><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/计数排序.png" style="zoom:80%;"></p><ul><li>代价：当$k=O(n)$时，排序的运行时间为$\Theta(n)$</li></ul><h3 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h3><p>从最低有效位到最高有效位进行排序。</p><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/基数排序2.png" alt="基数排序2" style="zoom:80%;"></p><ul><li>代价：给定$n$个$d$位数，其中每一个数位可能有$k$个可能的取值，若使用的稳定排序方法耗时$n+k$，那么它就可以在$\Theta(d(n+k))$时间内将这些数排好序。</li></ul><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/基数排序.png" alt="基数排序" style="zoom:80%;"></p><h3 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h3><p>假设数据服从$[0,1)$上的均匀分布</p><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/桶排序.png" alt="桶排序" style="zoom:80%;"></p><ul><li>时间代价：$O(n)$</li></ul><h2 id="中位数和顺序统计量"><a href="#中位数和顺序统计量" class="headerlink" title="中位数和顺序统计量"></a>中位数和顺序统计量</h2><h3 id="同时找到最大值和最小值"><a href="#同时找到最大值和最小值" class="headerlink" title="同时找到最大值和最小值"></a>同时找到最大值和最小值</h3><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/最大值最小值.png" alt="最大值最小值" style="zoom: 67%;"></p><h3 id="期望时间为线性的选择算法"><a href="#期望时间为线性的选择算法" class="headerlink" title="期望时间为线性的选择算法"></a>期望时间为线性的选择算法</h3><p>一种分治算法，用到了快速排序中的RANDOMIZED-SELECT算法，但快排会递归处理划分的两边，这里只处理一边。</p><ul><li>算法返回数组$A[p…r]$中第$i$小的元素</li><li>期望运行时间：$\Theta(n)$</li><li>最坏运行时间：$\Theta(n^2)$</li></ul><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/随机选择.png" alt="随机选择" style="zoom:80%;"></p><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//LeetCode 215: 在未排序的数组中找到第 k 个最大的元素。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">int</span> x = a[right];</span><br><span class="line">        <span class="keyword">int</span> i = left - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = left; j &lt;= right - <span class="number">1</span>; j++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (a[j] &gt;= x)    <span class="comment">// 注意这里改成了&gt;=</span></span><br><span class="line">            {</span><br><span class="line">                i++;</span><br><span class="line">                swap(a[i], a[j]);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        swap(a[i+<span class="number">1</span>], a[right]);<span class="number">4</span></span><br><span class="line">        <span class="keyword">return</span> i + <span class="number">1</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Select</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> left, <span class="keyword">int</span> right, <span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">if</span> (left == right)</span><br><span class="line">            <span class="keyword">return</span> a[left];</span><br><span class="line">        <span class="keyword">int</span> x = Partition(a, left, right);</span><br><span class="line">        <span class="keyword">int</span> k = x - left + <span class="number">1</span>;   <span class="comment">// 当前的x是第几个数字</span></span><br><span class="line">        <span class="keyword">if</span> (i == k)</span><br><span class="line">            <span class="keyword">return</span> a[x];</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (i &lt; k)</span><br><span class="line">            <span class="keyword">return</span> Select(a, left, x - <span class="number">1</span>, i);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> Select(a, x + <span class="number">1</span>, right, i - k);</span><br><span class="line">    }</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findKthLargest</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="keyword">return</span> Select(nums, <span class="number">0</span>, nums.<span class="built_in">size</span>()<span class="number">-1</span>, k);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><h3 id="最坏情况为线性的选择算法"><a href="#最坏情况为线性的选择算法" class="headerlink" title="最坏情况为线性的选择算法"></a>最坏情况为线性的选择算法</h3><p>与随机选择相比，保证对数组能做出一个较好的划分：</p><p><img src="/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/select.png" alt="select" style="zoom: 67%;"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E7%AE%97%E6%B3%95/">算法</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/">算法导论</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E6%8E%92%E5%BA%8F/">排序</category>
      
      
      <comments>https://rubychen0611.github.io/2020/12/15/%E3%80%90%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E3%80%91%E6%8E%92%E5%BA%8F/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>用python从零实现一个神经网络</title>
      <link>https://rubychen0611.github.io/2020/11/30/My-Neural-Network/</link>
      <guid>https://rubychen0611.github.io/2020/11/30/My-Neural-Network/</guid>
      <pubDate>Mon, 30 Nov 2020 06:03:50 GMT</pubDate>
      
      <description>&lt;p&gt;不用tensorflow、pytorch等任何现有深度学习框架以及各种封装好的机器学习库，仅使用python语言及矩阵运算的库，从零开始实现一个全联接的神经网络。&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>不用tensorflow、pytorch等任何现有深度学习框架以及各种封装好的机器学习库，仅使用python语言及矩阵运算的库，从零开始实现一个全联接的神经网络。<a id="more"></a></p><h2 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h2><p>我们以一个回归任务为例，实现一个多层神经网络来拟合一个非线性函数$ y= \sin (x_{1})- \cos (x_{2}),x_{1} \in \left[ -5,5 \right] ,x_{2} \in \left[ -5,5 \right] $，函数图像如下图所示，输入层包含2个神经元，输出层包含1个神经元，隐藏层的数目和神经元数自定义，隐藏层采用ReLU作为激活函数，输出层用恒等函数做为激活函数，损失函数为均方误差（MSE）。</p><p>实际上我们实现的神经网络可以自定义输入输出大小、隐藏层结构、激活函数、损失函数等，因此用在其他任务上也是完全可以的。</p><p><img src="/2020/11/30/My-Neural-Network/1.png" alt="1" style="zoom:50%;"></p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>训练神经网络之前，我们需要先完成一些准备工作，包括生成训练和验证的数据集、定义用到的激活函数和损失函数。</p><h3 id="数据集生成"><a href="#数据集生成" class="headerlink" title="数据集生成"></a>数据集生成</h3><p>在轴$x_1$和$x_2$的$[-5,5]$区间上每隔0.1均匀地对样本点进行采样，然后计算目标函数的值，生成100*100=10000个训练数据，转换成为numpy​矩阵，作为训练数据集：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成训练数据集</span></span><br><span class="line">x_train = []</span><br><span class="line">y_train = []</span><br><span class="line"><span class="keyword">for</span> x1 <span class="keyword">in</span> np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="keyword">for</span> x2 <span class="keyword">in</span> np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>):</span><br><span class="line">        x_train.append([x1, x2])</span><br><span class="line">        y_train.append(np.sin(x1)-np.cos(x2))</span><br><span class="line">x_train = np.array(x_train)</span><br><span class="line">y_train = np.array(y_train)</span><br></pre></td></tr></tbody></table></figure><p>另外随机采样并生成大小为1000的验证数据集：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成验证数据集</span></span><br><span class="line">x_val = []</span><br><span class="line">y_val = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    x1 = random.uniform(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">    x2 = random.uniform(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">    x_val.append([x1, x2])</span><br><span class="line">    y_val.append(np.sin(x1)-np.cos(x2))</span><br><span class="line">x_val = np.array(x_val)</span><br><span class="line">y_val = np.array(y_val)</span><br><span class="line"> </span><br></pre></td></tr></tbody></table></figure><h3 id="激活函数和损失函数"><a href="#激活函数和损失函数" class="headerlink" title="激活函数和损失函数"></a>激活函数和损失函数</h3><p>定义本次任务用到的数学函数，包括激活函数和损失函数：</p><ul><li><p>激活函数</p><p>定义ActivationFunction类作为激活函数的基类，包括calculate（计算）和derivative（求导）两个抽象方法。本次任务中用到了ReLU和Pureline两种激活函数，若想使用其他激活函数可以仿照下面的形式定义（注意接受的参数x都是一个numpy矩阵）。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABCMeta, abstractmethod</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActivationFunction</span>:</span></span><br><span class="line">    <span class="string">'''激活函数基类'''</span></span><br><span class="line">    __metaclass__ = ABCMeta</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derivative</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReLU</span>(<span class="params">ActivationFunction</span>):</span></span><br><span class="line">    <span class="string">'''relu激活函数'''</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.maximum(x, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derivative</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.where(x &gt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pureline</span>(<span class="params">ActivationFunction</span>):</span></span><br><span class="line">    <span class="string">'''恒等激活函数'''</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derivative</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.ones(shape=x.shape)</span><br></pre></td></tr></tbody></table></figure></li></ul><ul><li><p>MSE损失函数</p><p>假设样本个数为$n$，计算公式：$MSE=\frac{1}{n} \sum_{i=1}^{n}(y_{true}[i]-y_{pred}[i])^2$</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse_loss</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    <span class="string">'''MSE损失函数'''</span></span><br><span class="line">    <span class="keyword">return</span> ((y_true - y_pred) ** <span class="number">2</span>).mean()</span><br></pre></td></tr></tbody></table></figure></li></ul><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="数据结构定义和初始化"><a href="#数据结构定义和初始化" class="headerlink" title="数据结构定义和初始化"></a>数据结构定义和初始化</h3><p>首先定义类NeuralNetwork表示神经网络类，初始化时用户向类构造函数传入参数input_size（输入层维度）、output_size（输出层维度）、hidden_size（隐藏层维度，用一个数组表示，如[5,3]表示两个隐藏层，分别包含5个和3个神经元），从而可以达到任意调整神经网络结构的目的。</p><p>接着定义两个数组self.w和self.b表示神经网络的权重和偏移量，每层的权重是一个numpy矩阵，用np.random.normal函数进行高斯分布初始化（设置均值为0，方差为0.1）；同时定义一个数组self.activations表示每层的激活函数，这里隐藏层激活函数为ReLU，输出层激活函数为Pureline：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机初始化权重和偏移量</span></span><br><span class="line">self.len = len(hidden_size) + <span class="number">1</span>    <span class="comment"># 层数(除输入层)</span></span><br><span class="line">loc = <span class="number">0.0</span></span><br><span class="line">scale = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 隐藏层</span></span><br><span class="line">self.w = [np.random.normal(loc, scale, size=(input_size, hidden_size[<span class="number">0</span>]))]</span><br><span class="line">self.b = [np.random.normal(loc, scale, size=(hidden_size[<span class="number">0</span>],))]</span><br><span class="line">self.activations = [ReLU]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(hidden_size)<span class="number">-1</span>):</span><br><span class="line">    self.w.append(np.random.normal(loc, scale, size=(hidden_size[i], hidden_size[i+<span class="number">1</span>])))</span><br><span class="line">    self.b.append(np.random.normal(loc, scale, size=(hidden_size[i+<span class="number">1</span>],)))</span><br><span class="line">    self.activations.append(ReLU)</span><br><span class="line"><span class="comment"># 输出层</span></span><br><span class="line">self.w.append(np.random.normal(loc, scale, size=(hidden_size[<span class="number">-1</span>], output_size)))</span><br><span class="line">self.b.append(np.random.normal(loc, scale, size=(output_size,)))</span><br><span class="line">self.activations.append(Pureline)</span><br></pre></td></tr></tbody></table></figure><p>然后定义五个数组用于保存训练时的中间结果，均初始化为全0：</p><ul><li>self.z：每个神经元激活前的输出。</li><li>self.a：每个神经元激活后的输出。</li><li>self.delta：神经单元误差$\delta$，即loss对每个神经元激活前输出$z$的梯度（用于反向传播更新权重）。</li><li>self.delta_w：权重w的累积更新值，用于处理完每个batch后更新权重。</li><li>self.delta_b：权重b的累积更新值。</li></ul><h3 id="前向计算中间结果"><a href="#前向计算中间结果" class="headerlink" title="前向计算中间结果"></a>前向计算中间结果</h3><p>每轮训练首先调用一个自定义的shuffle方法打乱训练样本集合。然后开始训练，设置训练轮数为30，batch_size为10，初始学习率为0.01。对每个输入样本，通过前向传播计算得到每层的输出self.z和self.a，如假设第$i$层（$i&gt;1$)的输入为$a[i-1]$，权重和偏移为$w[i]$和$b[i]$，则有:</p><p>$z[i]=a[i-1]\times w[i] + b[i]$，</p><p>$a[i]=activations[i].calculate(z[i])$</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="string">'''随机打乱数据'''</span></span><br><span class="line">    xy= np.c_[x,y]</span><br><span class="line">    np.random.shuffle(xy)</span><br><span class="line">    x = xy[:, :<span class="number">-1</span>]</span><br><span class="line">    y = xy[:, <span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, x_train, y_train, x_val, y_val, epochs=<span class="number">30</span>, batch_size=<span class="number">10</span>, lr=<span class="number">0.01</span></span>):</span></span><br><span class="line">    <span class="string">'''训练模型'''</span></span><br><span class="line"></span><br><span class="line">    self.validate(<span class="number">0</span>, x_train, y_train, x_val, y_val)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        x_train_shuffle, y_train_shuffle = shuffle(x_train, y_train)</span><br><span class="line">        batch_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x, y_true <span class="keyword">in</span> zip(x_train_shuffle, y_train_shuffle):</span><br><span class="line">            <span class="comment"># 前向传播得到每层输出</span></span><br><span class="line">            layer_output = x</span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">0</span>, self.len):</span><br><span class="line">                self.z[layer] = np.matmul(layer_output, self.w[layer]) + self.b[layer]</span><br><span class="line">                layer_output = self.activations[layer].calculate(self.z[layer])</span><br><span class="line">                self.a[layer] = layer_output</span><br></pre></td></tr></tbody></table></figure><h3 id="反向传播计算梯度"><a href="#反向传播计算梯度" class="headerlink" title="反向传播计算梯度"></a>反向传播计算梯度</h3><p>接下来计算输出层（第-1层）的神经单元误差$\delta$，对于每个样本，假如输出层维度为$m$，损失函数$C=\frac{1}{m}\sum_{i=1}^{m}(y_{true}[i]-a[-1][i])^2$（本次任务中$m=1$），则$\delta[-1] = \frac{\partial C}{\partial z[-1]} = \frac{\partial C}{\partial a[-1]} \cdot \frac{\partial a[-1]}{\partial z[-1]}=  \frac{1}{m} \cdot 2 \cdot (a[-1] - y_{true}) \cdot \theta’(z[-1])$，其中$\theta$为输出层的激活函数。</p><p>然后可以通过递推式反向计算出前面每层的神经单元误差，第$i$层的神经单元误差$\delta[i]=w[i+1] \times \delta[i+1] \times \theta’(z[i])$ ，其中$\theta$表示第$i$层的激活函数。</p><p>通过神经单元误差$\delta$我们可以计算出损失函数对w和b的导数，对第$i$层：$\frac{\partial C}{\partial w[i]} = a[i-1]^T \cdot \delta[i]$，$\frac{\partial C}{\partial b[i]}= \delta[i]$。并将导数累加结果记录在self.delta_w、self.delta_b数组中。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算最后一层的神经单元误差δ</span></span><br><span class="line">self.delta[<span class="number">-1</span>] = <span class="number">2</span> * (self.a[<span class="number">-1</span>] - y_true) * self.activations[<span class="number">-1</span>].derivative(self.z[<span class="number">-1</span>]) / len(self.delta[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向递推计算每层神经单元误差δ</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(self.len<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">    self.delta[layer] = np.matmul(self.w[layer + <span class="number">1</span>], self.delta[layer + <span class="number">1</span>]) * self.activations[layer].derivative(self.z[layer])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 累加导数</span></span><br><span class="line">last_layer_output = x</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(self.len):</span><br><span class="line">    self.delta_w[layer] += np.matmul(np.transpose([last_layer_output]), [self.delta[layer]])</span><br><span class="line">    self.delta_b[layer] += self.delta[layer]</span><br><span class="line">    last_layer_output = self.a[layer]</span><br></pre></td></tr></tbody></table></figure><h3 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h3><p>每个batch结束后，更新w和b。对第i层：</p><p>$w[i] \leftarrow w[i]- lr \cdot \Delta w[i]$</p><p>$b[i] \leftarrow b[i]- lr \cdot \Delta b[i]$</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个batch结束后更新w、b</span></span><br><span class="line">batch_count += <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> batch_count == batch_size:</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> range(self.len):</span><br><span class="line">        self.w[layer] -= lr * self.delta_w[layer]</span><br><span class="line">        self.b[layer] -= lr * self.delta_b[layer]</span><br><span class="line">        self.delta_w[layer].fill(<span class="number">0</span>)</span><br><span class="line">        self.delta_b[layer].fill(<span class="number">0</span>)</span><br><span class="line">batch_count = <span class="number">0</span></span><br></pre></td></tr></tbody></table></figure><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>这里采用了简单的学习率衰减方案，每过三轮学习率衰减为0.9倍。大家可以尝试其他衰减方案。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">3</span> == <span class="number">0</span>:</span><br><span class="line">    lr *= <span class="number">0.9</span></span><br></pre></td></tr></tbody></table></figure><h2 id="验证和可视化"><a href="#验证和可视化" class="headerlink" title="验证和可视化"></a>验证和可视化</h2><p>每轮训练完成后调用validate方法验证模型在训练数据集和验证数据集上的损失，并绘制图像。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="string">'''前向传播计算预测结果'''</span></span><br><span class="line">    layer_output = x</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">0</span>, self.len):</span><br><span class="line">        layer_output = self.activations[layer].calculate((np.matmul(layer_output, self.w[layer]) + self.b[layer]))</span><br><span class="line">    <span class="keyword">return</span> layer_output.squeeze()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">self, epoch, x_train, y_train, x_val, y_val</span>):</span></span><br><span class="line">    <span class="string">'''计算训练集和验证集上的损失，生成可视化图像'''</span></span><br><span class="line">    y_pred_train = self.feedforward(x_train)</span><br><span class="line">    train_loss = mse_loss(y_train, y_pred_train)</span><br><span class="line">    y_pred_val = self.feedforward(x_val)</span><br><span class="line">    val_loss = mse_loss(y_val, y_pred_val)</span><br><span class="line">    self.visualize(epoch, y_pred_train, train_loss, val_loss)</span><br><span class="line">    print(<span class="string">"Epoch %d, train loss: %.4f, validation loss: %.4f"</span> % (epoch, train_loss, val_loss))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span>(<span class="params">self, epoch, y_pred, train_loss, val_loss</span>):</span></span><br><span class="line">   <span class="string">'''可视化训练结果'''</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.gca(projection=<span class="string">'3d'</span>)</span><br><span class="line">    X = np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">    Y = np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">    X, Y = np.meshgrid(X, Y)</span><br><span class="line">    Z = y_pred.reshape((<span class="number">100</span>,<span class="number">100</span>))</span><br><span class="line">    surf = ax.plot_surface(X, Y, Z, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>, cmap=cm.jet, linewidth=<span class="number">0</span>, antialiased=<span class="literal">False</span>)</span><br><span class="line">    ax.set_zlim(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">    ax.zaxis.set_major_locator(LinearLocator(<span class="number">10</span>))</span><br><span class="line">    ax.zaxis.set_major_formatter(FormatStrFormatter(<span class="string">'%.02f'</span>))</span><br><span class="line">    ax.set_title(<span class="string">'epoch=%d, train loss=%.4f, validation loss=%.4f'</span>% (epoch, train_loss, val_loss))</span><br><span class="line">    fig.colorbar(surf, shrink=<span class="number">0.5</span>, aspect=<span class="number">5</span>)</span><br><span class="line">    plt.savefig(<span class="string">'figure/%04d.png'</span> % epoch)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure><h2 id="拟合效果"><a href="#拟合效果" class="headerlink" title="拟合效果"></a>拟合效果</h2><h3 id="增加隐藏层宽度对拟合效果的影响"><a href="#增加隐藏层宽度对拟合效果的影响" class="headerlink" title="增加隐藏层宽度对拟合效果的影响"></a>增加隐藏层宽度对拟合效果的影响</h3><p>设置神经网络只有一个隐藏层，考察隐藏层分别包含10、20、50、100个神经元时，训练集loss随训练轮数增加的变化情况（如下图）。（设置学习率为恒定值0.001，训练轮数为30轮）</p><p><img src="/2020/11/30/My-Neural-Network/3.png" alt="image-20201129230937618" style="zoom:70%;"></p><p>可见随着训练轮数增长，各个模型的训练集loss首先快速下降，接着缓慢下降。单隐藏层神经元个数越多，训练完成后的loss越低，说明增加神经元宽度能提升模型对目标函数的拟合能力。</p><h3 id="增加隐藏层深度对拟合效果的影响"><a href="#增加隐藏层深度对拟合效果的影响" class="headerlink" title="增加隐藏层深度对拟合效果的影响"></a>增加隐藏层深度对拟合效果的影响</h3><p>考察神经网络分别包含1、2、3、4个隐藏层，且每个隐藏层包含20个神经元时，训练集loss随训练轮数增加的变化情况（如下图）。（设置学习率为恒定值0.001，训练轮数为30轮）。</p><p><img src="/2020/11/30/My-Neural-Network/4.png" alt="image-20201129232317776" style="zoom:70%;"></p><p>由图可见，神经网络层数越多，30轮训练结束后训练集的loss越低，说明层数越多，模型对目标函数的拟合能力越强。</p><h3 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h3><p>经过以上实验，我们验证了增加隐藏层的宽度和深度能减小拟合的误差。最后为了让拟合误差尽量小，经过多次尝试后，我们最终选择设置隐藏层个数为5，分别包含100、80、50、30、10个神经元，学习率衰减方案为每经过3轮训练，lr衰减为0.9倍。最终训练集loss为0.00015，验证集loss为0.00016。第0轮（训练前）、第1轮、第5轮、第30轮训练后的拟合结果可视化后如下图所示：</p><p><img src="/2020/11/30/My-Neural-Network/5.jpg" alt="图片1"></p><p>动图效果：</p><p><img src="/2020/11/30/My-Neural-Network/2.gif" alt="2" style="zoom:80%;"></p><p>完整代码见：<a href="https://github.com/rubychen0611/MyNeuralNetwork">https://github.com/rubychen0611/MyNeuralNetwork</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/">技术笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</category>
      
      <category domain="https://rubychen0611.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      <comments>https://rubychen0611.github.io/2020/11/30/My-Neural-Network/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>【论文笔记】On Decomposing a Deep Neural Network into Modules</title>
      <link>https://rubychen0611.github.io/2020/11/16/DNN-Decomposition/</link>
      <guid>https://rubychen0611.github.io/2020/11/16/DNN-Decomposition/</guid>
      <pubDate>Mon, 16 Nov 2020 03:21:36 GMT</pubDate>
      
      <description>&lt;p&gt;On Decomposing a Deep Neural Network into Modules （FSE’20）&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>On Decomposing a Deep Neural Network into Modules （FSE’20）<a id="more"></a></p><p>会议视频：<a href="https://www.youtube.com/watch?v=EH1aUbFj0HQ">https://www.youtube.com/watch?v=EH1aUbFj0HQ</a></p><p>代码：<a href="https://github.com/rangeetpan/decomposeDNNintoModules">https://github.com/rangeetpan/decomposeDNNintoModules</a></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul><li><p>传统软件的模块化分解与重组：可用于软件重用、替换、独立测试、独立开发等</p><p><img src="/2020/11/16/DNN-Decomposition/1.png" alt="1" style="zoom:67%;"></p></li><li><p>为什么DNN也需要模块分解？应用场景举例</p><ul><li><p>场景1：数据集内部分解</p><ul><li>手写数字识别→0，1识别，传统方法需要重新训练，本方法可直接将DNN解构成10个模块，组合成0、1识别模型。</li></ul><p><img src="/2020/11/16/DNN-Decomposition/2.png" alt="2" style="zoom:50%;"></p></li><li><p>场景2：数据集之间</p><ul><li><p>数字识别+字母识别-&gt;16进制数字识别</p><p><img src="/2020/11/16/DNN-Decomposition/3.png" alt="3" style="zoom:50%;"></p></li></ul></li><li><p>场景3：模块替换</p><ul><li><p>数字识别模型中某个数字5识别效果较差，从另一个模型中分解出单独识别5的模块，与A的其他模块组合在一起</p><p><img src="/2020/11/16/DNN-Decomposition/4.png" alt="4" style="zoom:50%;"></p></li></ul></li></ul></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="步骤1：关注点识别-Concern-Identification-CI"><a href="#步骤1：关注点识别-Concern-Identification-CI" class="headerlink" title="步骤1：关注点识别 Concern Identification (CI)"></a>步骤1：关注点识别 Concern Identification (CI)</h3><ul><li>关注点识别本质上是识别出整体模型中对特定功能或关注点有贡献的那些部分，解构DNN。</li><li>具体地，依次对模型喂入关注类别的训练样本：<ul><li>对未激活节点：将输入边和输出边的权重都置为零（删除边）</li><li>对激活节点：将输出边权重置为正（<code>算法1没看懂为什么是min</code>）</li><li>删除与输出层其他类别相连的边。</li></ul></li></ul><p><img src="/2020/11/16/DNN-Decomposition/5.png" alt="5" style="zoom:60%;"></p><h3 id="步骤2：纠缠识别-Tangling-Identification（TI）"><a href="#步骤2：纠缠识别-Tangling-Identification（TI）" class="headerlink" title="步骤2：纠缠识别 Tangling Identification（TI）"></a>步骤2：纠缠识别 Tangling Identification（TI）</h3><ul><li><p>第一步之后，解构出的模块只对目标类别的识别起作用，即目前只是一个<strong>单分类器</strong>，无法对非本类的样本进行判断。这就好比从一个程序中删除一个条件及其分支，从而产生直接执行剩余分支功能的子程序，但无条件地这样做。（<code>即所有样本都会被分类成这一个类别</code>）</p></li><li><p>解决方法：<font color="red">加回一些节点和边，帮助区分非目标类别。</font>四种TI方法：</p><ul><li><p>Imbalance (TI-I)</p><p><img src="/2020/11/16/DNN-Decomposition/6.png" alt="6" style="zoom:60%;"></p></li><li><p>Punish Negative Examples (TI-PN)</p><p><img src="/2020/11/16/DNN-Decomposition/7.png" alt="7" style="zoom:60%;"></p></li><li><p>Higher Priority to Negative Examples (TI-HP)</p><p><img src="/2020/11/16/DNN-Decomposition/8.png" alt="8" style="zoom:60%;"></p></li><li><p>Strong Negative Edges (TI-SNE)</p><p><img src="/2020/11/16/DNN-Decomposition/9.png" alt="9" style="zoom:60%;"></p></li></ul></li></ul><h3 id="步骤3：关注点模块化-Concern-Modularization-CM"><a href="#步骤3：关注点模块化-Concern-Modularization-CM" class="headerlink" title="步骤3：关注点模块化 Concern Modularization (CM)"></a>步骤3：关注点模块化 Concern Modularization (CM)</h3><p>这一步作用是将多个非关注点及其相应的神经元和边，抽象成一个输出层的节点（如下图的非0节点）：</p><p><img src="/2020/11/16/DNN-Decomposition/10.png" alt="10" style="zoom:50%;"></p><ul><li><p>Channeling (CM-C)：将最后一层输出到非关注点的边，通过取权重的平均值，都改向到一个”非“节点上</p><p><img src="/2020/11/16/DNN-Decomposition/11.png" alt="11" style="zoom:60%;"></p></li><li><p>Remove Irrelevant Edges (CM-RIE)：在Channeling之前，去掉倒数第二层仅对非关注点有贡献的边及其相关神经元</p><p><img src="/2020/11/16/DNN-Decomposition/12.png" alt="12" style="zoom:60%;"></p><p><img src="/2020/11/16/DNN-Decomposition/13.png" alt="13" style="zoom:70%;"></p></li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h3><ul><li>数据集：<ul><li>MNIST</li><li>EMNSIT（手写英文字母）：只用A-J（10个字母）训练</li><li>FMNIST：衣服</li><li>KMNIST：日语字母</li></ul></li><li>模型：<ul><li>4个数据集分别训练带1、2、3、4个隐藏层的全连接神经网络，每个隐藏层49个神经元</li></ul></li><li>测试准则<ul><li>准确率：衡量DNN模型模块化后的准确率，对输入的样本用分解后的每个模块进行预测，将给出positive预测结果且置信度最高并的子模型预测结果作为预测类别（投票）。</li><li>Jaccard系数（JI）：衡量模块之间的相似度，将所有权重和偏置放在一个向量中，比较Jaccard系数。</li></ul></li></ul><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul><li><p>实验1：分解后的模块有效性如何？</p><p><img src="/2020/11/16/DNN-Decomposition/14.png" alt="14" style="zoom:80%;"></p><ul><li>TI-HP方法：JI最低，但准确率同样很低<ul><li>因为先更新负类样本权重，再更新正类样本权重，负类样本权重被覆盖</li></ul></li><li>TI-PN方法：多个模型上低准确率<ul><li>先更新正类样本权重，再更新负类样本权重，JI较高，因为负类样本权重重合较多</li></ul></li><li>TI-SNE方法：准确率最高</li><li>CM-RIE相比CM-C降低了JI系数，且在准确率上表现较好。</li><li>准确率不变或有提升的模型中：33.79%的边是失效的，说明这些模型与原模型并不是完全一样。</li><li>调整了每层神经元的个数至70个之后，CM-RIE方法效果几乎一致。</li></ul></li><li><p>实验2：模块化后的DNN支持<strong>重用</strong>吗？</p><ul><li><p>数据集内部重用</p><ul><li><p>从原模型模型构造两类别模型（共C(10,2)=45种情况)：用这两类数据重训练的相同结构的模型作为对比。结果显示与重训练的模型准确率差不多。</p><p><img src="/2020/11/16/DNN-Decomposition/15.png" alt="15" style="zoom:67%;"></p><p><img src="/2020/11/16/DNN-Decomposition/17.png" alt="17" style="zoom:50%;"></p></li></ul></li><li><p>数据集之间重用</p><ul><li>绝大多数模型有一定的精度下降</li></ul><p><img src="/2020/11/16/DNN-Decomposition/18.png" alt="18" style="zoom:50%;"></p></li></ul></li></ul><ul><li><p>实验3：模块化后的DNN支持<strong>替换</strong>吗？</p><ul><li><p>同数据集模型替换</p></li><li><p>不同数据集模型替换</p><p><img src="/2020/11/16/DNN-Decomposition/19.png" alt="19" style="zoom:60%;"></p></li></ul></li></ul><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul><li><p>应用场景设想很新颖。</p></li><li><p>文章笔误太多，算法解释得不清楚。</p></li><li><p>直接对模型权重进行操纵的做法很大胆，实验用的模型都是全联接的小型模型，怀疑在其他大型模型上的可行性。</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content:encoded>
      
      
      <category domain="https://rubychen0611.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</category>
      
      
      <category domain="https://rubychen0611.github.io/tags/DNN%E5%88%86%E8%A7%A3/">DNN分解</category>
      
      
      <comments>https://rubychen0611.github.io/2020/11/16/DNN-Decomposition/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
